[
{
	"uri": "https://the-going.github.io/website/core/build-steps/",
	"title": "Building EVL",
	"tags": [],
	"description": "",
	"content": "Building EVL from source   The build process. Building EVL from the source code is a two-step process: we need to build a kernel enabling the EVL core, and the library implementing the user API to this core - aka libevl - using the proper toolchain. These steps may happen in any order. The output of this process is:\n  a Linux kernel image featuring Dovetail and the EVL core on top of it.\n  the libevl.so shared library* which enables applications to request services from the EVL core, along with a few basic utilities and test programs.\n* The static archive libevl.a is generated as well.\n    Getting the sources EVL sources are maintained in GIT repositories. As a preliminary step, you may want to have a look at the EVL development process, in order to determine which GIT branches you may be interested in these repositories:\n  The kernel tree featuring the EVL core:\n git@git.xenomai.org:Xenomai/xenomai4/linux-evl.git https://git.xenomai.org/xenomai4/linux-evl.git    The libevl tree which provides the user interface to the core:\n git@git.xenomai.org:Xenomai/xenomai4/libevl.git https://git.xenomai.org/xenomai4/libevl.git    Other prerequisites In addition to the source code, we need:\n  a GCC toolchain for the target CPU architecture.\n  the UAPI headers from the target Linux kernel fit with the EVL core. Each UAPI file exports a set of definitions and interface types which are shared with libevl.so running in user-space, so that the latter can submit well-formed system calls to the former. In other words, to build libevl.so, we need access to the contents of include/uapi/asm/ and include/uapi/evl/ from a source kernel tree which contains the EVL core which is going to handle the system calls.\n  libevl relies on thread-local storage support (TLS), which might be broken in some obsolete (ARM) toolchains. Make sure to use a current one.\n Building the core Once your favorite kernel configuration tool is brought up, you should see the EVL configuration block somewhere inside the General setup menu. This configuration block looks like this:\nEnabling CONFIG_EVL should be enough to get you started, the default values for other EVL settings are safe to use. You should make sure to have CONFIG_EVL_LATMUS and CONFIG_EVL_HECTIC enabled too; those are drivers required for running the latmus and hectic utilities available with libevl, which measure latency and validate the context switching sanity.\nIf you are unfamiliar with building kernels, this document may help. If you face hurdles building directly into the kernel source tree as illustrated in the document mentioned, you may want to check whether building out-of-tree might work, since this is how Dovetail/EVL developers usually rebuild kernels. If something goes wrong while building in-tree or out-of-tree, please send a note to the EVL mailing list with the relevant information.\n All core configuration options   #kconfig { width: 100%; } #kconfig th { text-align: center; } #kconfig td { text-align: left; } #kconfig tr:nth-child(even) { background-color: #f2f2f2; } #kconfig td:nth-child(2) { text-align: center; }   Symbol name Default Purpose   CONFIG_EVL N Enable the EVL core   CONFIG_EVL_SCHED_QUOTA N Enable the quota-based scheduling policy   CONFIG_EVL_SCHED_TP N Enable the time-partitioning scheduling policy   CONFIG_EVL_SCHED_TP_NR_PART N Number of time partitions for CONFIG_EVL_SCHED_TP   CONFIG_EVL_HIGH_PERCPU_CONCURRENCY N Optimizes the implementation for applications with many real-time threads running concurrently on any given CPU\tcore   CONFIG_EVL_RUNSTATS Y Collect runtime statistics about threads   CONFIG_EVL_COREMEM_SIZE 2048 Size of the core memory heap (in kilobytes)   CONFIG_EVL_NR_THREADS 256 Maximum number of EVL threads   CONFIG_EVL_NR_MONITORS 512 Maximum number of EVL monitors (i.e. mutexes + semaphores + flags + events)   CONFIG_EVL_NR_CLOCKS 8 Maximum number of EVL clocks   CONFIG_EVL_NR_XBUFS 16 Maximum number of EVL cross-buffers   CONFIG_EVL_NR_PROXIES 64 Maximum number of EVL proxies   CONFIG_EVL_NR_OBSERVABLES 64 Maximum number of EVL observables (does not include threads)   CONFIG_EVL_LATENCY_USER 0 Pre-set core timer gravity value for user threads (0 means use pre-calibrated value)   CONFIG_EVL_LATENCY_KERNEL 0 Pre-set core timer gravity value for kernel threads (0 means use pre-calibrated value)   CONFIG_EVL_LATENCY_IRQ 0 Pre-set core timer gravity value for interrupt handlers (0 means use pre-calibrated value)   CONFIG_EVL_DEBUG N Enable debug features   CONFIG_EVL_DEBUG_CORE N Enable core debug assertions   CONFIG_EVL_DEBUG_CORE N Enable core debug assertions   CONFIG_EVL_DEBUG_MEMORY N Enable debug checks in core memory allocator. **This option adds a significant overhead affecting latency figures**   CONFIG_EVL_DEBUG_WOLI N Enable warn-on-lock-inconsistency checkpoints   CONFIG_EVL_WATCHDOG Y Enable watchdog timer   CONFIG_EVL_WATCHDOG_TIMEOUT 4 Watchdog timeout value (in seconds).   CONFIG_GPIOLIB_OOB n Enable support for out-of-band GPIO line handling requests.   CONFIG_SPI_OOB, CONFIG_SPIDEV_OOB n Enable support for out-of-band SPI transfers.   Enabling 32-bit support in a 64-bit kernel (CONFIG_COMPAT) Starting from EVL ABI 20 in the v5.6 series, the EVL core generally allows 32-bit applications to issue system calls to a 64-bit kernel when both the 32 and 64-bit CPU architectures are supported, such as ARM (aka Aarch32) code running over an arm64 (Aarch64) kernel. For arm64, you need to turn on CONFIG_COMPAT and CONFIG_COMPAT_VDSO in the kernel configuration. To be allowed to change the latter, the CROSS_COMPILE_COMPAT environment variable should be set to the prefix of the 32-bit ARMv7 toolchain which should be used to compile the vDSO (yes, this is quite convoluted). For instance:\n$ make \u0026lt;your-make-args\u0026gt; ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- CROSS_COMPILE_COMPAT=arm-linux-gnueabihf- (x|g|menu)config  For instance, if you plan to run EVL over any of the Raspberry PI 64-bit computers, you may find useful to use the PI-centric 32-bit Linux distributions readily available such as Raspbian. To do so, make sure to enable CONFIG_COMPAT and CONFIG_COMPAT_VDSO for your EVL-enabled kernel, building the 32-bit vDSO alongside as mentioned earlier.\n Building libevl The generic command for building libevl is:\n$ make [-C $SRCDIR] [ARCH=$cpu_arch] [CROSS_COMPILE=$toolchain] UAPI=$uapi_dir [OTHER_BUILD_VARS] [goal...]  Main build variables\n    Variable Description     $SRCDIR Path to this source tree   $cpu_arch CPU architecture you build for (\u0026lsquo;arm\u0026rsquo;, \u0026lsquo;arm64\u0026rsquo;, \u0026lsquo;x86\u0026rsquo;)   $toolchain Optional prefix of the binutils filename (e.g. \u0026lsquo;arm-linux-gnueabihf-\u0026rsquo;, \u0026lsquo;aarch64-linux-gnu-')     Other build variables\n    Variable Description Default     D={0|1} Disable or enable debug build, i.e. -g -O0 vs -O2 0   O=$output_dir Generate binary output files into $output_dir .   V={0|1} Set build verbosity level, 0 is terse 0   DESTDIR=$install_dir Install library and binaries into $install_dir /usr/evl     Make goals\n    Goal Action     all generate all binaries (library, utilities and tests)   clean remove the build files   install do all, copying the generated system binaries to $DESTDIR in the process   install_all install, copying all the generated binaries including the tidbits    Cross-compiling EVL Let\u0026rsquo;s say the library source code is located at ~/git/libevl, and the kernel sources featuring the EVL core is located at ~/git/linux-evl.\nCross-compiling EVL and installing the resulting library and utilities to a staging directory located at /nfsroot/\u0026lt;machine\u0026gt;/usr/evl would amount to this:\n Cross-compiling from a separate build directory\n # First create a build directory the where output files should go $ mkdir /tmp/build-imx6q \u0026amp;\u0026amp; cd /tmp/build-imx6q # Then start the build+install process $ make -C ~/git/libevl O=$PWD ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- UAPI=~/git/linux-evl DESTDIR=/nfsroot/imx6q/usr/evl install or,\n Cross-compiling from the EVL library source tree\n $ mkdir /tmp/build-hikey $ cd ~/git/libevl $ make O=/tmp/build-hikey ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- UAPI=~/git/linux-evl DESTDIR=/nfsroot/hikey/usr/evl install  This is good practice to always generate the build output files to a separate build directory using the O= directive on the make command line, not to clutter your source tree with those. Generating output to a separate directory also creates convenience Makefiles on the fly in the output tree, which you can use to run subsequent builds without having to mention the whole series of variables and options on the make command line again.\n Native EVL build Conversely, you may want to build EVL natively on the target system. Installing the resulting library and utilities directly to their final home located at e.g. /usr/evl can be done as follows:\n Building natively from a build directory\n $ mkdir /tmp/build-native \u0026amp;\u0026amp; cd /tmp/build-native $ make -C ~/git/libevl O=$PWD UAPI=~/git/linux-evl DESTDIR=/usr/evl install or,\n Building natively from the EVL library source tree\n $ mkdir /tmp/build-native $ cd ~/git/libevl $ make O=/tmp/build-native UAPI=~/git/linux-evl DESTDIR=/usr/evl install Testing the installation At this point, you really want to test the EVL installation.\n Last modified: Mon, 21 Jun 2021 14:06:41 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/oob-drivers/dma/",
	"title": "DMA",
	"tags": [],
	"description": "",
	"content": "A cornerstone of many real-time capable drivers which can process requests from the out-of-band stage is DMA support. Being able to offload data transfers to a DMA unit goes a long way toward implementing efficient acquisition loops, especially if they have to run at high frequency, sparing precious CPU cycles.\nIn order to support out-of-band transactions, a DMA controller (e.g. bcm2835-dma, or imx-sdma) must include Dovetail-specific changes. Since DMA drivers are commonly based on the virtual channel layer (aka virt-dma), Dovetail adds the required changes to this layer in order to cope with execution from the out-of-band stage. The current list of DMA controllers which support out-of-band transactions is available at this location.\nOut-of-band DMA channel Dovetail introduces a simple interface for switching a DMA channel to out-of-band mode, based on special flags passed to any of the supported prep calls, such as dmaengine_prep_slave_sg(), dmaengine_prep_slave_single() or dmaengine_prep_dma_cyclic(). This mode remains active until dmaengine_terminate_async() (or the deprecated dmaengine_terminate_all()) routine is called for the channel, which is therefore reserved for out-of-band operations until terminated.\nTwo modes of operation are available for running a DMA channel out-of-band:\n  the common cyclic mode, where an I/O peripheral is driving the data transaction, triggering transfers repeatedly on a periodic basis. If the DMA_OOB_INTERRUPT flag is passed to dmaengine_prep_dma_cyclic(), the DMA completion callback set for the TX descriptor (see struct dma_async_tx_descriptor) is called at the end of each cycle from the out-of-band stage, so that such callback can wake up real-time capable tasks without involving any non-deterministic in-band activity. A typical use case is that an application on the Linux side with strict low latency timing requirement runs the data exchanged with an audio codec through some processing pipeline.\n  the new pulsed mode introduced by Dovetail, in which the kernel software wants to trigger every data transfer manually and repeatedly, acting as the master side of the transaction. This mode should be available from any type of DMA transaction the out-of-band capable driver supports, but the cyclic one. It is turned on by passing the DMA_OOB_PULSE flag to the corresponding prep call, such as dmaengine_prep_slave_sg() or dmaengine_prep_slave_single(). Once the transfer is completed, any callback set for the TX descriptor is fired from the out-of-band stage. The key aspect is the ability to trigger multiple transfers and receive the corresponding completion events directly from the out-of-band stage using a single TX descriptor, which is different from the usual case in which every TX descriptor describes a single transfer which might be delayed by other pending operations. A typical use case is with implementing closed-control loops driven by the Linux system with stringent timing requirements, for instance over a SPI bus.\n  Programming a DMA channel for out-of-band operation Cyclic transaction The way to set up a cyclic transaction with out-of-band completion events is straightforward:\n  create a cyclic DMA transaction via dmaengine_prep_dma_cyclic(), OR\u0026rsquo;ing the DMA_OOB_INTERRUPT flag into the flags parameter. This flag ensures that the completion callback set into the DMA TX descriptor is called from the out-of-band stage.\n  submit the transaction by a call to dmaengine_submit(),\n  finally bring this transaction to active state by issuing the pending DMA requests the way you would do for common transactions using dma_async_issue_pending(). Once your out-of-band transaction is picked by the DMA engine for execution, it has a lock on the channel until terminated.\n   Setting up a DMA transaction for cyclic out-of-band transfers\n Pulsed mode transaction A DMA transaction of pulsed transfers is a common slave transaction for the most part, except that each transfer must be started manually by a call to dma_pulse_oob() for the related channel. To set it up, you need to:\n  create a slave DMA transaction via dmaengine_prep_slave_sg() or dmaengine_prep_slave_single() for instance, OR\u0026rsquo;ing the DMA_OOB_PULSE flag into the flags parameter. This flag ensures that:\n  each transfer in either direction can only be started at your command by a call to dma_pulse_oob() instead of leaving this decision to the DMA engine,\n  the completion callback set into the DMA TX descriptor is called from the out-of-band stage each time such a transfer has completed.\n    submit the transaction by a call to dmaengine_submit(),\n  finally bring this transaction to active state by issuing the pending DMA requests the way you would do for common transactions using dma_async_issue_pending(). Once your out-of-band transaction is picked by the DMA engine for execution, it has a lock on the channel until terminated.\n   Setting up a DMA transaction for pulsed out-of-band transfers\n   int dma_pulse_oob(struct dma_chan *chan)  This routine triggers the next transfer on a DMA channel set up for a pulsed mode transaction, either for sending or receiving data depending on the prep call.\nchanThe DMA channel descriptor for which to start the next transfer.\n\nZero is returned on success, otherwise:\n -EIO. No transfer from an out-of-band transaction is ready to be started manually. Typically, chan was not previously switched to out-of-band operation mode by submitting a TX descriptor bearing DMA_OOB_PULSE, followed by a call to dmaengine_submit(). You may also need to tell the DMA engine to start processing the pending transactions by calling dma_async_issue_pending() right after submitting the TX descriptor.   An out-of-band completion handler executes in out-of-band IRQ context, so you may only run code which is legit there.\n Last modified: Sat, 17 Apr 2021 11:55:02 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/function_index/",
	"title": "Kernel function index",
	"tags": [],
	"description": "",
	"content": " Last modified: Fri, 13 Mar 2020 12:44:32 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/function_index/",
	"title": "libevl function index",
	"tags": [],
	"description": "",
	"content": "Thread services   #usvctable { width: 30%; } #usvctable td { text-align: center; }   Function name EVL Switch1 non-EVL2   evl_attach_thread() N/A \u0026#9989;   evl_attach_self() N/A \u0026#9989;   evl_detach_thread() \u0026#9661; \u0026#10005;   evl_detach_self() \u0026#9661; \u0026#10005;   evl_demote_thread() \u0026#9866; \u0026#9989;   evl_get_self() \u0026#9866; \u0026#10005;   evl_get_state() \u0026#9866; \u0026#9989;   evl_get_thread_mode() \u0026#9866; \u0026#9989;   evl_is_inband() \u0026#9866; \u0026#9989;   evl_set_thread_mode() \u0026#9866; \u0026#9989;   evl_subscribe() \u0026#9661; \u0026#10005;   evl_switch_inband() \u0026#9661; \u0026#10005;   evl_switch_oob() \u0026#9651; \u0026#10005;   evl_unblock_thread() \u0026#9866; \u0026#9989;   evl_unsubscribe() \u0026#9661; \u0026#10005;    Scheduler services   Function name EVL Switch1 non-EVL2   evl_get_schedattr() \u0026#9866; \u0026#9989;   evl_set_schedattr() \u0026#9866; \u0026#9989;  evl_control_sched() \u0026#9866; \u0026#9989;   evl_get_cpustate() \u0026#9866; \u0026#9989;   evl_yield() \u0026#9651; \u0026#10005;    Clock services   Function name EVL Switch1 non-EVL2   evl_new_clock() \u0026#9661; \u0026#9989;   evl_read_clock() \u0026#9866; \u0026#9989;   evl_set_clock() \u0026#9866; \u0026#9989;   evl_get_clock_resolution() \u0026#9866; \u0026#9989;   evl_sleep_until() \u0026#9651; \u0026#10005;   evl_udelay() \u0026#9651; \u0026#10005;    Timer services   Function name EVL Switch1 non-EVL2   evl_new_timer() \u0026#9661; \u0026#9989;   evl_set_timer() \u0026#9866; \u0026#9989;   evl_get_timer() \u0026#9866; \u0026#9989;    Mutex services   Function name EVL Switch1 non-EVL2   evl_create_mutex() \u0026#9661; \u0026#9989;   evl_new_mutex() \u0026#9661; \u0026#9989;   evl_open_mutex() \u0026#9661; \u0026#9989;   evl_lock_mutex() \u0026#9651;4 \u0026#10005;   evl_trylock_mutex() \u0026#9651;4 \u0026#10005;   evl_timedlock_mutex() \u0026#9651;4 \u0026#10005;   evl_unlock_mutex() \u0026#9866;3 \u0026#10005;   evl_get_mutex_ceiling() \u0026#9866; \u0026#9989;   evl_set_mutex_ceiling() \u0026#9866; \u0026#9989;   evl_close_mutex() \u0026#9661; \u0026#9989;    Event services   Function name EVL Switch1 non-EVL2   evl_create_event() \u0026#9661; \u0026#9989;   evl_new_event() \u0026#9661; \u0026#9989;   evl_open_event() \u0026#9661; \u0026#9989;   evl_wait_event() \u0026#9866;4 \u0026#10005;   evl_timedwait_event() \u0026#9866;4 \u0026#10005;   evl_signal_event() \u0026#9866;4 \u0026#10005;   evl_signal_thread() \u0026#9866;4 \u0026#10005;   evl_broadcast_event() \u0026#9866;4 \u0026#10005;   evl_close_event() \u0026#9661; \u0026#9989;    Flags services   Function name EVL Switch1 non-EVL2   evl_create_flags() \u0026#9661; \u0026#9989;   evl_new_flags() \u0026#9661; \u0026#9989;   evl_open_flags() \u0026#9661; \u0026#9989;   evl_wait_flags() \u0026#9651;4 \u0026#10005;   evl_timedwait_flags() \u0026#9651;4 \u0026#10005;   evl_trywait_flags() \u0026#9866;4 \u0026#9989;   evl_peek_flags() \u0026#9866; \u0026#9989;   evl_post_flags() \u0026#9866;4 \u0026#9989;   evl_close_flags() \u0026#9661; \u0026#9989;    Semaphore services   Function name EVL Switch1 non-EVL2   evl_create_sem() \u0026#9661; \u0026#9989;   evl_new_sem() \u0026#9661; \u0026#9989;   evl_open_sem() \u0026#9661; \u0026#9989;   evl_get_sem() \u0026#9651;4 \u0026#10005;   evl_timedget_sem() \u0026#9651;4 \u0026#10005;   evl_tryget_sem() \u0026#9866;4 \u0026#9989;   evl_peek_sem() \u0026#9866; \u0026#9989;   evl_put_sem() \u0026#9866;4 \u0026#9989;   evl_close_sem() \u0026#9661; \u0026#9989;    Observable services   Function name EVL Switch1 non-EVL2   evl_create_observable() \u0026#9661; \u0026#9989;   evl_new_observable() \u0026#9661; \u0026#9989;   evl_read_observable() \u0026#9651;5 \u0026#9989;   evl_update_observable() \u0026#9651;5 \u0026#9989;    Polling services   Function name EVL Switch1 non-EVL2   evl_new_poll() \u0026#9661; \u0026#9989;   evl_add_pollfd() \u0026#9651; \u0026#10005;   evl_mod_pollfd() \u0026#9651; \u0026#10005;   evl_del_pollfd() \u0026#9651; \u0026#10005;   evl_poll_sem() \u0026#9651; \u0026#10005;   evl_timedpoll_sem() \u0026#9651; \u0026#10005;    Memory heap services   Function name EVL Switch1 non-EVL2   evl_init_heap() \u0026#9661; \u0026#9989;   evl_extend_heap() \u0026#9866; \u0026#10005;   evl_alloc_block() \u0026#9651;3 \u0026#10005;   evl_free_block() \u0026#9651;3 \u0026#10005;   evl_check_block() \u0026#9651;3 \u0026#10005;   evl_destroy_heap() \u0026#9661; \u0026#9989;   evl_heap_raw_size() \u0026#9866; \u0026#9989;   evl_heap_size() \u0026#9866; \u0026#9989;   evl_heap_used() \u0026#9866; \u0026#9989;    Proxy services   Function name EVL Switch1 non-EVL2   evl_create_proxy() \u0026#9661; \u0026#9989;   evl_new_proxy() \u0026#9661; \u0026#9989;   evl_send_proxy() \u0026#9866; \u0026#9989;   evl_vprint_proxy() \u0026#9866; \u0026#9989;   evl_print_proxy() \u0026#9866; \u0026#9989;   evl_printf() \u0026#9866; \u0026#9989;    Cross-buffer services   Function name EVL Switch1 non-EVL2   evl_create_xbuf() \u0026#9661; \u0026#9989;   evl_new_xbuf() \u0026#9661; \u0026#9989;    I/O services   Function name EVL Switch1 non-EVL2   oob_read() \u0026#9651; \u0026#10005;   oob_write() \u0026#9651; \u0026#10005;   oob_ioctl() \u0026#9651; \u0026#10005;    Tube services   Function name EVL Switch1 non-EVL2   evl_init_tube() \u0026#9866; \u0026#9989;   evl_send_tube() \u0026#9866; \u0026#9989;   evl_receive_tube() \u0026#9866; \u0026#9989;   evl_get_tube_size() \u0026#9866; \u0026#9989;   evl_init_tube_rel() \u0026#9866; \u0026#9989;   evl_send_tube_rel() \u0026#9866; \u0026#9989;   evl_receive_tube_rel() \u0026#9866; \u0026#9989;   evl_get_tube_size_rel() \u0026#9866; \u0026#9989;    Misc routines   Function name EVL Switch1 non-EVL2   evl_init() N/A \u0026#9989;   evl_get_version() \u0026#9866; \u0026#9989;   evl_sigdebug_handler() N/A \u0026#9989;    1 Defines the stage switching behavior for EVL threads:\n  △ the core may promote the caller to the out-of-band execution stage if running in-band at the time of the call.\n  ▽ the core may demote the caller to the in-band execution stage if running out-of-band at the time of the call.\n  ⚊ the call does not entail any stage switch.\n  2 Whether this call is also available to non-EVL threads, i.e. threads not attached to the EVL core.\n3 Except if the caller undergoes the SCHED_WEAK policy, in which case it is switched back to in-band mode if it has released the last EVL mutex it holds by the end of the call.\n4 As an exception, if this synchronization object was statically initialized (EVL_*_INITIALIZER()), this routine may switch the caller to the in-band stage in order to finalize the construction before carrying out the requested operation. This is required only once in the object\u0026rsquo;s lifetime.\n5 If the caller undergoes the SCHED_WEAK policy, or is not attached to the core, this system call is directly handled from the in-band stage. In all other cases, the caller may be switched to the out-of-band execution stage.\n Last modified: Thu, 30 Apr 2020 15:53:14 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": " The EVL project is now Xenomai 4. The EVL core was originally a fork of the Xenomai 3 Cobalt core, which went through a significant overhaul to make it SMP-scalable, easier to grasp and maintain too. However, both still share the same principles when it comes to the dual kernel technique. The Xenomai project is adopting EVL\u0026rsquo;s Dovetail pipeline interface for its upcoming 3.2 release. In the same move, EVL becomes the Xenomai 4 project.\n   A dual kernel architecture. Like its predecessors in the Xenomai core series, Xenomai 4 with the EVL core brings real-time capabilities to Linux by embedding a companion core into the kernel, which specifically deals with tasks requiring ultra low and bounded response time to events. For this reason, this approach is known as a dual kernel architecture, delivering stringent real-time guarantees to some tasks alongside rich operating system services to others. In this model, the general purpose kernel and the real-time core operate almost asynchronously, both serving their own set of tasks, always giving the latter precedence over the former.\n  In order to achieve this, the Xenomai 4 project works on three components:\n  a piece of inner kernel code - aka Dovetail - which acts as an interface between the companion core and the general purpose kernel. This layer introduces a high-priority execution stage on which tasks with real-time requirements should run. In other words, with the Dovetail code in, the Linux kernel can plan for running out-of-band tasks in a separate execution context which is not subject to the common forms of serialization the general purpose work has to abide by (e.g. interrupt masking, spinlocks).\n  a compact and scalable real-time core, which is intended to serve as a reference implementation for other dual kernel systems based on Dovetail.\n  a library known as libevl, which enables invoking the real-time core services from applications.\n  Originally, the Dovetail code base forked off of the I-pipe interface back in 2015, mainly to address fundamental maintenance issues with respect to tracking the most recent kernel releases. In parallel, a simplified variant of the Cobalt core once known as \u0026lsquo;Steely\u0026rsquo; was implemented in order to experiment freely with Dovetail as the latter significantly departed from the I-pipe interface. At some point, Steely evolved into an even simpler, basic and more scalable real-time core which now serves as the reference implementation for Dovetail, known as the EVL core which is at the heart of the Xenomai 4 effort today.\nWhen is a dual kernel architecture a good fit? A dual kernel system should keep the functional overlap between the general purpose kernel and the companion core minimal, only to hand over the time-critical workload to a dedicated component which is simple and decoupled enough from the rest of the system for you to trust. Typical applications best-served by such infrastructure have to acquire data from external devices with only small jitter within a few tenths of microseconds (absolute worst case) once available, process such input over POSIX threads which can meet real-time requirements by running on the high-priority stage, offloading any non-(time-)critical work to common threads. Generally speaking, such approach like the EVL core implements may be a good fit for the job in the following cases:\n  if your application needs ultra-low response times and/or strictly limited jitter in a reliable fashion. Reliable as in « not impacted by any valid kernel or user code the general purpose kernel might run in parallel in a way which could prevent stringent real-time deadlines from being met ». Valid code in this case meaning not causing machine crashes; the companion core of a dual kernel system is not sensitive to slowdowns which might be induced by poorly written general purpose drivers. For instance, a low priority workload can put a strain on the CPU cache subsystem, causing delays for the real-time activities when it resumes for handling some external event:\n  if this workload manipulates a large data set continuously, causing frequent cache evictions. As the outer cache in the hierarchy is shared between CPUs, a ripple effect does exist on all of them, including the isolated ones.\n  if this workload involves many concurrent threads causing a high rate of context switches, which may get even worse if those threads belong to different processes (i.e. having distinct address spaces).\nThe small footprint of the dedicated core helps in this case, since less code and data are involved in managing the real-time system as a whole, lowering the impact of unfavorable cache conditions. In addition, the small core does not have to abide by the locking rules imposed on the general purpose kernel code when scheduling its threads. Instead, it may preempt it at any time, based on the interrupt pipelining technique Dovetail implements. This [piece of information]({{ relref \u0026ldquo;core/benchmarks/_index.md#stress-load\u0026rdquo; }}) describes typical runtime situations when the general purpose workload is putting pressure on the overall system, regardless of the relative priority of its tasks.\n    if your application system design requires the real-time execution path to be logically isolated from the general purpose activities by construction, so as not to share critical subsystems like the common scheduler.\n  if resorting to CPU isolation in order to mitigate the adverse effect the non real-time workload might have on the real-time side is not an option, or once tested, is not good enough for your use case. Obviously, using such trick with low-end hardware on a single-core CPU would not fly since at least one non-isolated CPU must be available to the kernel for carrying out the system housekeeping duties.\n  What does a dual kernel architecture require from your application? In order to meet the deadlines, a dual kernel architecture requires your application to exclusively use the dedicated system call interface the real-time core implements. With the EVL core, this API is libevl, or any other high-level API based on it which abides by this rule. Any regular Linux system call issued while running on the high-priority stage would automatically demote the caller to the low priority stage, dropping real-time guarantees in the process.\n This rule has consequences when using C++ for instance, which requires to define the set of usable classes and runtime features which may be available to the application.\n This said, your application may use any service from your favorite standard C/C++ library outside of the time-critical context.\nGenerally speaking, a clear separation of the real-time workload from the rest of the implementation in your application is key. Having such a split in place should be a rule of thumb regardless of the type of real-time approach including with native preemption, it is crucial with a dual kernel architecture. Besides, this calls for a clear definition of the (few) interface(s) which may exist between the real-time and general purpose tasks, which is definitely the right thing to do.\nThe basic execution unit the EVL core recognizes for delivering its services in user space is the thread, which commonly translates as POSIX threads in user space.\nWhich API should be used for implementing real-time device drivers? The EVL core exports a kernel API for writing drivers, extending the Linux device driver interface so that applications can benefit from out-of-band services delivered from the high-priority execution stage.\nThe basic execution unit the EVL core recognizes for delivering its services in kernel space is the kthread, which is a common Linux kthread on EVL steroids.\nWhat is the dual kernel code footprint? All figures reported in the charts below below have been determined by CLOC, only retaining C and assembly source files in the comparisons.\n Dovetail footprint on kernel code. The code footprint of the Dovetail interface is 7.8 Kloc added to the kernel as of v5.7-rc5. Most changes happen in the generic kernel and driver code, which amount for 73% of the total. The rest is split into ARM, arm64 and x86-specific code. An architecture-specific port represents less than 10% of the total on average.\n      EVL core footprint on kernel code. The EVL core on top of Dovetail is 15.2 Kloc. 97% of this code is architecture-agnostic. Each architecture port amounts for 1% of the rest, which is 163 lines of code on average. This shows that Dovetail is actually responsible for the overwhelming majority of the architecture-specific support a companion core should need.\n      Overall dual kernel code footprint. The overall footprint of the EVL dual kernel system amounts to 26 Kloc, which includes Dovetail, the EVL core and its out-of-band capable drivers so far. This is 0.13% of the total kernel code base as of v5.7-rc5.\n      Comparing I-pipe and Dovetail footprints. These figures compare the latest I-pipe implementation available to date based on kernel v4.19.x with Dovetail for v5.7-rc5. Dovetail provides additional core services such as built-in out-of-band task scheduling support which, as a consequence companion cores don\u0026rsquo;t have to implement for each CPU architecture. The ARM-specific code is notably smaller for Dovetail, thanks to a better integration of the interrupt pipeline logic within the mainline kernel.\n      Comparing Xenomai 3 Cobalt and Xenomai 4 EVL core footprints. These figures compare Cobalt 3.1 with the EVL core for kernel v5.7-rc5. The drastic reduction of the code footprint the EVL core shows is mainly due to focusing on a simpler yet flexible feature set and reusing the common driver model. Besides, most of the architecture-specific code is handled by Dovetail, unlike Cobalt which still has to deal with the nitty-gritty details of task switching, like FPU management.\n     Porting Dovetail If you intend to port Dovetail to:\n  some arbitrary kernel flavor or release.\n  an unsupported hardware platform.\n  another CPU architecture.\n  Then you could make good use of the following information:\n  first of all, the EVL development process is described in this document. You will need this information in order to track the EVL development your own work is based on.\n  detailed information about porting the Dovetail interface to another CPU architecture for the Linux kernel is given here.\n  the current collection of « rules of thumb » when it comes to developing software on top of EVL\u0026rsquo;s dual kernel infrastructure.\n  Implementing your own companion core If you plan to develop your own core to embed into the Linux kernel for running POSIX threads on the high-priority stage Dovetail introduces, you can use the EVL core implementation as a reference code with respect to interfacing your work with the general purpose kernel. To help you further in this task, you can refer to the following sections of this site:\n  all documentation sections mentioned earlier about porting Dovetail.\n  a description of the so-called alternate scheduling scheme, by which Linux kernel threads and POSIX threads may gain access to the high-prority execution stage in order to benefit from the real-time scheduling guarantees.\n  a developing series of technical documents which navigates you through the EVL core implementation.\n  Running the EVL core Recipe for the impatient   read this document about building the EVL core and libevl.\n  boot your EVL-enabled kernel.\n  write your first application code using the libevl API. You may find the following bits useful, particularly when discovering the system:\n  what does initializing an EVL application entail.\n  how to have POSIX threads run on the high-priority stage.\n  which is the proper calling context for each EVL service from this API.\n    calibrate and test the system.\n  For the rest of us The process for getting the EVL core running on your target system can be summarized as follows (click on the steps to open the related documentation):\ngraph LR; S(\"Build libevl\") -- X[\"Install libevl\"] style S fill:#99ccff; click S \"/core/build-steps#building-libevl\" X -- A[\"Build kernel\"] click A \"/core/build-steps#building-evl-core\" A -- B[\"Install kernel\"] style A fill:#99ccff; B -- C[\"Boot target\"] style C fill:#ffffcc; C -- U[\"Run evl check\"] style U fill:#ffffcc; click U \"/core/commands#evl-check-command\" U -- UU{OK?} style UU fill:#fff; UU --|Yes| D[\"Run unit tests\"] UU --|No| R[Fix Kconfig] click R \"/core/caveat\" style R fill:#99ccff; R -- A style D fill:#ffffcc; click D \"/core/testing#evl-unit-testing\" D -- L{OK?} style L fill:#fff; L --|Yes| E[\"Test with 'hectic'\"] L --|No| Z[\"Report upstream\"] style Z fill:#ff420e; click E \"/core/testing#hectic-program\" click Z \"https://xenomai.org/mailman/listinfo/xenomai/\" E -- M{OK?} style M fill:#fff; M --|Yes| F[\"Calibrate timer\"] M --|No| Z click F \"/core/runtime-settings#calibrate-core-timer\" style E fill:#ffffcc; F -- G[\"Test with 'latmus'\"] style F fill:#ffffcc; style G fill:#ffffcc; click G \"/core/testing#latmus-program\" G -- N{OK?} style N fill:#fff; N --|Yes| O[\"Go celebrate\"] N --|No| Z style O fill:#33cc66;  Once the EVL core runs on your target system, you can go directly to step #3 of the quick recipe above.\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/file/",
	"title": "File description",
	"tags": [],
	"description": "",
	"content": "An OOB-capable driver is a device driver which implements low latency operations on files like oob_read(), oob_write() and oob_ioctl(), in addition to a set of common in-band operations such as open(2), close(2), read(2), write(2), ioctl(2) etc. Out-of-band I/O requests are issued by EVL threads running in user-space. Those requests are directly handled from the out-of-band execution stage, ensuring low and bounded latency.\nEVL files are kernel file objects (struct file) which have been advertised by an OOB-capable driver as supporting these out-of-band I/O operations, in addition to common in-band requests. For instance, oob_ioctl() requests may be issued for an EVL file, like ioctl(2) may as well. Such advertisement usually happens in the open() operation handler of a character device driver by calling evl_open_file() for the new file description being initialized. Drivers which create (pseudo-)file objects with out-of-band support on-the-fly from their ioctl(2) handler may call evl_open_file() from this context as necessary.\nA typical example of the latter usage can be found in the EVL-enabled GPIOLIB driver, which advertises out-of-band support for files created by the GPIO_GET_LINEEVENT_IOCTL request from the character device interface to GPIO lines. A more usual way of advertising EVL files can be seen from the latmus_open() routine of the latmus driver code which supports the latmus application..\n Device file management services   int evl_open_file(struct evl_file *efilp, struct file *filp)  Advertise filp as a file which can be subject to out-of-band handling. filp is a pointer to a common file description (struct file) usually received by the open() operation handler of a character device driver. evl_open_file() extends this in-band description with an out-of-band context stored at efilp. Once evl_open_file() has returned, applications can send out-of-band I/O requests to this driver via any file descriptor which refers to filp. evl_open_file() does not block, it merely initializes efilp, binds it to filp then returns.\n*efilp is usually part of the private data structure a driver associates with each active file, as illustrated by the following example:\n #include \u0026lt;slab.h\u0026gt; #include \u0026lt;evl/file.h\u0026gt; struct foo_private_state { /* ... */ struct evl_file efile; }; static int foo_open(struct inode *inode, struct file *filp) { struct foo_private_state *p; int ret; p = kzalloc(sizeof(*p), GFP_KERNEL); if (p == NULL) return -ENOMEM; ret = evl_open_file(\u0026amp;p-\u0026gt;efile, filp); if (ret) { kfree(p); return ret;\t} filp-\u0026gt;private_data = p; return 0; }  efilpA pointer to an EVL file context (struct evl_file), which is initialized by this call. This memory object must live as long as filp is valid.\n\nfilpThe pointer to the file description structure received from the VFS by the open() operation handler of a character device driver.\n\nevl_open_file() always succeeds, returning zero in the current implementation. However, you may still want to check the return code in case error conditions surface in a later revision of this code.\n  void evl_release_file(struct evl_file *efilp)  This is the converse operation to evl_open_file(), which must be called by the release() handler of an OOB-capable character device driver.\nBecause the in-band kernel and the EVL core run almost fully asynchronously, some out-of-band I/O operations - which the in-band kernel does not know about - might still be pending for efilp when the release() handler is invoked by the in-band kernel. For instance, some application which issued an oob_read() request might still be waiting for data in the oob_read() operation handler of the driver, sleeping on an EVL wait queue or a semaphore, when some other thread of the program closes all the file descriptors referring to that file, leading to the release() handler to be called by the VFS. In order to track those operations, EVL maintains an internal reference count on every device file registered with evl_open_file(). evl_release_file() waits for this reference count to reach zero before returning to the caller, acting as a synchronization barrier with respect to any concurrent out-of-band operation which might be running on any CPU. When this routine returns, you can be sure that no such operation is ongoing in the system for the released file, therefore it can be safely dismantled afterwards.\nFor this reason, you do want to call evl_release_file() before the context data attached to the file being released is freed by the release() handler eventually . The following example illustrates such precaution, considering that some out-of-band operations might still be in-flight while the device file is released by the in-band kernel, such as waiting on an EVL semaphore.\n#include \u0026lt;slab.h\u0026gt; #include \u0026lt;evl/flag.h\u0026gt; #include \u0026lt;evl/file.h\u0026gt; struct foo_private_state { /* ... */ struct evl_flag eflag; struct evl_file efile; }; static int foo_open(struct inode *inode, struct file *filp) { struct foo_private_state *p; int ret; p = kzalloc(sizeof(*p), GFP_KERNEL); if (p == NULL) return -ENOMEM; ret = evl_open_file(\u0026amp;p-\u0026gt;efile, filp); if (ret) { kfree(p); return ret;\t} evl_init_flag(\u0026amp;p-\u0026gt;eflag); filp-\u0026gt;private_data = p; return 0; } static int foo_release(struct inode *inode, struct file *filp) { struct foo_private_state *p = filp-\u0026gt;private_data; /* * Flush and destroy the flag some out-of-band operation(s) * might still be pending on. Some EVL thread(s) might be * unblocked from oob_read() as a result of this call. */ evl_destroy_flag(\u0026amp;p-\u0026gt;eflag); /* * Synchronize with these operations. Unblocked threads will drop * any pending reference on the file being released, eventually * allowing this call to return. */ evl_release_file(\u0026amp;p-\u0026gt;efile); /* * Neither in-band nor out-of-band users of this file anymore * for sure, we can free the per-file private context. */ kfree(p); return 0; }  efilpA pointer to the EVL file context (struct evl_file), which was initialized by a previous call to evl_open_file().\n\nevl_release_file() may block before releasing the file until all references to efilp are gone, which indicates that no out-of-band operation is undergoing for this EVL file.\n  struct evl_file *evl_get_file(unsigned int fd)  Search for the EVL file indexed on fd and get a reference on it atomically, which prevents its release by evl_release_file() until the last reference is dropped by a call to evl_put_file().\nInternally, the EVL core holds a reference on any file subject to an out-of-band request across the call to the corresponding operation handler in the driver, so that such file does not go stale unexpectedly while the request is still ongoing. You may want to get additional references on EVL files from any EVL driver which receives file descriptors directly from applications via ioctl(2) arguments. Such reference should be held as long as the operation on the underlying EVL file is not complete.\nThe implementation of the polling services in the EVL core illustrates how evl_get_file() is used for retrieving EVL file descriptions from descriptors passed by the application.\n fdA valid file descriptor referring to the EVL file.\n\nThis call returns a valid pointer to an active EVL file - such as efilp passed to evl_open_file())\n holding a reference on such file in the same move.    void evl_get_fileref(struct evl_file *efilp)  Get a reference on the EVL file efilp. This is the core helper which actually gets a reference on any EVL file. Once it has returned, the file cannot be released by evl_release_file() until the reference is dropped by a converse call to evl_put_file().\nefilpA pointer to a valid EVL file description. This description was originally initialized by a call to evl_open_file().\n\n  void evl_put_file(struct evl_file *efilp)  Drop a reference on an EVL file previously obtained by a call to evl_get_fileref() or evl_get_file().\nIf the reference count is zero at the time evl_release_file() is called for efilp, the release is performed immediately.\n Last modified: Sat, 05 Jun 2021 16:47:53 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/socket/",
	"title": "Socket interface",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://the-going.github.io/website/core/oob-drivers/spi/",
	"title": "SPI",
	"tags": [],
	"description": "",
	"content": "EVL provides support for running high frequency SPI transfers which are useful in implementing closed-loop control systems. Applications manage the out-of-band transfers from user space via requests sent to the SPIDEV driver, which exports a user-space API to reach the SPI devices overs a given bus. To this end, EVL makes a few of strong assumptions:\n  a DMA is available for transferring the data over the SPI bus between the controller and the device. Since configuring the DMA cannot be done from the out-of-band stage, selecting the SPI device to talk to over the bus involves stopping the real-time operations, in order to switch to the in-band execution stage. Fortunately, most use cases which require high frequency, ultra-low latency transfers involve talking to a single device over a dedicated SPI bus. In this case, the device selection and DMA configuration need to be done only once, when setting up the communication.\n  the data to be exchanged with the SPI device is stored into a fixed-size memory buffer, shared between the kernel and the application. The buffer is split in two parts: an input area containing the data received from the remote device during the last transfer cycle, and an output area containing the data to be sent to that device during the same cycle. The shared memory is coherent, CPU cache is disabled. The DMA is managed in pulsed out-of-band mode from the SPIDEV interface.\n  while operating in out-of-band mode, the SPI bus cannot be used for regular in-band traffic.\n  only the SPI master mode is supported for out-of-band operations.\n  Enabling the out-of-band SPI capabilities is threefold, this requires:\n  adding the out-of-band management logic to the generic SPI master framework. This is done once and readily available from EVL.\n  having the SPIDEV driver export the out-of-band I/O interface to applications, which is also done once and readily available from EVL.\n  adding the needed bits to the SPI controller driver which manages the particular controller chip to be used for out-of-band I/O. This is the part you may have to implement for your own SPI controller of choice. The list of SPI controllers EVL currently extends with out-of-band capabilities is visible at this location.\n  Adding out-of-band capabilities to a SPI controller As stated in the introduction, the out-of-band I/O logic slips into the regular Linux device driver model as much as possible, without imposing a separate driver stack. In the SPI case, this means extending the generic SPI framework with a small set of operations which control the bus from the out-of-band execution stage. The handlers for those operations must be added to the struct spi_controller descriptor as follows:\n .prepare_oob_transfer should finish the setup for preparing a particular SPI bus for out-of-band transfers. This is called once, after the generic SPI framework has created the shared memory area, and configured the DMA. This is the right place to provide for any setup which is specific to out-of-band I/O, or any additional sanity check which is specific to the controller in such a context. For instance, the controller could make sure the size of the data frame to send/receive at each transfer is within the bounds supported by the hardware. This is an in-band operation, the SPI bus is locked for out-of-band traffic which means that regular in-band request to the same bus will have to wait until it leaves the out-of-band mode. This handler runs upon request from the application to enable out-of-band mode (SPI_IOC_ENABLE_OOB_MODE) for a given SPI bus.   Example: the prepare_oob_transfer() handler for the BCM2835 chip\n static int bcm2835_spi_prepare_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { /* * The size of a transfer is limited by DLEN which is 16-bit * wide, and we don't want to scatter transfers in out-of-band * mode, so cap the frame size accordingly. */ if (xfer-\u0026gt;setup.frame_len \u0026gt; 65532) return -EINVAL; return 0; }  .start_oob_transfer should select the SPI device to talk to, set the SPI communication settings in the controller (e.g. speed, word size), then turn on the DMA operations on the configured channels Since the DMA is set in pulsed mode, no transfer takes place yet, but once start_oob_transfer() has returned, everything should be ready to trigger them simply by pulsing the DMA. Like prepare_oob_transfer(), this is an in-band operation which is invoked by the SPIDEV driver, upon request from the application (SPI_IOC_ENABLE_OOB_MODE). The bus is first prepared for out-of-band operations, before these are effectively started.   Example: the start_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_start_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); struct spi_device *spi = xfer-\u0026gt;spi; u32 cs = bs-\u0026gt;prepare_cs[spi-\u0026gt;chip_select], effective_speed_hz; unsigned long cdiv; /* See bcm2835_spi_prepare_message(). */ bcm2835_wr(bs, BCM2835_SPI_CS, cs); cdiv = bcm2835_get_clkdiv(bs, xfer-\u0026gt;setup.speed_hz, \u0026amp;effective_speed_hz); xfer-\u0026gt;effective_speed_hz = effective_speed_hz; bcm2835_wr(bs, BCM2835_SPI_CLK, cdiv); bcm2835_wr(bs, BCM2835_SPI_DLEN, xfer-\u0026gt;setup.frame_len); if (spi-\u0026gt;mode \u0026amp; SPI_3WIRE) cs |= BCM2835_SPI_CS_REN; bcm2835_wr(bs, BCM2835_SPI_CS, cs | BCM2835_SPI_CS_TA | BCM2835_SPI_CS_DMAEN); }  .pulse_oob_transfer is called when the application asks for triggering the next transfer by a request to the SPIDEV driver (SPI_IOC_RUN_OOB_XFER). This handler is called to apply the controller-specific tweaks which might be needed before the generic SPI framework pulses the DMA, causing the I/O operation to take place. This is an out-of-band operation; what this handler may do is restricted to the set of calls available to the out-of-band execution stage (reading/writing a couple of I/O registers should be enough in most cases here).   Example: the pulse_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_pulse_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); /* Reload DLEN for the next pulse. */ bcm2835_wr(bs, BCM2835_SPI_DLEN, xfer-\u0026gt;setup.frame_len); }  .terminate_oob_transfer should stop and disable out-of-band DMA operations for the controller. This handler is called when the generic SPI framework was told by the SPIDEV driver to leave the out-of-band management mode for the controller (SPI_IOC_DISABLE_OOB_MODE). This is an in-band operation. Once the out-of-band mode is left, the bus is available for regular in-band traffic anew.   Example: the terminate_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_reset_hw(struct bcm2835_spi *bs) { u32 cs = bcm2835_rd(bs, BCM2835_SPI_CS); /* Disable SPI interrupts and transfer */ cs \u0026amp;= ~(BCM2835_SPI_CS_INTR | BCM2835_SPI_CS_INTD | BCM2835_SPI_CS_DMAEN | BCM2835_SPI_CS_TA); /* * Transmission sometimes breaks unless the DONE bit is written at the * end of every transfer. The spec says it's a RO bit. Either the * spec is wrong and the bit is actually of type RW1C, or it's a * hardware erratum. */ cs |= BCM2835_SPI_CS_DONE; /* and reset RX/TX FIFOS */ cs |= BCM2835_SPI_CS_CLEAR_RX | BCM2835_SPI_CS_CLEAR_TX; /* and reset the SPI_HW */ bcm2835_wr(bs, BCM2835_SPI_CS, cs); /* as well as DLEN */ bcm2835_wr(bs, BCM2835_SPI_DLEN, 0); } static void bcm2835_spi_terminate_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); bcm2835_spi_reset_hw(bs); } Eventually, the SPI controller should fill the corresponding function pointers into its descriptor with the address of the out-of-band handlers.\n Example: declaring the out-of-band handlers for the BCM2835 chip\n static int bcm2835_spi_probe(struct platform_device *pdev) { struct spi_controller *ctlr; struct bcm2835_spi *bs; int err; ctlr = devm_spi_alloc_master(\u0026amp;pdev-\u0026gt;dev, ALIGN(sizeof(*bs), dma_get_cache_alignment())); if (!ctlr) return -ENOMEM; platform_set_drvdata(pdev, ctlr); ... ctlr-\u0026gt;prepare_oob_transfer = bcm2835_spi_prepare_oob_transfer; ctlr-\u0026gt;start_oob_transfer = bcm2835_spi_start_oob_transfer; ctlr-\u0026gt;pulse_oob_transfer = bcm2835_spi_pulse_oob_transfer; ctlr-\u0026gt;terminate_oob_transfer = bcm2835_spi_terminate_oob_transfer; ... }  Last modified: Sat, 05 Jun 2021 16:47:53 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/init/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Initializing libevl When initializing, the core EVL library undertakes the following steps once for any given application process:\n  it sets up the specific bits for the CPU architecture, which boils down to resolving the address of the vDSO for the current process.\n  it locks the current and future process memory by a call to mlockall(2), so that we won\u0026rsquo;t get page faults due to on-demand memory schemes such as the copy-on-write technique.\n  it connects to the EVL core running in kernel space, performing a few sanity checks such as making sure the core can handle the ABI the application will use later on to issue EVL system calls.\n  it initializes a couple of built-in proxy streams, such as the one evl_printf() needs to send zero-latency output to stdout from the out-of-band context.\n  All these actions are performed once by a bootstrap routine named evl_init(), which may be either called explicitly by the application code - usually at startup on behalf of the main() thread - or indirectly as a result of invoking evl_attach_self() for the first time. Threads put aside, any attempt to create other EVL objects before evl_init() has run leads to the -ENXIO error.\n Explicit initialization from the main() entry point\n \t#include \u0026lt;error.h\u0026gt; #include \u0026lt;evl/evl.h\u0026gt; int main(int argc, char *const argv[]) { int ret; ret = evl_init(); if (ret) error(1, -ret, \u0026quot;evl_init() failed\u0026quot;); ... return 0; }  Indirect initialization via evl_attach_self()\n \t#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;error.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; int main(int argc, char *const argv[]) { int efd; efd = evl_attach_self(\u0026quot;me-with-pid:%d\u0026quot;, getpid()); if (efd \u0026lt; 0) error(1, -efd, \u0026quot;evl_attach_self() failed\u0026quot;); ... return 0; } Startup-time services   int evl_init(void)  This function boostraps the EVL services for the current process as explained in the introduction. Only the first call to this routine applies, subsequent calls to evl_init() are silently ignored, returning the status code of the initial call.\nOn success, this service returns zero. Otherwise, a negated error code is returned:\n  -EPERM\tThe caller is not allowed to lock memory via a call to mlockall(2). Since memory locking is a requirement for running EVL threads, no joy.\n  -ENOMEM\tNo memory available, whether the kernel could not lock down all of the calling process\u0026rsquo;s virtual address space into RAM, or some other reason related to some process or driver eating way too much virtual or physical memory.\tYou may start panicking.\n  -ENOSYS\tThe EVL core is not enabled in the running kernel.\n  -ENOEXEC\tThe EVL core in the running kernel exports a different ABI level than the ABI libevl.so requires. To fix this error, you need to rebuild libevl.so against the UAPI exported by the EVL core running on the target system.\n   Last modified: Thu, 23 Apr 2020 14:51:48 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/oob-drivers/gpio/",
	"title": "GPIO",
	"tags": [],
	"description": "",
	"content": "Dealing with GPIOs from the out-of-band execution stage enables the application to always respond to external signals within a few microseconds regardless of the in-band workload running in parallel on the system. Enabling CONFIG_GPIOLIB_OOB in the kernel configuration turns on such such capability in the regular GPIOLIB driver, which depends on the EVL core. The out-of-band GPIO support is available to applications using a couple of additional I/O requests to the character device interface exported by this driver to application code running in user-space (i.e. based on ioctl(2) and read(2) calls for the most part). This inner interface is tersely documented, but you may find your way by having a look at this demo code available from the mainline kernel tree.\nThis driver interface is used by the libgpiod API internally.\n Out-of-band GPIO operations in a nutshell As stated in the introduction, the out-of-band I/O logic slips into the regular Linux device driver model as much as possible, without imposing a separate driver stack. In the GPIO case, we have been able to add the out-of-band support to an existing driver such as the GPIOLIB core instead of providing a dedicated driver.\nThis translates as follows:\n  Common GPIO handling operations in this driver are extended with the specific GPIOHANDLE_REQUEST_OOB flag, which tells the GPIOLIB core about our intent to operate a GPIO pin directly from the out-of-band execution stage, for input and/or output. Line set up and configuration are still done using the regular ioctl(2) interface, since these are in-band operations by design.\n  Waiting for a GPIO event is done by calling the oob_read() I/O service, instead of read(2).\n  Toggling a GPIO state is done by calling the oob_ioctl() I/O service, instead of ioctl(2).\n  Out-of-band line event and line handle requests In order to use the out-of-band GPIO features, one simply needs to add the GPIOHANDLE_REQUEST_OOB flag defined by the EVL core to the common GPIOHANDLE_REQUEST_INPUT, GPIOHANDLE_REQUEST_OUTPUT operation flags, when issuing the GPIO_GET_LINEEVENT_IOCTL and GPIO_GET_LINEHANDLE_IOCTL ioctl(2) requests respectively. For instance, this is a fragment of code adapted from the latmus application which measures response time to GPIO events:\n#include \u0026lt;sys/ioctl.h\u0026gt; #include \u0026lt;linux/gpio.h\u0026gt; #include \u0026lt;uapi/evl/devices/gpio.h\u0026gt; static void setup_gpio_pins(int *fds) { struct gpiohandle_request out; struct gpioevent_request in; int ret; in.handleflags = GPIOHANDLE_REQUEST_INPUT | GPIOHANDLE_REQUEST_OOB; in.eventflags = GPIOEVENT_REQUEST_RISING_EDGE; in.lineoffset = gpio_inpin; /* Input pin number */ strcpy(in.consumer_label, \u0026quot;latmon-pulse\u0026quot;); ret = ioctl(gpio_infd, GPIO_GET_LINEEVENT_IOCTL, \u0026amp;in); if (ret) error(1, errno, \u0026quot;ioctl(GPIO_GET_LINEEVENT_IOCTL)\u0026quot;); /* in.fd now contains the oob-capable line event descriptor. */ out.lineoffsets[0] = gpio_outpin; /* Output pin number */ out.lines = 1; out.flags = GPIOHANDLE_REQUEST_OUTPUT | GPIOHANDLE_REQUEST_OOB; out.default_values[0] = 1; strcpy(out.consumer_label, \u0026quot;latmon-ack\u0026quot;); ret = ioctl(gpio_outfd, GPIO_GET_LINEHANDLE_IOCTL, \u0026amp;out); if (ret) error(1, errno, \u0026quot;ioctl(GPIO_GET_LINEHANDLE_IOCTL)\u0026quot;); /* out.fd now contains the oob-capable line handle descriptor. */ fds[0] = in.fd; fds[1] = out.fd; } Once a file descriptor is obtained on the GPIO controller - like /dev/gpiochip0 - for input (gpio_infd) and output (gpio_outfd), the application may ask for:\n  a line event descriptor for receiving GPIO interrupts directly from the out-of-band stage, by waiting on the oob_read() I/O service.\n  a line handle descriptor for changing the state of GPIO pins directly from the out-of-band stage, by calling the oob_ioctl() I/O service, with the GPIOHANDLE_SET_LINE_VALUES_IOCTL request code.\n  A thread which responds to GPIO events on an input pin by flipping the state of an output pin, all from the out-of-band stage could look like this:\nstatic void *gpio_responder_thread(void *arg) { struct gpiohandle_data data = { 0 }; struct gpioevent_data event; const int ackval = 0;\t/* Remote observes falling edges. */ int fds[2], efd, ret; setup_gpio_pins(fds); /* * Attach to the EVL core so that we may issue out-of-band * requests. */ efd = evl_attach_self(\u0026quot;/gpio-responder:%d\u0026quot;, getpid()); if (efd \u0026lt; 0) error(1, -efd, \u0026quot;evl_attach_self() failed\u0026quot;); /* * Loop: wait for the next event, then trigger an edge by flipping * the pin state. */ for (;;) { data.values[0] = !ackval; ret = oob_ioctl(fds[1], GPIOHANDLE_SET_LINE_VALUES_IOCTL, \u0026amp;data); if (ret) error(1, errno, \u0026quot;ioctl(GPIOHANDLE_SET_LINE_VALUES_IOCTL) failed\u0026quot;); /* Wait for the next interrupt on the input pin. */ ret = oob_read(fds[0], \u0026amp;event, sizeof(event)); if (ret != sizeof(event)) break; /* Start flipping the output pin. */ data.values[0] = ackval; ret = oob_ioctl(fds[1], GPIOHANDLE_SET_LINE_VALUES_IOCTL, \u0026amp;data); if (ret) error(1, errno, \u0026quot;ioctl(GPIOHANDLE_SET_LINE_VALUES_IOCTL) failed\u0026quot;); } return NULL; }  Last modified: Sat, 05 Jun 2021 16:47:53 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/runtime-settings/",
	"title": "Runtime configuration",
	"tags": [],
	"description": "",
	"content": "Calibrating the core timer When enabled in the kernel, EVL transparently controls the hardware timer chip via a proxy device, serving all timing requests including those originating from the in-band kernel logic. In order to maximize the timing accuracy, EVL needs to figure out the basic latency of the target platform.\nUpon receipt from an interrupt, the time spent traversing the kernel code from the low-level entry code until the interrupt handler installed by some driver is invoked is shorter than the time that would be required for a kernel thread to resume on such event instead. It would take even more time for a user-space thread to resume, since this may entail changing the current memory address space, which may involve time-consuming MMU-related operations affecting the CPU caches.\nThis basic latency may be increased by multiple factors such as:\n bus or CPU cache latency, delay required to program the timer chip for the next shot, code running with interrupts disabled on the CPU to receive the IRQ, inter-processor serialization within the EVL core (hard spinlocks).  In order to deliver events as close as possible to the ideal time, EVL defines the notion of clock gravity, which is a set of static adjustment values to account for the basic latency of the target system for responding to timer events from any given clock device, as perceived by the client code waiting for these wake up events. For this reason, a clock gravity is defined as a triplet of values, which indicates the amount of time by which every timer shot should be anticipated from, depending on the target context it should activate, either IRQ handler, kernel or user thread.\nWhen started with the -t option, latmus runs a series of tests for determining those best calibration values for the EVL core timer, then tells the core to use them.\nA typical output of this command looks like this:\n Complete core timer calibration\n # latmus -t == latmus started for core tuning, period=1000 us (may take a while) irq gravity...2000 ns kernel gravity...5000 ns user gravity...6500 ns == tuning completed after 34s You might want to restrict the calibration process to specific context(s), in which case you should pass the corresponding context modifiers to the latmus command, such as -u for user-space and -i for IRQ latency respectively:\n Context-specific calibration\n # latmus -tui == latmus started for core tuning, period=1000 us (may take a while) irq gravity...1000 ns user gravity...6000 ns == tuning completed after 21s  You might get gravity triplets differing from a few microseconds between runs of the same calibration process: this is normal, and nothing to worry about provided all those values look close enough to the expected jitter on the target system. The reason for such discrepancies is that although latmus does run the same tests time and again, the conditions on the target system may be different between runs, leading to slightly varying results (e.g. variations in CPU cache performance for the calibration loop).\n  Last modified: Sun, 13 Dec 2020 19:00:43 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/testing/",
	"title": "Testing the installation",
	"tags": [],
	"description": "",
	"content": "EVL comes with a series of tests you can run to make sure the core is performing correctly on your target system.\nUnit testing A series of unit testing programs is produced in $prefix/tests as part of building libevl. You should run each of them to make sure everything is fine. The simplest way to do this is as follows:\n Running the EVL unit tests\n # evl test duplicate-element: OK monitor-pp-dynamic: OK monitor-pi: OK clone-fork-exec: OK clock-timer-periodic: OK poll-close: OK sem-wait: OK monitor-pp-raise: OK monitor-pp-tryenter: OK heap-torture: OK monitor-pp-lower: OK poll-read: OK monitor-deadlock: OK monitor-wait-multiple: OK monitor-event: OK proxy-eventfd: OK monitor-flags.eshi: OK monitor-wait-multiple.eshi: OK sem-wait.eshi: OK detach-self.eshi: OK sem-timedwait.eshi: OK proxy-pipe.eshi: OK clock-timer-periodic.eshi: OK proxy-eventfd.eshi: OK monitor-event.eshi: OK heap-torture.eshi: OK poll-sem.eshi: OK poll-nested.eshi: OK sem-close-unblock: OK monitor-steal: OK basic-xbuf: OK simple-clone: OK monitor-flags: OK poll-sem: OK sem-timedwait: OK mapfd: OK proxy-pipe: OK poll-flags: OK poll-nested: OK monitor-pp-pi: OK fault: OK monitor-pi-deadlock: OK detach-self: OK monitor-pp-nested: OK monitor-pp-weak: OK stax-lock: OK fpu-preload: OK A few tests from the test suite may fail in case some kernel support is missing in order to support them, like:\nsched-quota-accuracy.c:213: FAILED: evl_control_sched(44, \u0026amp;p, \u0026amp;q, test_cpu) (=Operation not supported) sched-quota-accuracy: no kernel support In the example above, sched-quota-accuracy failed because CONFIG_EVL_SCHED_QUOTA was not set in the kernel configuration. Likewise, sched-tp-accuracy requires CONFIG_EVL_SCHED_TP to be enabled in the kernel configuration.\nThe test loop aborts immediately upon a test failure. You may disable this behavior by running evl test -k (i.e. keep going) instead.\n hectic: hammering the EVL context switching machinery By default, the hectic program runs a truckload of EVL threads both in user and kernel spaces, for exercising the scheduler of the autonomous core. In addition, this test can specifically stress the floating-point management code to make sure the FPU is shared flawlessly between out-of-band and in-band thread contexts.\nTo get this test running, you will need CONFIG_EVL_HECTIC to be enabled in the kernel configuration, and loaded into the kernel under test if you built it as a dynamic module.\n # /usr/evl/bin/hectic -s 200 == Testing FPU check routines... == FPU check routines: OK. == Threads: switcher_ufps0-0 rtk0-1 rtk0-2 rtup0-3 rtup0-4 rtup_ufpp0-5 rtup_ufpp0-6 rtus0-7 rtus0-8 rtus_ufps0-9 rtus_ufps0-10 rtuo0-11 rtuo0-12 rtuo_ufpp0-13 rtuo_ufpp0-14 rtuo_ufps0-15 rtuo_ufps0-16 rtuo_ufpp_ufps0-17 rtuo_ufpp_ufps0-18 fpu_stress_ufps0-19 switcher_ufps1-0 rtk1-1 rtk1-2 rtup1-3 rtup1-4 rtup_ufpp1-5 rtup_ufpp1-6 rtus1-7 rtus1-8 rtus_ufps1-9 rtus_ufps1-10 rtuo1-11 rtuo1-12 rtuo_ufpp1-13 rtuo_ufpp1-14 rtuo_ufps1-15 rtuo_ufps1-16 rtuo_ufpp_ufps1-17 rtuo_ufpp_ufps1-18 fpu_stress_ufps1-19 switcher_ufps2-0 rtk2-1 rtk2-2 rtup2-3 rtup2-4 rtup_ufpp2-5 rtup_ufpp2-6 rtus2-7 rtus2-8 rtus_ufps2-9 rtus_ufps2-10 rtuo2-11 rtuo2-12 rtuo_ufpp2-13 rtuo_ufpp2-14 rtuo_ufps2-15 rtuo_ufps2-16 rtuo_ufpp_ufps2-17 rtuo_ufpp_ufps2-18 fpu_stress_ufps2-19 switcher_ufps3-0 rtk3-1 rtk3-2 rtup3-3 rtup3-4 rtup_ufpp3-5 rtup_ufpp3-6 rtus3-7 rtus3-8 rtus_ufps3-9 rtus_ufps3-10 rtuo3-11 rtuo3-12 rtuo_ufpp3-13 rtuo_ufpp3-14 rtuo_ufps3-15 rtuo_ufps3-16 rtuo_ufpp_ufps3-17 rtuo_ufpp_ufps3-18 fpu_stress_ufps3-19 RTT| 00:00:01 RTH|---------cpu|ctx switches|-------total RTD| 0| 568| 568 RTD| 3| 853| 853 RTD| 2| 739| 739 RTD| 1| 796| 796 RTD| 0| 627| 1195 RTD| 2| 1258| 1997 RTD| 3| 1197| 2050 RTD| 1| 1311| 2107 RTD| 0| 627| 1822 RTD| 2| 1250| 3247 RTD| 3| 1254| 3304 RTD| 1| 1254| 3361 RTD| 2| 1254| 4501 RTD| 1| 1254| 4615 RTD| 0| 684| 2506 RTD| 3| 1311| 4615 RTD| 3| 1256| 5871 RTD| 2| 1311| 5812 RTD| 0| 684| 3190 RTD| 1| 1311| 5926 ... latmus: the litmus test for latency If you plan for measuring the worst case latency on your target system, you should run the evl check command on such system in order to detect any obvious misconfiguration of the kernel early on.\n With the sole -m option or without any argument, the latmus application runs a 1Khz sampling loop, collecting the min, max and average latency values obtained for an EVL thread running in user-space which responds to timer events. This is a basic latency benchmark which does not require any additional interrupt source beyond the on-chip hardware timer readily available to the kernel.\nIn addition, you can use this application to measure the response time of a thread running in user-space to external interrupts, specifically to GPIO events. This second call form is selected by the -Z and -z option switches.\nFinally, passing -t starts a calibration of the EVL core timer, finding the best configuration values.\nUnless you only plan to measure in-band response time to GPIO events, you will need CONFIG_EVL_LATMUS to be enabled in the kernel configuration to run the timer calibration or the response to timer test. This driver must be loaded into the kernel under test if you built it as a dynamic module. For those familiar with Xenomai 3 Cobalt, this program combines and extends the features of the latency and autotune utilities.\n latmus accepts the following arguments, given as short or long option names:\n-i --irqCollect latency figures or tune the EVL core timer from the context of an in-kernel interrupt handler.\n\n-k --kernelCollect latency figures or tune the EVL core timer from the context of a kernel-based EVL thread.\n\n-u --userCollect latency figures or tune the EVL core timer from the context of an EVL thread running in user-space. This is the default mode, in absence of -i and -k.\n\n-s --sirqMeasure the delay between the moment a synthetic interrupt is posted from the out-of-band stage and when it is eventually received by its in-band handler. When measured under significant workload pressure, this gives the worst case interrupt latency experienced by the in-band kernel due to local interrupt disabling (i.e. stalling the in-band pipeline stage). Therefore, this has nothing to do with the much shorter and bounded interrupt latency observed from the out-of-band stage by EVL applications.\n\n-r --resetReset the gravity values of the EVL core timer to their factory defaults. These defaults are statically defined by the EVL platform code.\n\n-q --quietTame down verbosity of the test to the bare minimum, only the final latency report will be issued when in effect. Passing this option requires a timeout to be set with the -T option.\n\n-b --backgroundRun the test in the shell\u0026rsquo;s background. All output is suppressed until the final latency report.\n\n-K --keep-goingKeep the execution going upon unexpected switch to in-band mode of the responder thread. Normally, any switch to in-band mode from the thread responding to timer/GPIO events would cause the execution to stop with an error message, since the latency figures would be tainted by a transition to the non real-time context. This option tells latmus to keep going regardless; it only makes sense for debugging purpose, when collecting latency figures from an EVL thread running in user-space (i.e. -u).\n\n-m --measureMeasure the response time to timer events. In addition to this option, -i, -k and -u select a specific measurement context, -u applies by default. Measurement of response time to timer events is the default mode, in absence of the -t, -Z and -z options on the command line.\n\n-t --tuneRun a core timer calibration procedure. -i, -k and -u can be used to select a specific tuning context, all of them are applied in sequence otherwise. See below. This option is mutually exclusive with -m, -Z and -z.\n\n-p --period=\u0026lt;µsecs\u0026gt;Set the sampling period to \u0026lt;µsecs\u0026gt;. By default, 1000 is used (one tick every millisecond or 1Khz). The slowest sampling period is 1000000 (1Hz).\n\n-T --timeout=\u0026lt;duration\u0026gt;[dhms]The duration of the test, excluding the one second warmup period. This option enables a timeout which stops the test automatically after the specified runtime has elapsed. By default, the test runs indefinitely, or until ^C is pressed. The duration is interpreted according to the modifier suffix, as a count of days, minutes, hours or seconds. In absence of modifier, seconds are assumed.\n\n-A --maxlat-abort=\u0026lt;maxlat\u0026gt;Automatically abort the test whenever the max latency figure observed exceeds \u0026lt;maxlat\u0026gt;.\n\n-v --verbose=\u0026lt;level\u0026gt;Set the verbosity level to \u0026lt;level\u0026gt;. Setting 0 is identical to entering quiet mode with -q. Any non-zero value is considered when tuning the EVL core timer (-t option), to control the amount of debug information the latmus companion driver sends to the kernel log. Defaults to 1, maximum is 2.\n\n-l --lines=\u0026lt;count\u0026gt;Set the number of result lines per page. In measurement mode (-m), a new result header is output after every \u0026lt;count\u0026gt; result lines.\n\n-g --plot=\u0026lt;file\u0026gt;Dump an histogram of the collected latency values to \u0026lt;file\u0026gt; in a format which is easily readable by the gnuplot utility.\n\n-H --histogram=\u0026lt;cells\u0026gt;Set the number of cells in the histogram, each cell covers one microsecond of additional latency from 1 to \u0026lt;cells\u0026gt; microseconds. This value is used only if -g is given on the command line. Defaults to 200, covering up to 200 microseconds in worst-case latency, which should never be as high on any target platform with EVL.\n\n-P --priority=\u0026lt;prio\u0026gt;Set the scheduling priority of the responder thread in the SCHED_FIFO class. This option only makes sense when collecting latency figures or tuning the EVL core timer from an EVL thread context (i.e. -u or -k). Defaults to 90.\n\n-c --cpu=\u0026lt;nr\u0026gt;Set the CPU affinity of the responder thread. This option only makes sense when collecting latency figures or tuning the EVL core timer from an EVL thread context (i.e. -u or -k). Defaults to 0.\n\n-Z --oob-gpio=\u0026lt;host\u0026gt;Start an out-of-band test measuring the response time to GPIO events from the out-of-band stage, i.e. relying on real-time capabilities of the EVL core. The argument is the host name or IPv4 addresses of the remote board which monitors the response time from the SUT running the latmus application. This option must be associated with -I and -O to specify the GPIO chip(s) and pin numbers to use.\n\n-z --inband-gpio=\u0026lt;host\u0026gt;Start an in-band test measuring the response time to GPIO events in plain in-band mode. The argument is the host name or IPv4 address of the remote board which monitors the response time from the SUT running the latmus application. This option must be associated with -I and -O to specify the GPIO chip(s) and pin numbers to use.\n\n-I --gpio-in=\u0026lt;gpiochip-name\u0026gt;,\u0026lt;pin-number\u0026gt;[,rising-edge|falling-edge]Specify the GPIO chip and pin number to be used for receiving the GPIO pulses from the remote monitor board. Optionally, you can select whether GPIO events should be triggered on the rising edge (default) or falling edges of GPIO signals. This option only makes sense whenever -Z or -z are present on the command line too.\n\n-O --gpio-out=\u0026lt;gpiochip-name\u0026gt;,\u0026lt;pin-number\u0026gt;Specify the GPIO chip and pin number to be used for acknowledging the GPIO pulses received from the monitor board. This option only makes sense whenever -Z or -z are present on the command line too.\n\nIf latmus fails starting with an Invalid argument error, double-check the CPU number passed to -c if given. The designated CPU must be part of the out-of-band CPU set known to the EVL core. Check this file /sys/devices/virtual/evl/control/cpus to know which CPUs are part of this set.\n  Last modified: Sun, 06 Jun 2021 18:28:51 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/",
	"title": "The EVL core",
	"tags": [],
	"description": "",
	"content": "Pitching the real-time EVL core For certain types of applications, offloading a particular set of time-critical tasks to an autonomous software core embedded into the Linux kernel may deliver the best performance at the lowest engineering and runtime costs in comparison to imposing real-time behavior on the whole kernel logic in order to meet the deadlines which only those tasks have, like the native preemption model requires.\nIn a nutshell, the Xenomai 4 project is about introducing a simple, scalable and dependable dual kernel architecture for Linux, based on the Dovetail interface for coupling a high-priority software core to the main kernel. This interface is showcased by a real-time core delivering basic services to applications via a straightforward API. The EVL core is an ongoing development toward a production-ready real-time infrastructure, which can also be a starting point for other flavours of dedicated software core embedded into the Linux kernel. This work is composed of:\n  the Dovetail interface, which introduces a high-priority execution stage into the main kernel logic, where a functionally-independent software core runs.\n  the EVL core which delivers dependable low-latency services to applications which have to meet real-time requirements. Applications are developed using the common Linux programming model.\n  an in-depth documentation which covers both Dovetail and the EVL core, with many cross-references between them, so that engineers can use the EVL core to support a real-time application, improve it, or even implement their own software core of choice on top of Dovetail almost by example.\n  What we are looking for:\n  Low engineering and maintenance costs. Working on EVL should only require common kernel development knowledge, and the code footprint and complexity must remain tractable for small development teams (currently about 20 KLOC, which is not even half the size of the Xenomai 3 Cobalt core.\n  Low runtime cost. Reliable, ultra low and bounded response time for the real-time workload including on low-end, single-core hardware with minimum overhead, leaving plenty of CPU cycles for running the general purpose workload concurrently.\n  High scalability. From single core to high-end multi-core machines running real-time workloads in parallel with low and bounded latency. Running these workloads on isolated CPUs significantly improves the worst-case latency figure in SMP configurations, but if your fixture only has one of them, the EVL core should still be able to deliver on ultra low and bounded latency.\n  Low configuration. We want very few to no runtime tweaks at all to be required to ensure the real-time workload is not affected by the regular, general purpose workload. Once enabled in the kernel, the EVL core should be ready to deliver.\n  Make it ordinary, make it simple The EVL core is a dedicated software core which is embedded into the kernel, delivering real-time services to applications with stringent timing requirements. This small core is built like any ordinary feature of the Linux kernel, not as a foreign extension slapped on top of it. Dovetail plays an important part here, as it hides the nitty-gritty details of embedding a companion core into the kernel. Its fairly low code footprint and limited complexity makes it a good choice as a plug-and-forget real-time infrastructure, which can also be used as a starting point for custom core implementations. The following figures have been obtained from the CLOC tool counting the lines of source code from the RTAI, Xenomai 3 Cobalt and Xenomai 4 EVL core implementation respectively:\nThe user-space interface to this core is the EVL library (libevl.so), which implements the basic system call wrappers, along with the fundamental thread synchronization services. No bells and whistles, only the basics. The intent is to provide simple mechanisms, complex semantics and policies can and should be implemented in high level APIs based on this library running in userland.\nElements As the name suggests, elements are the basic features we may require from the EVL core for supporting real-time applications in this dual kernel environment. Also, only the kernel could provide such features in an efficient way, pure user-space code could not deliver. The EVL core defines six elements:\n  Thread. As the basic execution unit, we want it to be runnable either in real-time mode or regular GPOS mode alternatively, which exactly maps to Dovetail\u0026rsquo;s out-of-band and in-band contexts.\n  Monitor. This element has the same purpose than the main kernel\u0026rsquo;s futex, which is about providing an integrated - although much simpler - set of fundamental thread synchronization features. Monitors are used internally by the EVL library to implement mutexes, condition variables, event flag groups and semaphores in user-space.\n  Clock. We may find platform-specific clock devices in addition to the core ones defined by the architecture, for which ad hoc drivers should be written. The clock element ensures all clock drivers present the same interface to applications in user-space. In addition, this element can export individual software timers to applications which comes in handy for running periodic loops or waiting for oneshot events on a specific time base.\n  Observable. This element is the building block event-driven applications can use for implementing the observer design pattern, in which any number of observer threads can be notified of updates to any number of observable subjects, in a loosely coupled fashion.\n  Cross-buffer. A cross-buffer (aka xbuf) is a bi-directional communication channel for exchanging data between out-of-band and in-band thread contexts, without impacting the real-time performance on the out-of-band side. Any kind of thread (EVL or regular) can wait/poll for input from the other side. Cross-buffers serve the same purpose than Xenomai 3\u0026rsquo;s message pipes implemented by the XDDP socket protocol.\n  File proxy. Linux-based dual kernel systems are nasty by design: the huge set of GPOS features is always visible to applications but they should not to use it when they carry out real-time work with the help of the autonomous core, or risk unbounded response time. Because of such exclusion, issuing I/O file requests such as calling printf(3) should not be done directly from time-critical loops. A file proxy solves such issue by offloading I/O operations on in-band files to dedicated workers, keeping the caller on the out-of-band execution stage.\n  Everything is a file Each resource exported by EVL to applications is represented by a file. In addition, each EVL element is associated to a kernel device object:\n  Since all EVL resources are backed by a kernel file internally, the hard work of managing their lifetime, preventing stale references by tracking their users, is left to the VFS.\n  Applications can create public or private elements. A public element appears in the EVL device file hierarchy, which enables multi-process applications to share elements.\n  EVL elements benefit from the permission control, monitoring and auditing logic which come with the file semantics.\n  udev rules can be attached to events of interest which might happen for any element. Additionally, the internal kernel state of elements is exported to user space via the /sys filesystem.\n  EVL device drivers are (almost) common drivers EVL does not introduce any specific driver model. It exports a dedicated kernel API for implementing real-time I/O operations in common character device drivers. In fact, the EVL core is composed of a set of such drivers, implementing each class of elements.\nEVL also provides a way to extend existing socket protocol families with out-of-band I/O capabilities, or add your own protocols via the new PF_OOB family.\n Last modified: Sun, 06 Jun 2021 18:28:51 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/commands/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "The \u0026lsquo;evl\u0026rsquo; command The \u0026lsquo;evl\u0026rsquo; umbrella utility can run the set of base commands available for controlling, inspecting and testing the state of the EVL core and any command matching the \u0026lsquo;evl-*\u0026rsquo; glob pattern which may be reachable from the shell $PATH variable. The way the \u0026lsquo;evl\u0026rsquo; utility centralizes access to a variety of EVL-related commands is very similar to that of git on purpose. Each of the EVL commands is implemented by an external plugin, which can be a mere executable, or a script in whatever language. The only requirement is that the caller must have execute permission on such file to run it.\nThe general syntax is as follows:\nevl [-V] [-P \u0026lt;cmddir\u0026gt;] [-h] [\u0026lt;command\u0026gt; [command-args]]   \u0026lt;command\u0026gt; may be any command word listed by \u0026lsquo;evl -h\u0026rsquo;, such as: check which checks a kernel configuration for common issues ps which reports a snapshot of the current EVL threads test for running the EVL test suite trace which is a simple front-end to the ftrace interface for EVL\n  -P switches to a different installation path for base command plugins, which is located at $prefix/libexec by default.\n  -V displays the version information then exits. The information is extracted from the libevl library the EVL command depends on, displayed in the following format:\nevl.\u0026lt;serial\u0026gt; -- #\u0026lt;git-HEAD-commit\u0026gt; (\u0026lt;git-HEAD-date\u0026gt;) [ABI \u0026lt;revision\u0026gt;]\nwhere \u0026lt;serial\u0026gt; is the libevl serial release number, the \u0026lt;git-HEAD\u0026gt; information refers to the topmost GIT commit which is present in the binary distribution the \u0026lsquo;evl\u0026rsquo; command is part of, and \u0026lt;revision\u0026gt; refers to the kernel ABI this binary distribution is compatible with. For instance:\n  ~ # evl -V evl.0 -- #1c6115c (2020-03-06 16:24:00 +0100) [requires ABI 19]  The information following the double dash may be omitted if the built sources were not version-controlled by GIT.\n  given only -h or without any argument, \u0026lsquo;evl\u0026rsquo; displays this general help, along with a short help string for each of the supported commands found in \u0026lt;cmddir\u0026gt;, such as:  ~ # evl usage: evl [options] [\u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;]] -P --prefix=\u0026lt;path\u0026gt; set command path prefix -V --version print library and required ABI versions -h --help this help available commands: check check kernel configuration gdb debug EVL command plugin with GDB ps report a snapshot of the current EVL threads test run EVL tests trace ftrace control front-end for EVL Checking a kernel configuration (check) evl check may be the very first evl command you should run from a newly installed target system which is going to run the EVL core. This command checks a kernel configuration for common issues which may increase latency. The general syntax is as follows:\n$ evl check [-f --file=\u0026lt;.config\u0026gt;] [-L --check-list=\u0026lt;file\u0026gt;] [-a --arch=\u0026lt;cpuarch\u0026gt;] [-H --hash-size=\u0026lt;N\u0026gt;] [-q --quiet] [-h --help] The kernel configuration to verify is a regular .config file which contains all the settings for building a kernel image. If none is specified using the -f option, the command defaults to reading /proc/config.gz on the current machine. If this fails because any of CONFIG_IKCONFIG or CONFIG_IKCONFIG_PROC was disabled in the running kernel, the command fails.\nThe check list contains a series of single-line assertions which are tested against the contents of the kernel configuration. You can override the default check list stored at $prefix/libexec/kconf-checklist.evl with our own set of checks with the -L option. Each assertion follows the BNF-like syntax below:\nassertion : expr conditions | \u0026quot;!\u0026quot; expr conditions expr : symbol /* matches =y and =m */ | symbol \u0026quot;=\u0026quot; tristate tristate : \u0026quot;y\u0026quot; | \u0026quot;m\u0026quot; | \u0026quot;n\u0026quot; conditions : dependency | dependency arch dependency : \u0026quot;if\u0026quot; symbol /* true if set as y/m */ arch : \u0026quot;on\u0026quot; cputype cputype : $(uname -m) For instance:\n  CONFIG_FOO must be set whenever CONFIG_BAR is unset can be written as CONFIG_FOO if !CONFIG_BAR.\n  CONFIG_FOO must not be set can be written as !CONFIG_FOO, or conversely CONFIG_FOO=n.\n  CONFIG_FOO must be built as module on aarch32 or aarch64 can be written as CONFIG_FOO=m on aarch.\n  CONFIG_FOO must not be built-in on aarch64 if CONFIG_BAR is set can be written as !CONFIG_FOO=y if CONFIG_BAR on aarch.\n  Assertions in the check list may apply to a particular CPU architecture. Normally, the command should be able to figure out which architecture the kernel configuration file applies to by inspecting the first lines, looking for the \u0026ldquo;Linux/\u0026rdquo; pattern. However, you might have to specify this information manually to the command using the -a option if the file referred to by the -f option does not contain such information. The architecture name (cputype) should match the output of $(uname -m) or some abbreviated portion of it. However, arm64 and arm are automatically translated to aarch64 and aarch32 when found in an assertion or passed to the -a option.\nThe default check list translates the configuration-related information gathered in the caveat section as follows:\nCONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y if CONFIG_CPU_FREQ CONFIG_DEBUG_HARD_LOCKS=n CONFIG_ACPI_PROCESSOR_IDLE=n CONFIG_LOCKDEP=n CONFIG_DEBUG_LIST=n CONFIG_DEBUG_VM=n CONFIG_DEBUG_PER_CPU_MAPS=n CONFIG_KASAN=n The command returns the following information:\n  the wrong settings detected in the kernel configuration are written to stdout, unless the quiet -q option was given.\n  the number of failed assertions is returned via the shell exit code ($?).\n   Example: checking the current kernel configuration\n ~ # evl check CONFIG_ACPI_PROCESSOR_IDLE=y ~ # echo $? 1 Reporting a snapshot of the current EVL threads (ps) When you need to know which threads are currently present in your system, evl ps comes in handy. The command syntax - which supports short and long options formats - is as follows:\n$ evl ps [-c --cpu=\u0026lt;cpu\u0026gt;[,\u0026lt;cpu\u0026gt;...]] [-s --state] [-t --times] [-p --policy] [-l --long] [-n --numeric] [-S --sort=\u0026lt;key\u0026gt;] [-h --help] This command fetches the information it needs from the /sysfs attributes the EVL core exports for every thread it manages. The output is organized in groups of values, representing either runtime parameters or statistics for each displayed thread:\n  NAME reports the thread name, as specified in the evl_attach_self() library call.\n  CPU is the processor id. the thread is currently pinned to.\n  PID is the process id. inband-wise, since any EVL thread is originally a regular Linux [k]thread. This value belongs to the global namespace (i.e. task_pid_nr()).\n  SCHED is the current scheduling policy for the thread.\n  PRIO is the priority level in the scheduling policy for the thread.\n  ISW counts the number of inband switches. Under normal circumstances, this count should remain stable over time once the thread has entered its work loop. As the only exception, a thread which undergoes the SCHED_WEAK policy may see this counter progress as a result of calling out-of-band services. For all other scheduling policies, observing any increase in this value after the time-critical loop was entered is a sure sign of a problem in the application code, which might be calling real-time unsafe services when it should not.\n  CTXSW counts the number of context switches performed by the EVL core for the thread. This value is incremented each time the thread resumes from preemption or suspension in the EVL core. CAUTION: this has nothing to do with the context switches performed by the main kernel logic.\n  SYS is the number of out-of-band system calls issued by the thread to the EVL core. Basically, this is the number of out-of-band I/O requests the thread has issued so far, since everything is a file in the EVL core.\n  RWA counts the number of Remote WAkeup signals the EVL core had to send so far for waking up the thread whenever it was sleeping on a remote CPU. For instance, this would happen if two threads running on different CPUs were to synchronize on an EVL event. Waking up a remote thread entails sending an inter-processor interrupt to the CPU that thread sleeps on for kicking the rescheduling procedure, which entails more overhead than a local wakeup. If this counter increases like crazy when your application runs, you might want to check the situation with respect to CPU affinity, to make sure the current distribution of threads over the available CPUs is actually what you want.\n  STAT gives an abbreviated runtime status of the thread as follows:\n \u0026lsquo;w\u0026rsquo;/\u0026lsquo;W\u0026rsquo; ⇾ Waits on a resource with/without timeout (TIMEOUT displays the time before timeout) \u0026lsquo;D\u0026rsquo; ⇾ Delayed (TIMEOUT displays the remaining sleep time) \u0026lsquo;p\u0026rsquo; ⇾ Periodic timeline (kthread only, TIMEOUT displays the remaining time until the next period starts) \u0026lsquo;R\u0026rsquo; ⇾ Ready to run (i.e. neither blocked nor suspended, but waiting for the CPU to be available) \u0026lsquo;X\u0026rsquo; ⇾ Running in-band \u0026lsquo;T\u0026rsquo; ⇾ Ptraced and stopped (whenever traced by a debugger such as gdb[server]) \u0026lsquo;r\u0026rsquo; ⇾ Undergoes round-robin (when SCHED_RR is in effect) \u0026lsquo;S\u0026rsquo; ⇾ Forcibly suspended (cumulative with \u0026lsquo;W\u0026rsquo; state, won\u0026rsquo;t resume until lifted)    TIMEOUT is the remaining time before some timer which was started specifically for the thread fires. Which timer was started depends on the undergoing operation for such thread, which may block until a resource is available, wait for the next period in a timeline and so on. See STAT for details.\n  %CPU is the current amount of CPU horsepower consumed by the thread over the last second. When an out-of-band interrupt preempts a thread, the time spent handling it is charged to that thread.\n  CPUTIME reports the cumulated CPU time already consumed by the thread, using a minutes:milliseconds.microseconds format.\n  The command options allow to select which threads and which data should be displayed:\n the -c option filters the output on the CPU the threads are pinned on. The argument is a comma-separated list of CPU numbers. Ranges are also supported via the usual dash separator. For instance, the following command would report threads pinned on CPU0, and all CPUs from CPU3 to CPU15.  $ evl ps -c 0,3-15   -s includes information about the thread state, which is ISW, CTXSW, SYS, RWA and STAT.\n  -t includes the thread times, which are TIMEOUT, CPU% and CPUTIME.\n  -p includes the scheduling policy information, which is SCHED and PRIO.\n  -l enables the long output format, which is a combination of all information groups.\n  -n selects a numeric output for the STAT field, instead of the one-letter flags. This actually dumps the 32-bit value representing all aspects of a thread status in the EVL core, which contains more information than reported by the abbreviated format. EVL hackers want that.\n  -S sorts the output according to a given sort key in increasing order. The following sort keys are understood:\n \u0026lsquo;c\u0026rsquo; sorts by CPU values \u0026lsquo;i\u0026rsquo; sorts by ISW values \u0026rsquo;t' sorts by CPUTIME values \u0026lsquo;x\u0026rsquo; sorts by CTXSW values \u0026lsquo;w\u0026rsquo; sorts by RWA values \u0026lsquo;r\u0026rsquo; sorts in reverse order (decreasing order)    For instance, the following command would list the times of all threads from CPU14 by decreasing CPU time consumption:\n$ evl ps -t -Srt -c14 CPU PID TIMEOUT %CPU CPUTIME NAME 14 2603 - 0.6 00:435.952 rtup_ufpp14-5:2069 14 2604 - 0.6 00:430.147 rtup_ufpp14-6:2069 14 2599 - 0.5 00:423.118 rtup14-3:2069 14 2600 - 0.5 00:420.293 rtup14-4:2069 14 2595 - 0.3 00:207.143 [rtk1@14:2069] 14 2597 - 0.3 00:204.301 [rtk2@14:2069] 14 2619 - 0.2 00:186.139 rtuo_ufpp14-14:2069 14 2617 - 0.2 00:185.497 rtuo_ufpp14-13:2069 14 2623 - 0.2 00:184.812 rtuo_ufpp_ufps14-18:2069 14 2622 - 0.2 00:184.772 rtuo_ufpp_ufps14-17:2069 14 2621 - 0.2 00:181.692 rtuo_ufps14-16:2069 14 2616 - 0.2 00:181.329 rtuo14-12:2069 14 2615 - 0.2 00:181.230 rtuo14-11:2069 14 2620 - 0.2 00:180.604 rtuo_ufps14-15:2069 14 2572 - 0.0 00:000.006 rtus_ufps13-9:2069 14 2125 - 0.0 00:000.006 rtus1-7:2069 14 2646 - 0.0 00:000.005 rtus15-8:2069 14 2650 - 0.0 00:000.005 rtus_ufps15-10:2069 14 2310 - 0.0 00:000.005 rtus6-7:2069  -h displays the command help string.  Controlling the kernel tracer (trace) The trace command provides a simple front-end for controlling the function tracer which is part of the FTRACE kernel framework, in a way which is Dovetail-aware. We typically use this tracer to analyze high latency spots during the course of the latmus program execution.\nThere is no Dovetail (or EVL-specific) tracer. Latency spots can be analyzed using the common kernel function tracer, which reports additional information about the current execution stage and interrupt state. Trace snapshots are automatically taken at appropriate times by the latmus utility in order to help in such analysis.\n In order to use this tracer, make sure to enable the following features in your kernel build:\n CONFIG_TRACER_SNAPSHOT CONFIG_TRACER_SNAPSHOT_PER_CPU_SWAP CONFIG_FUNCTION_TRACER  The command syntax is as follows:\nevl trace [-e[\u0026lt;trace_group\u0026gt;][-E\u0026lt;tracepoint_file\u0026gt;][-s\u0026lt;buffer_size\u0026gt;][-t]] [-d] [-p] [-f] [-h] [-c \u0026lt;cpu\u0026gt;] [-h]  Arguments to options must immediately follow the option letter, without any spacing in between.\n The command options allow for a straightforward use of the function tracer:\n  -e enables the tracer in the kernel, optionally turning on a trace group, which is a set of pre-defined tracepoints. From this point, FTRACE starts logging information about a set of kernel tracepoints which may be traversed while the system executes.\nUp to libevl r24, this option-less switch enables tracing for out-of-band IRQ events, CPU idling events, and all (in-kernel) EVL core routines.\nSince libevl r25, the name of a trace group can be mentioned right after the option letter, which refers to a pre-defined set of tracepoints. Those tracepoints are listed in a separate file which should be stored at $EVL_CMDDIR/trace.$name. A tracepoint in such file is specified relative to FTRACE\u0026rsquo;s tracing/ hierarchy, such as irq/irq_pipeline_entry, which would refer to $EVL_TRACEDIR/tracing/irq/irq_pipeline_entry. Without argument, -e behaves as -e -f, which enables all kernel tracepoints (see -f).\n  ~# evl trace -eirq tracing enabled ~ # cat /usr/evl/libexec/trace.irq irq/irq_pipeline_entry irq/irq_pipeline_exit irq/irq_handler_entry irq/irq_handler_exit evl/evl_timer_shot evl/evl_trigger evl/evl_latspot You can either extend the set of pre-defined trace groups by adding your own sets to $EVL_CMDDIR, or use the -E option to specify an arbitray tracepoint file.\nIf a particular CPU is mentioned with -c along with -e, then per-CPU tracing is enabled for \u0026lt;cpu\u0026gt;.\n  -E is similar to -e, except that its argument refers to an arbitrary tracepoint file. This is handy for working with your own custom set of tracepoints.\nIf a tracepoint listed in the file is invalid, it is silently ignored.\n  ~# cat \u0026gt; /tmp/custom_traces evl/evl_schedule evl/evl_pick_next evl/evl_switch_context evl/evl_switch_tail evl/evl_finish_wait ^D ~# evl trace -E/tmp/custom_traces tracing enabled   -t turns on the dry run mode for -e and -E, meaning that all commands enabling tracepoints are echoed to the output but not actually applied. This is a quick way to check the sanity of a (custom) tracepoint file.\n  if -f is mentioned, all kernel functions traversed in the course of execution are logged. CAUTION: enabling full tracing may cause a massive overhead.\n  -s changes the size of the FTRACE buffer on each tracing CPU to \u0026lt;buffer_size\u0026gt;. If a particular CPU is mentioned with -c along with -s, then the change is applied to the snapshot buffer of \u0026lt;cpu\u0026gt; only.\n  -d fully disables the tracer which stops logging events on all CPUs.\n  -p prints out the contents of the trace buffer. If a particular CPU is mentioned with -c along with -p, then only the snapshot buffer of \u0026lt;cpu\u0026gt; is dumped.\n  -h displays the command help string.\n  For instance, the following command starts tracing all kernel routines:\n$ evl trace -ef Interpreting the Dovetail-specific trace information The Dovetail-specific information is about:\n  whether the in-band stage is stalled and/or irqs are disabled in the CPU. \u0026rsquo;d' appears in the entry state flags if the in-band stage is stalled while hard irqs are enabled in the CPU, \u0026lsquo;D\u0026rsquo; denotes an unstalled in-band stage with hard irqs off in the CPU, and \u0026lsquo;*\u0026rsquo; denotes a combined stalled in-band stage and hard irqs off in the CPU.\n  whether we are running on the out-of-band stage, if \u0026lsquo;~\u0026rsquo; appears in the entry flags.\n  You may want to read this document for details on the notion of interrupt stage Dovetail implements.\n For instance:\n/* hard irqs off, running in-band */ \u0026lt;...\u0026gt;-4164 [003] D... 122.047972: do_syscall_64 \u0026lt;-entry_SYSCALL_64_after_hwframe /* in-band stalled and hard irqs off, running out-of-band */ \u0026lt;...\u0026gt;-4164 [003] *.~. 122.048021: __evl_schedule \u0026lt;-run_oob_call /* in-band stalled, hard irqs on, running in-band */ \u0026lt;...\u0026gt;-4164 [003] d... 122.048082: rcu_lockdep_current_cpu_online \u0026lt;-rcu_read_lock_sched_held In addition to this basic information, latmus emits a special tracepoint named evl_latspot in the trace event log before taking a trace snapshot, each time the observed maximum latency increases. The frozen trace is visible in the corresponding per-CPU snapshot buffer. From that point, you may be able to backtrack to the source(s) of the extra latency. A typical debug session would look like this:\n~ # evl trace -ef tracing enabled ~ # latmus warming up on CPU1... RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 26.675| 26.951| 27.826| 0| 0| 26.675| 27.826 RTD| 26.712| 27.067| 31.204| 0| 0| 26.675| 31.204 RTD| 26.653| 26.961| 29.160| 0| 0| 26.653| 31.204 RTD| 26.678| 27.067| 29.285| 0| 0| 26.653| 31.204 RTD| 26.759| 27.051| 29.542| 0| 0| 26.653| 31.204 RTD| 26.770| 27.079| 29.266| 0| 0| 26.653| 31.204 ^C---|-----------|-----------|-----------|--------|------|------------------------- RTS| 10.119| 27.029| 31.204| 0| 0| 00:00:06/00:00:06 ~ # evl trace -c 1 ... \u0026lt;idle\u0026gt;-0 [001] *.~. 135.363256: do_trace_write_msr \u0026lt;-__switch_to \u0026lt;idle\u0026gt;-0 [001] *.~. 135.363256: write_msr: c0000100, value 7ff90973e700 timer-responder-234 [001] *.~. 135.363256: switch_fpu_return \u0026lt;-dovetail_context_switch timer-responder-234 [001] *.~. 135.363257: do_raw_spin_unlock \u0026lt;-__evl_schedule timer-responder-234 [001] *.~. 135.363257: do_raw_spin_lock \u0026lt;-evl_wait_schedule timer-responder-234 [001] *.~. 135.363258: do_raw_spin_unlock \u0026lt;-evl_wait_schedule timer-responder-234 [001] *.~. 135.363258: do_raw_spin_lock \u0026lt;-latmus_oob_ioctl timer-responder-234 [001] *.~. 135.363258: do_raw_spin_unlock \u0026lt;-latmus_oob_ioctl timer-responder-234 [001] d.~. 135.363259: evl_oob_sysexit: result=0 timer-responder-234 [001] d.~. 135.363262: pipeline_syscall \u0026lt;-do_syscall_64 timer-responder-234 [001] d.~. 135.363262: handle_oob_syscall \u0026lt;-pipeline_syscall timer-responder-234 [001] d.~. 135.363263: do_oob_syscall \u0026lt;-handle_oob_syscall timer-responder-234 [001] d.~. 135.363263: evl_oob_sysentry: syscall=oob_ioctl timer-responder-234 [001] d.~. 135.363264: EVL_ioctl \u0026lt;-do_oob_syscall timer-responder-234 [001] d.~. 135.363264: evl_get_file \u0026lt;-EVL_ioctl timer-responder-234 [001] *.~. 135.363264: do_raw_spin_lock \u0026lt;-evl_get_file timer-responder-234 [001] *.~. 135.363265: do_raw_spin_unlock \u0026lt;-evl_get_file timer-responder-234 [001] d.~. 135.363265: latmus_oob_ioctl \u0026lt;-EVL_ioctl timer-responder-234 [001] d.~. 135.363266: add_measurement_sample \u0026lt;-latmus_oob_ioctl timer-responder-234 [001] d.~. 135.363266: evl_latspot: ** latency peak: 31.204 us ** In the latmus case, part of this analysis would include estimating the delay between the latest tick date programmed in the hardware and the actual receipt of the timer interrupt. When tracing is enabled, this information is automatically produced in the trace log:\n/* This is when the timer chip is programmed for the next tick. */ \u0026lt;idle\u0026gt;-0 [001] *.~. 135.362244: evl_timer_shot: latmus_pulse_handler at 135.363228 (delay: 984 us, 196195 cycles ... /* This is when the corresponding timer interrupt is received by Dovetail. */ \u0026lt;idle\u0026gt;-0 [001] *.~. 135.363233: irq_handler_entry: irq=4354 name=Out-of-band LAPIC timer interrupt Running the test suite (test) This command is a short-hand for running the EVL test suite. The usage is as follows:\n$ evl test [-l][-L][-k] [test-list] With no argument, this command runs all of the tests available from the default installation path at $prefix/tests:\n$ evl test duplicate-element: OK monitor-pp-dynamic: OK monitor-pi: OK clone-fork-exec: OK ... You can also chose to run a specific set of tests by mentioning them as arguments to the command, such as:\n$ evl test duplicate-element monitor-pi duplicate-element: OK monitor-pi: OK You may ask for listing the available tests instead of executing them, by using the -l switch:\n$ evl test -l duplicate-element monitor-pp-dynamic monitor-pi clone-fork-exec ... In a variant aimed at making scripting easier, you can ask for the absolute paths instead:\n$ evl test -L proxy-pipe mapfd /usr/evl/tests/proxy-pipe /usr/evl/tests/mapfd If some test goes wrong, the command normally stops immediately. Passing -k would allow it to keep going until the end of the series.\nImplementing your own EVL commands You can implement your own \u0026lsquo;evl\u0026rsquo; command plugins, which may be located anywhere provided it is reachable from the shell PATH variable with the proper execute permission bit set. EVL comes with a set of base plugins available from $prefix/libexec (*). The latter directory is implicitly searched for the command after the PATH variable was considered, which means that you may override any base command with your own implementation whenever you see fit.\n Crash course: adding the \u0026lsquo;foo\u0026rsquo; command script to ~/tools\n $ mkdir ~/tools $ cat \u0026gt; ~/tools/evl-foo #! /bin/sh echo \u0026quot;this is your 'evl foo' command\u0026quot; ^D $ chmod +x ~/tools/evl-foo $ export PATH=$PATH:~/tools $ evl foo this is your 'evl foo' command In addition, \u0026lsquo;evl\u0026rsquo; sets a few environment variables before calling a plugin. Your plugin executable/script can retrieve them using getenv(3) from a C program, or directly dereference those variables from a shell:\n   Variable Description Default value     EVL_CMDDIR Where to find the base plugins $prefix/libexec   EVL_TESTDIR Where to find the tests $prefix/tests   EVL_SYSDIR root of the /sysfs hierarchy for EVL devices /sys/devices/virtual   EVL_TRACEDIR root of ftrace hierarchy /sys/kernel/debug/tracing    (*) may be overriden using the -P option.\n Last modified: Sat, 05 Jun 2021 16:47:53 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/thread/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Thread element The main kernel\u0026rsquo;s thread is the basic execution unit in EVL. The most common kind of EVL threads is a regular POSIX thread started by pthread_create(3) which has attached itself to the EVL core by a call to evl_attach_self(). Once a POSIX thread attached itself to EVL, it can:\n  request real-time services to the core, exclusively by calling routines available from the EVL library . In this case, and only in this one, you get real-time guarantees for the caller. This is what time-critical processing loops are supposed to use. Such request may switch the calling thread to the out-of-band execution stage, for running under EVL\u0026rsquo;s supervision in order to ensure real-time behaviour.\n  invoke services from your favourite C library (glibc, musl, uClibc etc.), which may end up issuing system calls to the main kernel for carrying out the job. EVL may have to demote the caller automatically from the EVL context to the in-band stage, so that it enters a runtime mode which is compatible with using the main kernel services. As a result of this, you get NO help from EVL for keeping short and bounded latency figures anymore, but you do have access to any feature the main kernel provides. This mode is normally reserved to initialization and cleanup phases of your application. If you end up using them in the middle of a would-be time-critical loop, well, something is seriously wrong in this code.\n  A thread which is being scheduled by EVL instead of the main kernel is said to be running out-of-band, as defined by Dovetail. It remains in this mode until it asks for a service which the main kernel provides. Conversely, a thread which is being scheduled by the main kernel instead of EVL is said to be running in-band, as defined by Dovetail. It remains in this mode until it asks for a service which EVL can only provide to the caller when it runs out-of-band.\n Thread services   int evl_attach_thread(int flags, const char *fmt, ...)  EVL does not actually create threads; instead, it enables a regular POSIX thread to invoke its real-time services once this thread has attached to the EVL core. evl_attach_thread() is the initial service which requests such attachment. In most cases, applications would use the evl_attach_self() shorthand instead, which calls evl_attach_thread() under the hood with the default set of creation flags.\nThere is no requirement as of when evl_attach_thread() (or evl_attach_self()) should be called in the thread execution flow. You just have to call it before it starts requesting other EVL services. Note that the main thread of a process is no different from any other thread to EVL. It may call evl_attach_thread() whenever you see fit, or not at all if you don\u0026rsquo;t plan to request EVL services from this context.\nAs part of the attachment process, the calling thread is also pinned on its current CPU. You may change this default affinity by calling sched_setaffinity(2) as you see fit any time after evl_attach_thread() has returned, but keep in mind that such libc service will trigger a common Linux system call, which will cause your thread to switch to in-band context automatically when doing so. So you may want to avoid calling sched_setaffinity(2) from your time-critical loop, which would not make much sense anyway since this is fundamentally an heavyweight operation kernel-wise.\nAs part of the attachment process, the in-band scheduling settings your thread had before the call is translated to the closest EVL counterpart, as follows:\n   in-band settings out-of-band settings     SCHED_OTHER, 0 SCHED_WEAK, 0   SCHED_BATCH, 0 SCHED_WEAK, 0   SCHED_IDLE, 0 SCHED_WEAK, 0   \u0026lt;other policies\u0026gt;, prio SCHED_FIFO, prio    As a consequence, the thread would still run in-band on return from evl_attach_thread() if it was originally assigned to the SCHED_OTHER, SCHED_BATCH or SCHED_IDLE classes. Conversely, the thread would run out-of-band on return from evl_attach_thread() if it was originally assigned to any other in-band scheduling class (e.g. SCHED_FIFO).\n  #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sched.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;evl/sched.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; int main(int argc, char *argv[]) { struct sched_param param; int ret, tfd; param.sched_priority = 8; ret = pthread_setschedparam(pthread_self(), SCHED_FIFO, \u0026amp;param); ... /* EVL inherits the in-band scheduling params upon attachment. */ tfd = evl_attach_self(\u0026quot;app-main-thread:%d\u0026quot;, getpid()); /* * Now main() is running out-of-band, in the EVL SCHED_FIFO * class at priority 8. */ }  flagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_OBSERVABLE denotes a thread which may be observed for health monitoring purpose. See the Observable element.\n  Only if EVL_CLONE_OBSERVABLE is present in flags, EVL_CLONE_MASTER may be added to set the Observable associated to the new thread to master mode. Passing EVL_CLONE_MASTER for a non-observable thread causes the attachment to fail with -EINVAL.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new thread in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the thread name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_attach_thread() returns the file descriptor of the newly attached thread on success. You may use this fd to submit requests for this thread in any call which asks for a thread file descriptor. If the call fails, a negated error code is returned instead:\n  -EEXIST\tThe generated name is conflicting with an existing thread name.\n  -EINVAL\tEither flags is wrong, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -EPERM\tThe caller is not allowed to lock memory via a call to mlockall(2). Since memory locking is a requirement for running EVL threads, no joy.\n  -ENOMEM\tNo memory available, whether the kernel could not lock down all of the calling process\u0026rsquo;s virtual address space into RAM, or some other reason related to some process or driver eating way too much virtual or physical memory.\tYou may start panicking.\n  -ENOSYS\tThe EVL core is not enabled in the running kernel.\n  -ENOEXEC ABI mismatch error, as reported by evl_init().\n  #include \u0026lt;evl/thread.h\u0026gt; static void *byte_crunching_thread(void *arg) { int efd; /* Attach the current thread to the EVL core. */ efd = evl_attach_self(\u0026quot;/cruncher-%d\u0026quot;, getpid()); ... } As a result of this call, you should see a new device appear into the /dev/evl/thread hierarchy, e.g.:\n$ ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 248, 1 Jan 1 1970 clone crw-rw---- 1 root root 246, 0 Jan 1 1970 cruncher-2712   You can revert the attachment to EVL at any time by calling evl_detach_self() from the context of the thread to detach.\n  Closing all the file descriptors referring to an EVL thread is not enough to drop its attachment to the EVL core. It merely prevents to submit any further request for the original thread via calls taking file descriptors. You would still have to call evl_detach_self() from the context of this thread to fully detach it.\n  If a valid file descriptor is still referring to a detached thread, or after the thread has exited, any request submitted for that thread using such fd would receive -ESTALE.\n  An EVL thread which exits is automatically detached from the EVL core, you don\u0026rsquo;t have to call evl_detach_self() explicitly before exiting your thread.\n  The EVL core drops the kernel resources attached to a thread once it has detached itself or has exited, and only after all the file descriptors referring to that thread have been closed.\n  The EVL library sets the O_CLOEXEC flag on the file descriptor referring to the newly attached thread before returning from evl_attach_thread().\n    int evl_attach_self(const char *fmt, ...)  This call is a shorthand for attaching the calling thread to the EVL core, with the private visibility attribute set. It is identical to calling:\n\tevl_attach_thread(EVL_CLONE_PRIVATE, fmt, ...);  Note that if the generated name starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   int evl_detach_thread(int flags)  evl_detach_thread() reverts the action of evl_attach_thread(), detaching the calling thread from the EVL core. Once this operation has succeeded, the current thread cannot submit EVL requests anymore. Applications should use the evl_detach_self() shorthand, which calls evl_detach_thread() with flags set to zero as recommended.\nThis call returns zero on success, otherwise a negated error code if something went wrong:\nflagsThis parameter is currently unused and should be passed as zero.\n\n-EINVAL\tflags is not zero.\n-EPERM\tThe current thread is not attached to the EVL core.\n#include \u0026lt;evl/thread.h\u0026gt; static void *byte_crunching_thread(void *arg) { int efd; /* Attach the current thread to the EVL core (using the long call form). */ efd = evl_attach_thread(EVL_CLONE_PUBLIC, \u0026quot;cruncher-%d\u0026quot;, getpid()); ... /* Then detach it (also with the long call form). */ evl_detach_thread(0); ... }   You can re-attach the detached thread to EVL at any time by calling evl_attach_thread() again (or the evl_attach_self() shorthand).\n  If a valid file descriptor is still referring to a detached thread, or after the thread has exited, any request submitted for that thread using such descriptor would receive -ESTALE.\n  An EVL thread which exits is automatically detached from the EVL core, you don\u0026rsquo;t have to call evl_detach_thread() explicitly before exiting your thread.\n  The EVL core drops the kernel resources attached to a thread once it has detached itself or has exited, and only after all the file descriptors referring to that thread have been closed.\n    int evl_detach_self(void)  This call is a shorthand for detaching the calling thread from the EVL core. It is identical to calling:\n\tevl_detach_thread(0);   int evl_get_self(void)  evl_get_self() returns the file descriptor obtained for the current thread after a successful call to evl_attach_thread(). You may use this fd to submit requests for the current thread in other calls from the EVL library which ask for a thread file descriptor. This call returns a valid file descriptor referring to the caller on success, otherwise a negated error code if something went wrong:\n-EPERM\tThe current thread is not attached to the EVL core.\n#include \u0026lt;evl/thread.h\u0026gt; static void get_caller_info(void) { struct evl_thread_state statebuf; int efd, ret; /* Fetch the current thread's fd. */ efd = evl_get_self(); ... /* Retrieve the caller's state information. */ ret = evl_get_state(efd, \u0026amp;statebuf); ... } evl_get_self() fails with -EPERM after a call to evl_detach_thread().\n  int evl_switch_oob(void)  Applications are unlikely to ever use this call explicitly: it switches the calling thread to the out-of-band execution stage, for running under EVL\u0026rsquo;s supervision which ensures real-time behaviour. Any EVL service which requires it will enforce such switch if and when required automatically, so in most cases there should be no point in dealing with this manually in applications.\nevl_switch_oob() is defined for the rare circumstances where some high-level API based on the EVL core library might have to enforce a particular execution stage, based on a deep knowledge of how EVL works internally. Entering a syscall-free section of code for which running out-of-band must be guaranteed on entry would be the only valid reason to call evl_switch_oob(). This call returns zero on success, otherwise a negated error code if something went wrong:\n-EPERM\tThe current thread is not attached to the EVL core.\nForcing the current execution stage between in-band and out-of-band stages is a heavyweight operation: this entails two thread context switches both ways, as the switching thread is offloaded to the opposite scheduler. You really don\u0026rsquo;t want to force this explicitly unless you definitely have to and fully understand the implications of it runtime-wise. Bottom line is that calling a main kernel service from within a time-critical code is a clear indication that something is wrong in such code. This invalidates the reason why a time-critical code would need to switch back to the out-of-band stage eagerly.\n   int evl_switch_inband(void)  Applications are unlikely to ever use this call explicitly: it switches the calling thread to the in-band execution stage, for running under the main kernel supervision. Any EVL thread which issues a system call to the main kernel will be switched to the in-band context automatically, so in most cases there should be no point in dealing with this manually in applications.\nevl_switch_inband() is defined for the rare circumstances where some high-level API based on the EVL core library might have to enforce a particular execution stage, based on a deep knowledge of how EVL works internally. Entering a syscall-free section of code for which the in-band mode needs to be guaranteed on entry would be the only valid reason to call evl_switch_inband(). This call returns zero on success, otherwise a negated error code if something went wrong:\n-EPERM\tThe current thread is not attached to the EVL core.\nForcing the current execution stage between in-band and out-of-band stages is a heavyweight operation: this entails two thread context switches both ways, as the switching thread is offloaded to the opposite scheduler. You really don\u0026rsquo;t want to force this explicitly unless you definitely have to and fully understand the implications of it runtime-wise. Bottom line is that switching the execution stage to in-band from within a time-critical code is a clear indication that something is wrong in such code.\n   bool evl_is_inband(void)  In some cases, you may need to check the current execution stage for the caller. evl_is_inband() returns a true boolean value if the caller runs in-band, false otherwise.\nA POSIX thread which is not currently attached to the EVL core always receives a true value when issuing this call, which makes sense since it cannot run out-of-band.\n   int evl_get_state(int efd, struct evl_thread_state *statebuf)  evl_get_state() is an extended variant of evl_get_schedattr() for retrieving runtime information about the state of a thread. The return buffer is of type struct evl_thread_state, which is defined as follows:\nstruct evl_thread_state { struct evl_sched_attrs eattrs; int cpu; };   unlike evl_get_schedattr(), the value returned in statebuf-\u0026gt;attrs.sched_priority by evl_get_state() may reflect an ongoing priority inheritance/ceiling boost.\n  statebuf-\u0026gt;cpu is the CPU the target thread runs on at the time of the call.\n  efdA file descriptor referring to the thread to inquire about.\n\nstatbufA pointer to the information buffer.\n\nevl_get_state() returns zero on success, otherwise a negated error code:\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_unblock_thread(int efd)  Unblocks the thread referred to by efd if it is currently sleeping on some EVL core monitor element, waiting for it to be signaled/available. In other words, the blocking system call is forced to fail, and as a result the target thread receives the -EINTR error on return.\nefdA file descriptor referring to the thread to unblock.\n\nevl_unblock_thread() returns zero on success, otherwise a negated error code:\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_demote_thread(int efd)  Demotes the thread referred to by efd if it is currently running out-of-band with real-time scheduling attributes.\nDemoting a thread means to force it out of any real-time scheduling class, unblock it like evl_unblock_thread() would do, and kick it out of the out-of-band stage, all in the same move. Once demoted, a thread runs in-band and undergoes the SCHED_WEAK policy. evl_demote_thread() is a pretty big hammer you don\u0026rsquo;t want to use lightly; it should be reserved to specific (read: desperate) cases when you have to perform some aggressive recovery procedure, and/or you want to stop a thread running out-of-band from hogging a CPU.\nefdA file descriptor referring to the thread to demote.\n\nevl_demote_thread() returns zero on success, otherwise a negated error code:\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_set_thread_mode(int efd, int mask, int *oldmask)  Each EVL thread has a few so-called mode bits which affect its behavior depending on whether they are set. evl_set_thread_mode() can set the following flags when present in mask:\n T_WOSS: warn on stage switch T_WOLI: warn on locking inconsistency T_WOSX: warn on stage exclusion T_HMSIG: enable notification of HM events via the SIGDEBUG signal T_HMOBS: enable notification of HM events via the built-in observable  See the section about the health monitoring of EVL threads for details about these bits.\nIf any of T_WOSS, T_WOLI or T_WOSX are present in mask but none of T_HMSIG or T_HMOBS is, then T_HMSIG is turned on automatically, enabling notification delivery via the SIGDEBUG signal.\nefdA file descriptor referring to the target thread.\n\nmaskA bitmask mentioning the mode bits to set. Zero is valid, and leads to a no-op. Passing a null mask and a valid oldmask pointer allows peeking at the mode bits currently set for a thread without changing them.\n\noldmaskThe address of a bitmask which should collect the previous set of active mode bits for the thread, before the update. NULL can be passed to discard this information.\n\nevl_set_thread_mode() returns zero on success, otherwise a negated error code:\n-EINVAL mask contains invalid mode bits. Setting T_HMOBS for a thread which was not created with the EVL_CLONE_OBSERVABLE attribute set is an error.\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_clear_thread_mode(int efd, int mask, int *oldmask)  evl_clear_thread_mode() is the converse call to evl_set_thread_mode(), clearing the mode bits mentioned in mask.\nIf all of T_WOSS, T_WOLI and T_WOSX are cleared for the thread as a result, T_HMSIG and T_HMOBS are automatically cleared as well by evl_clear_thread_mode().\nefdA file descriptor referring to the target thread.\n\nmaskA bitmask mentioning the mode bits to clear. Zero is valid, and leads to a no-op. Passing a null mask and a valid oldmask pointer allows peeking at the mode bits currently set for a thread without changing them.\n\noldmaskThe address of a bitmask which should collect the previous set of active mode bits for the thread, before the update. NULL can be passed to discard this information.\n\nevl_clear_thread_mode() returns zero on success, otherwise a negated error code:\n-EINVAL\tmask contains invalid bits.\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_subscribe(int ofd, unsigned int backlog_count, int flags)  This service subscribes the current thread to an Observable element, which makes the former an observer of the latter. This thread does not have to be attached to EVL in order to subscribe to an Observable. Subscribers are independent from each other, the target Observable may vanish while subscriptions are still active, observers can come and go freely. In other words, the relationship between an Observable and its observers is losely coupled. However, a thread can only have a single active subscription to a particular Observable, although it can subscribe to any number of distinct Observables.\nofdA file descriptor referring to the Observable to subscribe to.\n\nbacklog_countThe number of notifications which the core can buffer for this subscription. On overflow, the unread events already queued are preserved, the new ones are lost for the observer.\n\nflagsA mask of ORed operation flags which further qualify the type of subscription. If EVL_NOTIFY_ONCHANGE is passed, the EVL core will merge multiple consecutive notifications for the same tag and event values. In other words, the returned ( tag, value ) pairs will be different at every receipt. Passing zero or EVL_NOTIFY_ALWAYS ensures that all notices received by the Observable are passed to this subscriber, unfiltered.\n\nevl_subscribe() returns zero on success, otherwise a negated error code:\n  -EINVAL flags contains invalid operations bits. The only valid bit is EVL_NOTIFY_ONCHANGE, or backlock_log_count is zero.\n  -EBADF\tofd is not a valid file descriptor.\n  -EPERM\tofd does not refer to an Observable element.\n  -ENOMEM\tNo memory available for the operation. That is a problem.\n    int evl_unsubscribe(int ofd)  This service unsubscribes the current thread from an Observable element. This is the converse call to evl_subscribe().\nofdA file descriptor referring to the Observable to unsubscribe from.\n\nevl_unsubscribe() returns zero on success, otherwise a negated error code:\n  -EBADF\tofd is not a valid file descriptor.\n  -EPERM\tofd does not refer to an Observable element.\n  -ENOENT\tthe current thread is not subscribed to the Observable referred to by _ofd.\n   Health monitoring of threads The EVL core has some health monitoring (HM) capabilities, which can be enabled separately on a per-thread basis using evl_set_thread_mode(), or global to the system via the kernel configuration. They are based on runtime error detection when performing user requests which involve threads. Each type of error is associated with a diagnostic code, such as:\n/* Health monitoring diag codes (via observable or SIGDEBUG). */ #define EVL_HMDIAG_SIGDEMOTE\t1 #define EVL_HMDIAG_SYSDEMOTE\t2 #define EVL_HMDIAG_EXDEMOTE\t3 #define EVL_HMDIAG_WATCHDOG\t4 #define EVL_HMDIAG_LKDEPEND\t5 #define EVL_HMDIAG_LKIMBALANCE\t6 #define EVL_HMDIAG_LKSLEEP\t7 #define EVL_HMDIAG_STAGEX\t8 Each of these codes identifies a specific cause of trouble for the thread which receives it:\n  EVL_HMDIAG_SIGDEMOTE, enabled by the T_WOSS mode bit. The thread was demoted to the in-band stage because it received a (POSIX) signal. In such an event, the core had to release the recipient from any blocked state from the out-of-band stage, because handling any pending in-band signal is a requirement for the overall system sanity.\n  EVL_HMDIAG_SYSDEMOTE, enabled by the T_WOSS mode bit. The thread was demoted to the in-band stage because it issued an in-band Linux syscall, such as those defined in your C library of choice. Requesting the in-band kernel to handle a system call is by definition a reason to switch to in-band execution.\n  EVL_HMDIAG_EXDEMOTE, enabled by the T_WOSS mode bit. The thread was demoted to the in-band stage because it received a processor exception while running on the out-of-band stage, which it could not handle from there. There are different sources of CPU exceptions, the most common ones involve invalid memory addressing which typically ends up with receiving a SIGSEGV or SIGBUS signal from the kernel as a result. When the exception cannot be handled directly from the out-of-band stage, the EVL core has to demote the faulting thread so that the common (in-band) exception handling code can run safely.\n  EVL_HMDIAG_WATCHDOG, enabled by kernel configuration. The thread was kicked out of out-of-band execution because it hogged a CPU for too long without reliquishing it to the in-band stage. The delay applies to the entire period while a CPU executes on the out-of-band stage, so this may involve multiple EVL threads. Only the thread which is running at the time the watchdog expires receives the notification. The longer the detection delay (4s by default), the higher the risk of breaking the whole system since there is a point when the kernel is going to freak out badly if some CPU is unavailable for too long for handling in-band work. The timeout delay can be configured using CONFIG_EVL_WATCHDOG.\n  EVL_HMDIAG_LKDEPEND, enabled by the T_WOLI mode bit. There are two converse conditions causing this error, both due to an incorrect usage of EVL mutexes leading to a priority inversion:\n  if a thread is about to switch in-band while owning an EVL mutex which is awaited by another thread. This situation would cause a priority inversion for the waiter(s), since the latter would depend on a mutex owner who lost any guarantee for real-time execution as a result of switching in-band.\n  if a thread running out-of-band is about to sleep on an EVL mutex owned by another thread running in-band. Same reasoning as previously, an out-of-band thread would depend on the non real-time scheduling undergone by the current owner of the mutex.\n    EVL_HMDIAG_LKIMBALANCE, enabled by the T_WOLI mode bit. An attempt to unlock a free EVL mutex was detected.\n  EVL_HMDIAG_LKSLEEP, enabled by the T_WOLI mode bit. A thread which undergoes the SCHED_WEAK which already holds an EVL mutex subsequently wants to wait for a different type of EVL resource to be available, i.e. pretty much any EVL synchronization mechanism which may block the caller except EVL mutexes. Such pattern is a bad idea: a weakly scheduled thread (EVL-wise, that is) has neither real-time requirements nor capabilities, and some real-time thread may well wait for it to release the mutex it holds. Therefore, waiting for an undefined amount of time for an event to - maybe - occur before the mutex can be released eventually is logically flawed.\n  EVL_HMDIAG_STAGEX, enabled by the T_WOSX mode bit. A thread is performing an out-of-band I/O operation which is blocked on a stage exclusion lock waiting for all in-band tasks to leave the section guarded by that lock. This issue leads to a priority inversion. Real-time I/O drivers using stage exclusion should provide an interface to applications which enforces a clear separation between in-band and out-of-band runtime modes, so that this does not normally happen. Blocking on a stax from the out-of-band stage might be fine in some circumstances in case portions of code are to be shared between in-band and out-of-band threads without risking priority inversions, this is the reason why such locks exist in the first place. However, this behavior has to be specifically allowed by the driver implementation. If T_WOSX is set for the thread, then such event must be unexpected.\n  SIGDEBUG and HM notifications via the observable Once an error condition is detected, the EVL core can notify the faulting thread by sending it a regular POSIX signal (aka SIGDEBUG, which is SIGXCPU in disguise), and/or pushing a notification to the observable component of the thread if enabled. Both options are cumulative.\nSignal-based HM notifications SIGDEBUG is enabled by setting the T_HMSIG mode bit for the thread. A signal handler should have been installed for receiving them, otherwise the process would be killed. The macro sigdebug_cause() retrieves the diag code (EVL_HMDIAG_xxx) from the SIGDEBUG information block. Checking that SIGDEBUG was actually sent by the EVL core is recommended, using the sigdebug_marked() macro as illustrated below. If this macro returns false when passed the signal information block, then your thread has received SIGXCPU from another source, this is not a HM event sent by the EVL core.\n#include \u0026lt;signal.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; /* A basic SIGDEBUG (aka SIGXCPU) handler which only prints out the cause. */ static void sigdebug_handler(int sig, siginfo_t *si, void *context) { if (!sigdebug_marked(si)) {\t/* Is this from EVL? */ you_should_handle_sigxcpu(sig, si, context); return; } /* This is a HM event, handle it. */ you_should_handle_the_hm_event(sigdebug_cause(si)); } void install_sigdebug_handler(void) { struct sigaction sa; sigemptyset(\u0026amp;sa.sa_mask); sa.sa_sigaction = sigdebug_handler; sa.sa_flags = SA_SIGINFO; sigaction(SIGDEBUG, \u0026amp;sa, NULL); } libevl defines the evl_sigdebug_handler() routine which simply prints out the diagnostics to stdout then returns.\nCan EVL threads run in kernel space? Yes. Drivers may create kernel-based EVL threads backed by regular kthreads, using EVL\u0026rsquo;s kernel API. The attachment phase is hidden inside the API call starting the EVL kthread in this case. Most of the notions explained in this document apply to them too, except that there is no system call interface between the EVL core and the kthread. For this reason, unlike EVL threads running in user-space, nothing prevents EVL kthreads from calling the in-band kernel routines from the wrong context.\nWhere do public thread devices live? Each time a new public thread element is created, it appears into the /dev/evl/thread hierarchy, e.g.:\n$ ls -l /dev/evl/threads total 0 crw-rw---- 1 root root 248, 1 Jan 1 1970 clone crw-rw---- 1 root root 246, 0 Mar 1 11:26 rtk1@0:1682 crw-rw---- 1 root root 246, 18 Mar 1 11:26 rtk1@1:1682 crw-rw---- 1 root root 246, 36 Mar 1 11:26 rtk1@2:1682 crw-rw---- 1 root root 246, 54 Mar 1 11:26 rtk1@3:1682 crw-rw---- 1 root root 246, 1 Mar 1 11:26 rtk2@0:1682 crw-rw---- 1 root root 246, 19 Mar 1 11:26 rtk2@1:1682 crw-rw---- 1 root root 246, 37 Mar 1 11:26 rtk2@2:1682 crw-rw---- 1 root root 246, 55 Mar 1 11:26 rtk2@3:1682 (snip) crw-rw---- 1 root root 246, 9 Mar 1 11:26 rtus_ufps0-10:1682 crw-rw---- 1 root root 246, 8 Mar 1 11:26 rtus_ufps0-9:1682 crw-rw---- 1 root root 246, 27 Mar 1 11:26 rtus_ufps1-10:1682 crw-rw---- 1 root root 246, 26 Mar 1 11:26 rtus_ufps1-9:1682 crw-rw---- 1 root root 246, 45 Mar 1 11:26 rtus_ufps2-10:1682 crw-rw---- 1 root root 246, 44 Mar 1 11:26 rtus_ufps2-9:1682 crw-rw---- 1 root root 246, 63 Mar 1 11:26 rtus_ufps3-10:1682 crw-rw---- 1 root root 246, 62 Mar 1 11:26 rtus_ufps3-9:1682  The clone file is a special device which allows the EVL library to request the creation of a thread element. This is for internal use only.\n How to reach a remote EVL thread? If you need to submit requests to an EVL thread which belongs to a different process, you first need it to have public visibility. If so, then you can open the device file representing this element in /dev/evl/thread, then use the file descriptor just obtained in the thread-related request you want to send it. For instance, we could change the scheduling parameters of an EVL kernel thread named rtk1@3:1682 from a companion application in userland as follows:\n\tstruct evl_sched_attrs attrs; int efd, ret; efd = open(\u0026quot;/dev/evl/thread/rtk1@3:1682\u0026quot;, O_RDWR); /* skipping checks */ attrs.sched_policy = SCHED_FIFO; attrs.sched_priority = 90; ret = evl_set_schedattr(efd, \u0026amp;attrs); /* skipping checks */ Where to look for thread information? Using the \u0026lsquo;evl ps\u0026rsquo; command Running the following command from the shell will report the current EVL thread activity on your system:\n# evl ps CPU PID SCHED PRIO NAME 0 398 rt 90 [latmus-klat:394] 0 399 weak 0 lat-measure:394 There are display options you can pass to the \u0026lsquo;evl ps\u0026rsquo; command to get more information about each EVL thread, sorting the result list according to various criteria.\nLooking at the /sysfs data Since every EVL element is backed by a regular character device, so are threads. Therefore, what to look for is the set of thread device attributes available from the /sysfs hierarchy. The \u0026lsquo;evl ps\u0026rsquo; command actually parses this raw information before rendering it in a human-readable format. Let\u0026rsquo;s have a look at the attributes exported by the sampling thread of some random run of EVL\u0026rsquo;s latmus utility:\n# cd /sys/devices/virtual/thread/timer-responder:2136 # ls -l total 0 -r--r--r-- 1 root root 4096 Mar 1 12:01 pid -r--r--r-- 1 root root 4096 Mar 1 12:01 sched -r--r--r-- 1 root root 4096 Mar 1 12:01 state -r--r--r-- 1 root root 4096 Mar 1 12:01 stats -r--r--r-- 1 root root 4096 Mar 1 12:01 timeout -r--r--r-- 1 root root 4096 Mar 1 12:01 observable # cat pid sched state stats timeout observable 2140 0 90 90 rt 0x8002 1 311156 311175 0 46999122352 0 0 0 The format of these fields is as follows:\n  pid is the thread identifier (kernel TID); this is a positive integer of type pid_t.\n  sched contains the scheduling attributes of the thread, with by order of appearance:\n  the CPU the thread is running on.\n  the current priority level of the thread within its scheduling class. With SCHED_FIFO for instance, that would be a figure in the [1..99] range. This value may reflect an ongoing priority boost due to enforcing the priority inheritance protocol with some EVL mutex(es) that thread contends for.\n  the base priority level of the thread within its scheduling class, not reflecting any priority boost. This is the value that you last set with evl_set_schedattr() when assigning the thread its scheduling class.\n  the name of the scheduling class the thread is assigned to. This is an ASCII string (unquoted), like rt for the SCHED_FIFO class.\n  depending on the scheduling class, you may see optional information after the class name which gives some class-specific details about the thread. Currently, only SCHED_TP and SCHED_QUOTA define such information:\n  SCHED_QUOTA appends the quota group identifier for that thread.\n  SCHED_TP appends the identifier of the partition the thread is attached to.\n      state is the hexadecimal value of the thread\u0026rsquo;s internal state word. This information is very ABI dependent, each bit is tersely documented in uapi/evl/thread.h from the linux-evl kernel tree. This is intended at being food for geek brain.\n  stats gives statistical information about the CPU consumption of the thread, in the following order:\n  the number of (forced) switches to in-band mode, which happens when a thread issues an in-band system call from an out-of-band context (ISW). This figure should not increase once a real-time EVL thread has entered its time-critical work loop, otherwise this would mean that such thread is actually leaving the out-of-band execution stage while it should not, getting latency hits in the process.\n  the number of EVL context switches the thread was subject to, meaning the number of times the thread was given back the CPU after a blocked state (CTXSW). This value exclusively reflects the number of switches performed by EVL as a result of resuming threads aslept on the out-of-band stage (context switches involved in resuming threads aslept on the in-band stage are not counted here).\n  the number of EVL system calls the thread has issued to the core (SYS). Here again, only the EVL system calls are counted, in-band system calls from the same threads are tracked by this counter.\n  the number of times the core had to wake up the thread from a remote CPU (RWA). This information is useful to find out thread placement issues on CPUs. The best situation is when the core can wake up threads directly from the CPU they were put to sleep, without inter-processor messaging (IPI) in order to force a remote CPU to re-schedule. Although this is not always possible, as multiple threads may have to synchronize from distinct CPUs, the lesser this number, the smaller the overhead caused by wake up requests.\n  the cumulated CPU usage of the thread since its creation, expressed as a count of nanoseconds.\n  the percentage of the CPU bandwidth consumed by the thread, from the last time this counter was read until the current readout.\n    timeout is a count of nanoseconds representing the ongoing delay until the thread wakes up from its current timed wait. Zero means no timeout. EVL starts a timer when a thread enters a timed wait on some kernel resource; timeout reports the time to go until this timer fires.\n  observable is a boolean value denoting the observability of the thread. Non-zero indicates that EVL_CLONE_OBSERVABLE was set for this thread, typically for health monitoring purpose, which made it observable to itself or to other threads.\n   Last modified: Mon, 17 May 2021 16:48:45 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/kthread/",
	"title": "Kernel thread",
	"tags": [],
	"description": "",
	"content": "Out-of-band threads in kernel space The EVL core can run common kernel threads on the out-of-band stage, which can be used in out-of-band capable drivers when ultra-low response time is required.\n  int evl_run_kthread(struct evl_kthread *kthread, void (*threadfn)(void *arg), void *arg, int priority, int clone_flags, const char *fmt, ...)  evl_run_kthread() is a macro-definition which spawns an EVL kernel thread, which is the EVL equivalent of its in-band kernel counterpart named kthread_run().\nThe new thread may be pinned on any of the out-of-band capable CPUs (See the evl.oob_cpus kernel parameter). If you need to spawn a kernel thread on a particular CPU, you may want to use evl_run_kthread_on_cpu() instead.\nkthreadA kernel thread descriptor where the core will maintain the per-thread context information. This memory area must remain valid as long as the associated kernel thread runs.\n\nthreadfnThe routine to run in the new thread context.\n\nargA user-defined opaque pointer which is passed unmodified to threadfn as its sole argument.\n\npriorityThe priority of the new thread, which is assumed to refer to the SCHED_FIFO class.\n\nclone_flagsA set of creation flags for the new kernel thread, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public thread which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to application processes for sharing.\n  EVL_CLONE_PRIVATE denotes a thread which is private to the kernel. No device file appears for it in the /dev/evl file hierarchy.\n  \nfmtA ksprintf()-like format string to generate the thread name. Unlike evl_attach_thread() from the user API, evl_run_kthread() does not look for any shorthand defined by the naming convention for application threads. Thread visibility can be set exclusively by using the clone_flags.\n\n...The optional variable argument list completing the format.\n\nevl_run_kthread() returns zero on success, or a negated error code otherwise:\n  -EEXIST\tThe generated name is conflicting with an existing thread name.\n  -EINVAL\tEither clone_flags or priority are wrong.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -ENOMEM\tNot enough memory available. Buckle up.\n    int evl_run_kthread_on_cpu(struct evl_kthread *kthread, int cpu, void (*threadfn)(void *arg), void *arg, int priority, int clone_flags, const char *fmt, ...)  As its name suggests, evl_run_kthread_on_cpu() is a variant of evl_run_kthread() which lets you pick a particular CPU for pinning the new kernel thread.\ncpuThe CPU number the new thread should be pinned to, among the out-of-band capable ones (See the evl.oob_cpus kernel parameter).\n\nevl_run_kthread_on_cpu() returns zero on success, or a negated error code. The set of error conditions for evl_run_kthread() apply to evl_run_kthread_on_cpu(), plus:\n -EINVAL\tcpu is not a valid, out-of-band capable CPU.    void evl_stop_kthread(struct evl_kthread *kthread)  This call requests the EVL kthread to exit at the first opportunity. It may be called from any stage, but only from a kernel thread context, regular in-band or EVL.\nThis is an advisory method for stopping EVL kernel threads, which requires kthread to call evl_kthread_should_stop() as part of its regular work loop, exiting when such test returns true. In other words, evl_stop_kthread() raises the condition which evl_kthread_should_stop() returns to its caller.\nevl_stop_kthread() first unblocks kthread from any blocking call, then waits for kthread to actually exit before returning to the caller. Therefore, an EVL kernel thread which receives a request for termination while sleeping on some EVL call unblocks with -EINTR as a result.\nThere is no way to forcibly terminate kernel threads since this would potentially leave the kernel system in a broken, unstable state. Both the requestor and the subject kernel thread must cooperate for the later to follow an orderly process for exiting. The in-band equivalent is achieved with kthread_stop(), kthread_should_stop().\n kthreadThe EVL kernel thread to send a stop request to. If kthread represents the calling context (i.e. self-termination), the call does not return and the caller is exited immediately. Otherwise, the stop request is left pending until kthread eventually calls evl_kthread_should_stop(), at which point it should act upon this event by exiting.\n\n  bool evl_kthread_should_stop(void)  This call is paired with evl_stop_kthread(). It should be called by any EVL kernel thread which intends to accept termination requests from other threads.\nWhenever evl_kthread_should_stop() returns true, the caller should plan for exiting as soon as possible, typically by returning from its entry routine. Otherwise, it may continue.\nA typical usage pattern is as follows:\n #include \u0026lt;evl/thread.h\u0026gt; #include \u0026lt;evl/flag.h\u0026gt; static DEFINE_EVL_FLAG(some_flag); void some_kthread(struct evl_kthread *kthread) { int ret; for (;;) { if (evl_kthread_should_stop()) break; /* wait for the next processing signal */ ret = evl_wait_flag(\u0026amp;some_flag); if (ret == -EINTR) break; /* do some useful stuff */ } /* about to leave, do some cleanup */ }   int evl_set_kthread_priority(struct evl_kthread *kthread, int priority)  This service changes the priority of an EVL kernel thread.\nkthreadThe descriptor of the EVL kernel thread.\n\npriorityThe new priority of kthread, which is assumed to refer to the SCHED_FIFO class.\n\nevl_set_kthread_priority() returns zero on success, otherwise a negated error code is returned:\n-EINVAL\tpriority is invalid. Check the documentation of the SCHED_FIFO class for details.\nevl_set_kthread_priority() immediately applies the changes to the scheduling attributes of kthread.\n  struct evl_kthread *evl_current_kthread(void)  Returns the descriptor of the current EVL kernel thread. NULL is returned when calling this service from any other type of thread context (i.e. EVL user thread, non-EVL thread).\nevl_current_kthread() does not account for the interrupt context; a non-NULL pointer to the interrupted kernel thread may be returned if called from the IRQ handler.\n   ktime_t evl_delay(ktime_t timeout, enum evl_tmode timeout_mode, struct evl_clock *clock)  int evl_sleep_until(ktime_t timeout)  int evl_sleep(ktime_t delay)  int evl_set_period(struct evl_clock *clock, ktime_t idate, ktime_t period)  int evl_wait_period(unsigned long *overruns_r)  Last modified: Tue, 22 Dec 2020 17:32:06 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/benchmarks/",
	"title": "Running benchmarks",
	"tags": [],
	"description": "",
	"content": "The issue of proper benchmarking is sisyphean, a never ending conversation which reboots as technology and user requirements change. Every benchmark has an agenda of some sort; that is ok provided it does not take the reader for a fool. For these reasons, this section is a work-in-progress by design, which only discusses test cases for which the source code is fully available, which does not require any black-box, so as to allow you to verify and reproduce them fairly easily.\nAny comments and other contributions improving this section and/or EVL in general are welcome.\nMeasuring response time to interrupts Since the real-time infrastructure has to deliver reliable response times to external events - as in strictly bounded - whatever the system may be running when they happen, we have to measure the latency between the ideal delivery date of such event, and the actual moment the application starts processing it. Then we may assess the jitter as the variation in latency. With Linux running on the hardware, reliable means that an upper bound to such latency can be determined, although we are using a non-formal, probabilistic method through countless hours of testing under a significant stress load. Although the outcome of such test is not by itself representative of the overall capability of a system to support real-time applications, such test going wrong would clearly be a showstopper. No doubt that every real-time infrastructure out there wants to shine on that one. To measure the response time to interrupts in different contexts, EVL provides the latmus utility. The following figure illustrates the potential delays which may exist between the moment an interrupt request is raised by a device, and the time a responder thread running in the application space can act upon it:\n(1) There may be multiple causes for the hardware-induced delay such as (but not limited to):\n  some devices may temporarily block the CPU from completing I/O transactions which may lead to a stall. For instance, some GPUs preventing CPUs to access their I/O memory for a short while, or burst mode DMA affecting the execution rate of instructions in the CPU by keeping it off the memory bus during transfers.\n  the CPU needs to synchronize with the interrupt request internally, the instruction pipeline may be affected by the operations involved in taking an interrupt, leading to additional latency.\n  The time interrupts are masked in the CPU upon request from the software, effectively preventing it to take IRQs.\n  (2) Although handling the IRQ in some service routine from the real-time core is (hopefully) a short process which ends up readying the responder thread in the scheduler, it may be further delayed:\n  I/D memory caches may have been dirtied by non real-time activities while the real-time system was waiting for the next event. This increases the level of cache misses for both code and data, and therefore slows down the execution of the real-time infrastructure as a whole when it starts handling the incoming event.\n  the time needed to handle a write miss is longer when the write-allocate policy is enabled in the cache controller, making the execution slower when this happens.\n  (3) Eventually, the scheduler is called in order to reconsider which thread should run on the CPU the responder thread was sleeping on when readied, which is subject to more potential delays:\n  the CPU receiving the IRQ might not be the one the responder sleeps on, in which case the former must send a rescheduling request to the latter, so that it will resume the responder. This is usually done via an inter-processor interrupt, aka IPI. The time required for the IPI to flow to the remote CPU and be handled there further extends the delay. A trivial work around would involve setting the IRQ affinity to the same CPU the responder runs on, but this may not be possible unless the interrupt controller does allow this on your SoC.\n  once the scheduler code has determined that the responder thread should resume execution, the context switch code is performed to restore the memory context and the register file for that thread to resume where it left off. Switching memory context may be a lenghty operation on some architectures as this affects the caches and requires strong synchronization.\n  the hardware-induced slowdowns mentioned for step (2) apply as well.\n  Getting into these issues is not a matter of following a dual kernel vs native preemption approach: all of them bite the same way regardless.\n Measuring response time to timer events A real-time system normally comes with a way to measure the latency of its threads on timer events. Xenomai 3 provides one, the PREEMPT_RT project as well, so does EVL with the latmus program. No wonder why, running precisely timed work loops is a basic requirement for real-time applications. Besides, such a test requires only little preparation: we don\u0026rsquo;t need any external event source for running it, no specific equipment is needed, the on-chip high-precision clock timer of the system under test should be sufficient, therefore we can measure easily the latency directly from there.\n[Xenomai 4] implements this test with the help of the latmus driver which sends a wake up event to an EVL-enabled responder thread created by the latmus application each time a new timer interrupt occurs. The responder thread gets a timestamp from EVL\u0026rsquo;s monotonic clock immediately when it resumes upon wake up, then sends that information to the driver, which in turn calculates the latency value, i.e. the delay between the ideal time the interrupt should have been received and the actual wake up time reported by the responder thread. This way, we account for all of the delays mentioned earlier which might affect the accuracy of a request for a timed wake up.\nThe driver accumulates these results, sending an intermediate summary every second to a logger thread with the minimum, maximum and average latency values observed over this period. The 1Hz display loop which is visible while the latmus application is running is synchronized on the receipt of such summary. The following figure illustrates this execution flow:\nWhen the test completes, the latmus application determines the minimum, worst case and average latency values over the whole test duration. Upon request by passing the -g option, the latmus application dumps an histogram showing the frequency distribution of the worst case figures which have been observed over time. The output format can be parsed by gnuplot.\nRunning the timer-based test First, we need the latmus driver to be loaded into the kernel for the SUT. Therefore CONFIG_EVL_LATMUS should be enabled in the kernel configuration. From the command line, the entire test is controlled by the latmus application using the -m option, which can be omitted since measuring the response time to timer events is the default test.\n Measuring response time to timer events\n # latmus RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 1.211| 1.325| 2.476| 0| 0| 1.211| 2.476 RTD| 1.182| 1.302| 3.899| 0| 0| 1.182| 3.899 RTD| 1.189| 1.314| 2.486| 0| 0| 1.182| 3.899 RTD| 1.201| 1.315| 2.510| 0| 0| 1.182| 3.899 RTD| 1.192| 1.329| 2.457| 0| 0| 1.182| 3.899 RTD| 1.183| 1.307| 2.418| 0| 0| 1.182| 3.899 RTD| 1.206| 1.318| 2.375| 0| 0| 1.182| 3.899 RTD| 1.206| 1.316| 2.418| 0| 0| 1.182| 3.899 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 1.182| 1.316| 3.899| 0| 0| 00:00:08/00:00:08  Collecting plottable histogram data (timer test)\n RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 1.156| 1.273| 1.786| 0| 0| 1.156| 1.786 RTD| 1.170| 1.288| 4.188| 0| 0| 1.156| 4.188 RTD| 1.135| 1.253| 3.175| 0| 0| 1.135| 4.188 RTD| 1.158| 1.275| 2.974| 0| 0| 1.135| 4.188 ... ^C # test started on: Fri Jan 24 15:36:34 2020 # Linux version 5.5.0-rc7+ (rpm@cobalt) (gcc version 9.2.1 20190827 (Red Hat 9.2.1-1) (GCC)) #45 SMP PREEMPT IRQPIPE Wed Jan 22 12:24:03 CET 2020 # BOOT_IMAGE=(tftp)/tqmxe39/switch/bzImage rw ip=dhcp root=/dev/nfs nfsroot=192.168.3.1:/var/lab/tftpboot/tqmxe39/switch/rootfs,tcp,nfsvers=3 nmi_watchdog=0 console=ttyS0,115200 isolcpus=1 evl.oob_cpus=1 # libevl version: evl.0 -- #a3ceb80 (2020-01-22 11:57:11 +0100) # sampling period: 1000 microseconds # clock gravity: 2000i 3500k 3500u # clocksource: tsc # vDSO access: architected # context: user # thread priority: 98 # thread affinity: CPU1 # C-state restricted # duration (hhmmss): 00:01:12 # peak (hhmmss): 00:00:47 # min latency: 0.205 # avg latency: 3.097 # max latency: 25.510 # sample count: 71598 0 95 1 25561 2 18747 3 2677 4 6592 5 17056 6 664 7 48 8 18 9 12 10 18 11 24 12 14 13 2 14 6 15 5 16 5 17 6 18 23 19 17 20 3 21 1 22 1 23 1 24 1 25 1 The output format starts with a comment section which gives specifics about the test environment and the overall results (all lines from this section begin with a hash sign). The comment section is followed by the frequency distribution forming the histogram, in the form of a series of value pairs: \u0026lt;latency-µs\u0026gt; . For instance, \u0026ldquo;1 25561\u0026rdquo; means that 25561 wake ups were delayed between 1 (inclusive) and 2 (exclusive) microseconds from the ideal time. A plus (+) sign appearing after the last count of occurrences means that there are outliers beyond the limit of the histogram size. In such event, raising the numnber of cells with the \u0026ndash;histogram= option may be a good idea.\nInterpreting the comment section of a data distribution   # test started on: Fri Jan 24 15:36:34 2020\nDate the test was started (no kidding).\n  # Linux version 5.5.0-rc7+ (rpm@cobalt) (gcc version 9.2.1 20190827 (Red Hat 9.2.1-1) (GCC)) #45 SMP PREEMPT IRQPIPE Wed Jan 22 12:24:03 CET 2020\nThe output of uname -a on the system under test.\n  # BOOT_IMAGE=(tftp)/tqmxe39/switch/bzImage rw ip=dhcp root=/dev/nfs nfsroot=192.168.3.1:/var/lab/tftpboot/tqmxe39/switch/rootfs,tcp,nfsvers=3 nmi_watchdog=0 console=ttyS0,115200 isolcpus=1 evl.oob_cpus=1\nThe kernel command line as returned by /proc/cmdline.\n  # libevl version: evl.0 \u0026ndash; #a3ceb80 (2020-01-22 11:57:11 +0100)\nThe version information extracted from libevl including its major version number, and optionally the GIT commit hash and date thereof libevl was built from.\n  # sampling period: 1000 microseconds\nThe frequency of the event to be responded to by the system under test, which can either be a timer tick or a GPIO signal.\n  # clock gravity: 2000i 3500k 3500u\nThe calibration settings of the EVL core clock which applied during the test.\n  # clocksource: tsc\nThe name of the kernel clock source used by the EVL core for reading timestamps. This value depends on the processor architecture, tsc commonly refers to x86.\n  # vDSO access: architected\nSince kernel v5.5, the core reports the type of access the EVL applications have to the clock source via the vDSO. The following values are defined:\n  architected denotes a fast (syscall-less) vDSO access to a built-in clock source defined by the architecture itself. This is the best case.\n  mmio denotes a fast (syscall-less) vDSO access to a [clock source] (/dovetail/porting/clocksource/) exported via Dovetail\u0026rsquo;s generic access to MMIO-based devices. This is the second best case.\n  none denotes a not-so-fast access to the kernel clock source without vDSO support, which is one of the possible issues with legacy x86 hardware. You could also have such value due to an incomplete port of Dovetail to your target system, which may be missing the conversion of existing MMIO clock source to a user-mappable one visible from the generic vDSO mechanism.\n    # context: user\nWhich was the context of the responder, among:\n  user for EVL threads running in user-space waiting for timer events,\n  kernel for EVL kernel threads waiting for timer events,\n  irq for EVL interrupt handlers receiving timer events,\n  oob-gpio for EVL threads running in user-space waiting for GPIO events,\n  inband-gpio for regular (non-EVL) threads running in user-space waiting for GPIO events.\n    # thread priority: 98\nThe priority of the responder thread in the out-of-band SCHED_FIFO class. By definition, any legit value starting from 1 and on gives the responder thread higher priority than any in-band task running in the system, including kernel threads of any sort.\n  # thread affinity: CPU1\nThe processor affinity of the responder thread. If the CPU id. is suffixed by -noisol, then the responder was not running on an isolated processor during the test, which most likely entailed higher latency values compared to isolating this CPU - unless you did not stress the SUT enough or at all while measuring, which would make the figures obtained quite flawed and probably worthless anyway.\n  # C-state restricted\nWhether the processor C-State was restricted to the shallowest level, preventing it to enter deeper sleep states which are known to induce extra latency.\n  # duration (hhmmss): 00:01:12\nHow long the test ran (according to the wall clock).\n  # peak (hhmmss): 00:00:47\nWhen the worst case value was observed, relatively to the beginning of the test.\n  # min latency: 0.205\nThe best/shortest latency value observed throughout the test.\n  # avg latency: 3.097\nThe average latency value observed, accounting for all measurement samples collected during the test.\n  # max latency: 25.510\nThe maximum value observed throughout the test. This is the worst case value we should definitely care about, provided the stress load and test duration are meaningful.\n  # sample count: 71598\nThe number of samples collected. The larger, the better (71598 samples in this example is obviously way too small for drawing any meaningful conclusion). Running a test for a period of 24 hours under significant stress load is common practice to get reliable results.\n  Measuring response time to GPIO events In addition to timer events, you will likely need to get a sense of the worst case response time to common device interrupts you may expect from EVL. Since you want the common programming model to be available for developing real-time applications, the responder has to be a thread running in user space. This test mode is also available from the latmus program, when paired with a Zephyr-based application which monitors the response time from a remote system to the GPIO events it sends.\nFrom the perspective of the monitor system, we will measure the time it takes the SUT to not only receive the incoming event, but also to respond to it by sending a converse GPIO acknowledge. Therefore we expect the worst case figures to be higher than those reported by a plain timer test which only gets a timestamp on wake up.\n For implementing this test, we need:\n  a small development board which supports the Zephyr RTOS, and offers external GPIO pins. This GPIO test was originally developed on a FRDM-K64F board, a low-cost development platform from NXP. For this reason, the Zephyr device tree and pinmux bits for enabling the GPIO lines are readily available from this patch.\n  the latency monitoring application, aka latmon, which runs on the Zephyr board. This application periodically sends a pulse on one GPIO (output) line to be received by the system under test, then waits for an acknowledge on another GPIO (input) line, measuring the time elapsed between the two events.\n  the EVL-based system under test, also offering external GPIO pins. This board runs latmus which sets up the test system, asking latmon to configure according to the settings requested by the user, then enters a responder loop which listens to then acknowledges GPIO events to the latency monitor using two separate GPIO lines. The period is chosen by calling latmus using the -p option (which defaults to 1 ms, i.e. 1 Khz sampling loop). This setting and a few others are passed by latmus to latmon during the setup phase via the TCP/IP connection they share.\n  a couple of wires between the GPIO pins of system under test and those of the Zephyr board running latmon, for transmitting pulses and receiving acknowledges to/from the system under test.\n  a network connection between both systems so that latmus can establish a TCP/IP stream with the remote latmon application.\n  a DHCP server reachable from the Zephyr board running on your LAN. The latency monitor asks for its IPv4 address by issuing a DHCP request at boot up.\n  The latmon application running on the monitor board periodically raises a GPIO pulse event on the TX wire, which causes the SUT to receive an interrupt. By calling into the [EVL-enabled gpiolib] (https://git.xenomai.org/xenomai4/linux-evl/-/blob/3451245cdc9846835f8a2786767b17037ee13dda/drivers/gpio/gpiolib-cdev.c) driver, the responder thread created by the latmus application waits for such interrupt by monitoring edges on the GPIO pulse signal. Upon receipt, it immediately acknowledges the event by raising an edge on the GPIO ack signal, which latmon monitors for calculating the latency value as the delay between the acknowledge and pulse events. The latter accumulates these results, sending an intermediate summary every second over a TCP/IP stream to a logger thread running on the remote latmus application. This summary includes the minimum, maximum and average latency values observed over the last 1Hz period. Here again, the 1Hz display loop you can observe while latmus is running is synchronized on the receipt of such summary. The following figure illustrates this execution flow:\nQuick recipe: monitoring a Raspberry PI board with the FRDM-K64F This recipe works with any Raspberry PI board featuring a 40-pin GPIO header. It has been successfully used for monitoring the response time to GPIO events from models 2B, 3B and 4B.\n  Build and install EVL on your Raspberry PI as described in this document.\n  Install the Zephyr SDK on your development system. Once the SDK is installed, if your are using Zephyr for the first time, you may want to get your feet wet with the blinky example. The rest of the description assumes that the Zephyr SDK is rooted at ~/zephyrproject, and the libevl source tree was cloned into ~/libevl.\n  Patch the device tree and pinmux changes to the FRDM-K64F board which enable the GPIO lines in the Zephyr source tree:\n  $ cd ~/zephyrproject/zephyr $ patch -p1 \u0026lt; ~/libevl/benchmarks/zephyr/frdm_k64f-enable-EVL-latency-monitor.patch  Connect the GPIO lines between both boards:\n  GPIO24 on the FRDM-K64F (pulse signal) should be wired to GPIO23 on the RPI3 (ack signal, BCM numbering).\n  GPIO25 on the FRDM-K64F (ack signal) should be wired to GPIO24 on the RPI3 (pulse signal, BCM numbering).\n    An OpenSDA J-Link Onboard Debug Probe is present on the FRDM-K64F, which can be used to flash the board. On your development system, the OpenOCD suite provides GDB remote debugging and flash programming support compatible with this probe over USB. In most cases, a binary OpenOCD package should be readily available from your favorite Linux distribution. Once the OpenOCD suite is installed, you may need to add some udev rules in order for the USB device to appear on your development system, such as these ones.\n  Connect a USB cable from the Open SDA micro-USB connector of the FRDM-K64F to your development system. This will power on the FRDM-K64F board, enabling firmware upload.\n  Build the latmon application using the Zephyr SDK, then flash it to the FRDM-K64F.\n  $ cd ~/zephyrproject/zephyr $ source zephyr-env.sh $ west build -p auto -b frdm_k64f ~/libevl/benchmarks/zephyr/latmon -- west build: build configuration: source directory: ~/libevl/benchmarks/zephyr/latmon build directory: ~/zephyrproject/zephyr/build BOARD: frdm_k64f (origin: command line) ... Memory region Used Size Region Size %age Used FLASH: 100452 B 1 MB 9.58% SRAM: 45044 B 192 KB 22.91% IDT_LIST: 168 B 2 KB 8.20% [156/160] Generating linker_pass_final.cmd [157/160] Generating isr_tables.c [158/160] Building C object zephyr/CMakeFiles/zephyr_final.dir/misc/empty_file.c.obj [159/160] Building C object zephyr/CMakeFiles/zephyr_final.dir/isr_tables.c.obj [160/160] Linking C executable zephyr/zephyr.elf $ west flash -- west flash: rebuilding ninja: no work to do. -- west flash: using runner pyocd -- runners.pyocd: Flashing file: /home/rpm/git/zephyrproject/zephyr/build/zephyr/zephyr.bin [====================] 100% Once (re-)flashed, the FRDM-K64F is automatically reset, with the  latency monitor taking over. You should see the following output in the serial console of the FRDM-K64F:\n*** Booting Zephyr OS build zephyr-\u0026lt;some version information\u0026gt; *** [00:00:00.006,000] \u0026lt;inf\u0026gt; latency_monitor: DHCPv4 binding... [00:00:03.001,000] \u0026lt;inf\u0026gt; eth_mcux: Enabled 100M full-duplex mode. [00:00:03.003,000] \u0026lt;inf\u0026gt; net_dhcpv4: Received: \u0026lt;IP address of the FRDM-K64F\u0026gt; [00:00:03.003,000] \u0026lt;inf\u0026gt; latency_monitor: DHCPv4 ok, listening on \u0026lt;IP\u0026gt;:2306 [00:00:03.003,000] \u0026lt;inf\u0026gt; latency_monitor: waiting for connection... From that point, the latency monitor running on the FRDM-K64F is ready to accept incoming connections from the latmus application running on the Raspberry PI (SUT).\nNative preemption and IRQ threading The GPIO response time of the standard or PREEMPT_RT kernel may be delayed by threading the GPIO interrupt the latmus application monitors, since this behavior is built in the generic GPIOLIB driver, and forced by PREEMPT_RT for most interrupts anyway. The overhead involved in waiting for a context switch to be performed to the threaded handler increases the latency under stress load. Disabling IRQ threading entirely in a single kernel configuration (i.e. without EVL) would be the wrong option though, making the latency figures generally really bad. However, you can raise the priority of the IRQ thread serving the latency pulse above any activity which should not be in its way, so that it is not delayed even further.\nIn order to do this, you first need to locate the IRQ thread which handles the GPIO pulse interrupts. A simple way to achieve this is to check the output of /proc/interrupts once the test runs, looking for the GPIO consumer called latmon-pulse. For instance, the following output was obtained from a PREEMPT_RT kernel running on an i.MX8M SoM:\n~ # cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 3: 237357 90193 227077 224949 GICv3 30 Level arch_timer 6: 0 0 0 0 GICv3 79 Level timer@306a0000 7: 0 0 0 0 GICv3 23 Level arm-pmu 8: 0 0 0 0 GICv3 128 Level sai 9: 0 0 0 0 GICv3 82 Level sai 20: 0 0 0 0 GICv3 110 Level 30280000.watchdog 21: 0 0 0 0 GICv3 135 Level sdma 22: 0 0 0 0 GICv3 66 Level sdma ... 52: 1355491 0 0 0 gpio-mxc 12 Edge latmon-pulse \u0026lt;\u0026lt;\u0026lt; The one we look for 55: 0 0 0 0 gpio-mxc 15 Edge ds1337 80: 0 0 0 0 gpio-mxc 8 Edge bd718xx-irq 84: 0 0 0 0 gpio-mxc 12 Edge 30b50000.mmc cd ... With this information, we can now figure out which IRQ thread is handling the pulse events monitored by latmus, raising its priority as needed. By default, all IRQ threads are normally set to priority 50 in the SCHED_FIFO class. Typically, you may want to raise the priority of this particular IRQ handler so that it does not have to compete with other handlers. For instance, continuing the previous example we would raise the priority of the kernel thread handling IRQ52 to 99:\nYou can refine even futher the runtime configuration of a kernel threading its interrupts by locking the SMP affinity of the device IRQ threads on particular CPUs, in order to either optimize the wake up time of processes waiting for such events, or reduce the jitter in processing the real-time workload. Whether you should move an IRQ thread to the isolated CPU also running the real-time workload in order to favour locality, or keeping them spatially separate in order to reduce the disturbance of interrupt handling on the workload is something you may have to determine on a case-by-case basis.\n ~ # pgrep irq/52 345 ~ # chrt -f -p 99 345 pid 345's current scheduling policy: SCHED_FIFO pid 345's current scheduling priority: 50 pid 345's new scheduling policy: SCHED_FIFO pid 345's new scheduling priority: 99 Which can be shortened as:\n~ # chrt -f -p 99 $(pgrep irq/52) pid 345's current scheduling policy: SCHED_FIFO pid 345's current scheduling priority: 50 pid 345's new scheduling policy: SCHED_FIFO pid 345's new scheduling priority: 99  In a single kernel configuration, the IRQ thread process varies between runs of the latmus application for the GPIO test, because the interrupt descriptor is released at the end of each execution by GPIOLIB. Make sure to apply the procedure explained above each time you spawn a new test.\n Running the GPIO-based test Once the Zephyr board is started with the latmon application flashed in, we can run the benchmark tests on the system under test.\nThis is done by running the latmus application on the SUT, passing either of the -Z or -z option switch to select the execution stage, depending on whether we look for out-of-band response time figures (i.e. using EVL) or plain in-band response time figures (i.e. without relying on EVL\u0026rsquo;s real-time capabilities) respectively. In the latter case, we would not even need EVL to be present in the kernel of the SUT; typically we would use a PREEMPT_RT kernel instead.\nRegardless of the execution stage they should run on, both tests are configured the same way. On the latmus application command line, we need to specify which GPIO chip and pin number should be used for receiving GPIO events (-I \u0026lt;gpiochip-name\u0026gt;,\u0026lt;pin-number\u0026gt;) and sending acknowledge signals (-O \u0026lt;gpiochip-name\u0026gt;,\u0026lt;pin-number\u0026gt;).\nWhen the test is started on the SUT, the latmon application should start monitoring the GPIO response times of the latmus application indefinitely until the latter stops, at which point the latency monitor goes back waiting for another connection.\n[00:04:15.651,000] \u0026lt;inf\u0026gt; latency_monitor: monitoring started /* ...remote latmus runs for some time... */ [00:04:24.877,000] \u0026lt;inf\u0026gt; latency_monitor: monitoring stopped [00:04:24.879,000] \u0026lt;inf\u0026gt; latency_monitor: waiting for connection...  Measuring out-of-band response time to GPIO events (on the SUT)\n /* * Caution: the following output was produced by running the test only * a few seconds on an idle EVL-enabled system: the results displayed do not * reflect the worst case latency (which is higher) on this platform when * the test runs long enough under proper stress load. */ # latmus -Z zephyr -I gpiochip0,23 -O gpiochip0,24 connecting to latmon at 192.168.3.60:2306... RTT| 00:00:02 (oob-gpio, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 11.541| 15.627| 18.125| 0| 0| 11.541| 18.125 RTD| 10.916| 15.617| 30.950| 0| 0| 10.916| 30.950 RTD| 12.500| 15.598| 25.908| 0| 0| 10.916| 30.950 RTD| 1.791| 15.571| 25.908| 0| 0| 1.791| 30.950 RTD| 13.958| 15.647| 26.075| 0| 0| 1.791| 30.950 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 1.791| 15.606| 30.950| 0| 0| 00:00:05/00:00:05  Measuring in-band response time to GPIO events (on the SUT)\n /* * Caution: the following output was produced by running the test only * a few seconds on an idle PREEMPT_RT-enabled system: the results displayed do * not reflect the worst case latency (which is higher) on this platform when * the test runs long enough under proper stress load. */ # latmus -z zephyr -I gpiochip0,23 -O gpiochip0,24 connecting to latmon at 192.168.3.60:2306... CAUTION: measuring in-band response time (no EVL there) RTT| 00:00:02 (inband-gpio, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 38.075| 52.401| 93.733| 0| 0| 38.075| 93.733 RTD| 39.700| 53.289| 91.608| 0| 0| 38.075| 93.733 RTD| 41.283| 54.914| 93.900| 0| 0| 38.075| 93.900 RTD| 38.075| 54.615| 91.608| 0| 0| 38.075| 93.900 RTD| 38.075| 54.767| 96.108| 0| 0| 38.075| 96.108 RTD| 41.283| 54.563| 91.608| 0| 0| 38.075| 96.108 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 38.075| 54.037| 96.108| 0| 0| 00:00:07/00:00:07 Configuring the test kernel for benchmarking Hints for configuring any test kernel   turn off all debug features and tracers in the kernel configuration.\n  ensure all CPUs keep running at maximum frequency by enabling the \u0026ldquo;performance\u0026rdquo; CPU_FREQ governor, or disabling CPU_FREQ entirely.\n  have a look at the caveats here.\n  make sure the GPU driver does not cause ugly latency peaks.\n  isolate a CPU for running the latency test. For instance, you could reserve CPU1 for this purpose, by passing isolcpus=1 on the kernel command line at boot.\n  verify your kernel configuration either offline or online using the evl check command.\n  Specific hints to configure a native preemption kernel   enable maximum preemption (CONFIG_PREEMPT_RT_FULL if available).\n  The GPIO response test over PREEMPT_RT involves a threaded interrupt handler for processing GPIO events. This interrupt thread must have higher scheduling priority than other interrupt threads. See this illustration for more.\n  check that no common thread can compete with the responder thread on the same priority level. Some kernel housekeeping threads might, but regular threads should not. For latency measurement, setting the scheduling parameters of the responder thread to [SCHED_FIFO, 98] is recommended. Setting the responder to priority 99 would work for this purpose as well (with no observable gain actually), however it is not recommended to allow real-world application threads to compete with threaded IRQ handlers and other critical housekeeping tasks running in kernel space, since the latter might assume that no user code may preempt in tricky corner cases.\n  switch to a non-serial terminal (ssh, telnet). Although this problem is being worked on upstream, significant output to a serial device might affect the worst case latency on some platforms with native preemption because of the implementation issues in console drivers, so the console should be kept quiet. You could also add the \u0026ldquo;quiet\u0026rdquo; option to the kernel boot arguments as an additional precaution.\n  Specific hints to configure an EVL-enabled kernel   turn on CONFIG_EVL in the configuration.\n  turn off all features from the CONFIG_EVL_DEBUG section. The cost of leaving the watchdog enabled should be marginal on the latency figures though.\n  if you plan to measure the response time to GPIO interrupts, you will need CONFIG_GPIOLIB and CONFIG_GPIOLIB_OOB to be enabled in the kernel configuration. You will also need to enable the GPIO pin control driver for your hardware platform, which must be GPIOLIB-compliant (most are these days), and out-of-band capable.\n  The issue of proper stress load Finding the worst case latency in measuring the response time to interrupts requires applying a significant stress load to the system in parallel to running the test itself. There have been many discussions about what significant should mean in this context. Some have argued that real-time applications should have reasonable requirements, defined by a set of restriction on their behavior and environment, so that bounded response time can be guaranteed, which sounds like asking application developers to abide by the rules defined by kernel folks. When was the last time any of them did so anyway?\nObviously, we cannot ask the infrastructure to be resilient to any type of issue, including broken hardware or fatal kernel bugs. Likewise, it is ok to define and restrict which API should be used by applications to meet their real-time requirements. For instance, there would be no point in expecting low and bounded latency from all flavours of clone(2) calls, or whenever talking to some device involves a slow bus interface like i2c. Likewise, we may impose some restrictions on the kernel when it deals with these applications, like disabling ondemand loading and copy-on-write mechanisms with mlock(2).\n Per Murphy\u0026rsquo;s laws, we do know that there is no point in wishful thinking, like hoping for issues to never happen provided that we always do reasonable things which would meet some hypothetical standard of the industry. Application developers do not always do reasonable things, they just do what they think is best doing, which almost invariably differs from what kernel folks want or initially envisioned. After all, the whole point of using Linux in this field is the ability to combine real-time processing with the extremely rich GPOS feature set such system provides, so there is no shortage of options and varieties. Therefore, when it comes to testing a real-time infrastructure, let\u0026rsquo;s tickle the dragon\u0026rsquo;s tail.\n There are many ways to stress a system, often depending on which kind of issues we would like to trigger, and what follows does not pretend to exhaustiveness. This said, these few aspects have proved to be relevant over time when it comes to observing the worst case latency:\n  Each time the kernel needs to switch between tasks which belong to distinct user address spaces, some MMU operations have to be performed in order to change the active memory context, which might also include costly cache maintenance in some cases. Those operations tend to take longer when the cache and memory subsystems have been under pressure at the time of the switch. Because the context switching process runs with interrupts disabled in the CPU, the higher the task switch rate, the more likely such extended interrupt masking may delay the response time to an external event.\n  How the response time of the real-time infrastructure is affected by unfavourable cache situations is important. While no real-time work is pending, the real-time infrastructure just sleeps until the next event to be processed arrives. In the meantime, GPOS (as in non-realtime) activities may jump in, mobilizing all the available hardware resources for carrying out their work. As they do this, possibly treading on a lot of code and manipulating large volumes of data, the real-time program is gradually evicted from the CPU caches. When it resumes eventually in order to process an incoming event, it faces many cache misses, which induce delays. For this reason, and maybe counter-intuitively at first, the faster the timed loop the responder thread undergoes, the fewer the opportunities for the GPOS work to disturb the environment, the better the latency figures (up to a certain rate of course). On the contrary, a slower loop increases the likeliness of cache evictions when the kernel runs GPOS tasks while the real-time system is sleeping, waiting for the next event. If the CPU caches have been disturbed enough by the GPOS activities from the standpoint of the real-time work, then you may get closer to the actual worst case latency figures.\nIn this respect, the - apparently - dull dd(1) utility may become your worst nightmare as a real-time developer if you actually plan to assess the worst-case latency with your system. For instance, you may want to run this stupid workload in parallel to your favourite latency benchmark (hint: CPU isolation for the real-time workload won\u0026rsquo;t save the day on most platforms):\n$ dd if=/dev/zero of=/dev/null bs=128M \u0026amp;  Using a block factor of 128M is to make sure the loop will be disturbing the CPU caches enough. Too small a value here would only create a mild load, barely noticeable in the latency figures on many platforms.\n   A real-time application system is unlikely to be only composed of a single time-critical responder thread. We may have more real-time threads involved, likely at a lower priority though. So we need to assess the ability of the real-time infrastructure to schedule all of these threads efficiently. In this case, we want the responder thread to compete with other real-time threads for traversing the scheduler core across multiple CPUs in parallel. Efficient serialization of these threads within a CPU and between CPUs is key.\n  Since we have to follow a probabilistic approach for determining the worst case latency, we ought to run the test long enough in order to increase the likeliness of exercizing the code path(s) which might cause the worst latency. Practically, running the test under load for 24 hours uninterrupted may deliver a worst case value we can trust.\n  Defining the stress workloads  A WORD OF CAUTION: several stress workloads mentioned in this section are likely to ignite the CPUs of your test machine(s) quite badly. Before any attempt is made at running any stress workload, you do have to check for potential thermal issues with your hardware first. Typically, the Raspberry PI 4B was known to suffer from overheating until a firmware revision greatly improved the situation.\n Using Linux for running real-time workloads means that we have to meet contradictory requirements on a shared hardware, maximum throughtput and guaranteed response time at the same time, which on the face of it looks pretty insane, therefore interesting. Whichever real-time infrastructure we consider, we have to assess how badly non real-time applications might hurt the performances of real-time ones. With this information, we can decide which is best for supporting a particular real-time application on a particular hardware. To this end, the following stress workloads are applied when running benchmarks:\n  As its name suggests, the Idler workload is no workload at all. Under such conditions, the response time of the real-time system to some event (i.e. timer or GPIO interrupt) is measured while the GPOS is doing nothing in particular except waiting for something to do. The purpose is to get a sense of the best case we might achieve with a particular hardware and software configuration, unimpeded by GPOS activities.\n  The Scary Grinder workload combines hackbench loops to a continuous dd(1) copy from /dev/zero to /dev/null with a large block size (128Mb). This workload most often causes the worst latency spots on a platform for any type of real-time infrastructure, dual kernel and native preemption (PREEMPT_RT). As it pounds the CPU caches quite badly, it reveals the inertia of the real-time infrastructure when it has to ramp up quickly from an idle state in order to handle an external event. The more complex the code code paths involved in doing so, the longer the response time. Do not expect ugly figures on Big Irons and other high-end x86 though, however this workload is likely to thrash common embedded systems. The shell commands to start this workload are:\n~ # while :; do hackbench; done \u0026amp; ~ # dd if=/dev/zero of=/dev/null bs=128M \u0026amp;   The Pesky Neighbour workload is based on the stress-ng test suite. Several \u0026ldquo;stressors\u0026rdquo; imposing specific loads on various kernel subsystems are run sequentially. The goal is to assess how sensitive the real-time infrastructure is to the pressure non real-time applications might put on the system by using common kernel interfaces. Given the ability some stress-ng stressors have to thrash and even break the system, we limit the amount of damage they can do with specific settings, so that the machine stays responsive throughout long-running tests. With this workload, we generally focus on stressors which affect the scheduler and the memory subsystem.\n  Common set up rules for all benchmarks   The kernel configuration is double-checked for features which may have an adverse impact on the latency figures. Typically, for benchmarking purpose only, all debug options are turned off for both types of real-time infrastructures, dual kernel like EVL or native preemption (PREEMPT_RT). evl check can be used to verify a kernel configuration.\n  On a multi-core system, we always reserve CPU1 for running the test code which is monitored for response time, since native preemption requires some CPU(s) to be dedicated to the real-time workload for best results. This implies that isolcpus=1 evl.oobcpus=1 are passed to the kernel as boot options. With stress-ng, we also restrict the CPUs usable for running the stress load using the --taskset option. Assuming CPU1 is the only isolated CPU running a test, this shell expression should produce the correct CPU set for that option: 0,2-$(nproc).\n  Some limits should be put on what the stress workload is allowed to do, in order to avoid bringing the machine down to a complete stall, or triggering OOM situations which may have unwanted outcomes such as wrecking the test (or even the machine) entirely. Since we are not in the business of crash testing, we need to set these limits depending on the compute power of the test hardware.\n  All threads running some stress workload should belong to the SCHED_OTHER scheduling class. They should NOT compete with the real-time threads whose response time is being monitored during the test. Although this would have actually no impact on the real-time performances of a dual kernel system, this would lead to an unfair comparison with a native preemption system such as PREEMPT_RT which is sensitive to this issue by design. For instance, we ensure this with stress-ng by excluding all stressors implicitly involving the SCHED_FIFO or SCHED_RR policies, setting the --sched parameter to other for good measure.\n  PREEMPT_RT specific tweaks for benchmarking   With native preemption, if a particular test involves responding to a device interrupt such as the GPIO response test served by a threaded handler, we make sure to raise the task priority of such handler above all others. This rule does not apply to a dual kernel infrastructure like EVL, which always processes IRQ events immediately from the interrupt context.\n  on x86, PREEMPT_RT v5.6.10-rt5 may trigger a BUG() assertion then crash as a result of stress-ng injecting a memory error via the madvise(2) interface if CONFIG_MEMORY_FAILURE is enabled in the kernel configuration. This is typically something the Pesky workload we apply while monitoring latency would do, so you need to disable CONFIG_MEMORY_FAILURE to prevent this.\n  The benchmark scenarios We want to characterize the following aspects of a real-time infrastructure:\n  its response time to external interrupts in the worst (observed) case when the GPOS side of the system is under stress. From this information we should be able to get a clear picture of the typical applications such infrastructure can best support, in terms of low latency and jitter requirements. We will be using the following test programs to do so, for measuring the response time to timer and GPIO events respectively:\n  for EVL, the latmus program in timer response mode and GPIO response mode.\n  for PREEMPT_RT, the cyclictest program, and the latmus program in GPIO response mode which can run without EVL support.\n    the overhead it adds to the GPOS side as a result of running the in-kernel real-time machinery. Because we aim at sharing a single hardware between GPOS and RTOS activities, we need hints about the actual cost of running the real-time infrastructure on the overall system. In many cases, especially in the embedded space, the ability to downsize the compute power required for running mixed applications is valuable. There is no test program ready for this part yet. Any thought about the way to best do this, contribution of any sort is welcome.\n   Last modified: Mon, 21 Jun 2021 14:06:41 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/caveat/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Things you definitely want to know Generic issues isolcpus is our friend too Isolating some CPUs on the kernel command line using the isolcpus= option, in order to prevent the load balancer from offloading in-band work to them is not only a good idea with PREEMPT_RT, but for any dual kernel configuration too.\nBy doing so, having some random in-band work evicting cache lines on a CPU where real-time threads briefly sleep is less likely, increasing the odds of costly cache misses, which translates positively into the latency numbers you can get. Even if EVL\u0026rsquo;s small footprint core has a limited exposure to such kind of disturbance, saving a handful of microseconds is worth it when the worst case figure is already within tenths of microseconds.\nCONFIG_DEBUG_HARD_LOCKS is cool but ruins real-time guarantees When CONFIG_DEBUG_HARD_LOCKS is enabled, the lock dependency engine (CONFIG_LOCKDEP) which helps in tracking down deadlocks and other locking-related issues is also enabled for Dovetail\u0026rsquo;s hard locks, which underpins most of the serialization mechanisms the EVL core uses.\nThis is nice as it has the lock validator monitor the hard spinlocks EVL uses too. However, this comes with a high price latency-wise: seeing hundreds of microseconds spent in the validator with hard interrupts off from time to time is not uncommon. Running the latency monitoring utility (aka latmus) which is part of libevl in this configuration should give you pretty ugly numbers.\nIn short, it is fine enabling CONFIG_DEBUG_HARD_LOCKS for debugging some locking pattern in EVL, but you won\u0026rsquo;t be able to meet real-time requirements at the same time in such configuration.\nCPU frequency scaling (usually) has a negative impact on latency Enabling the ondemand CPUFreq governor - or any governor performing dynamic adjustment of the CPU frequency - may induce significant latency for EVL on your system, from ten microseconds to more than a hundred depending on the hardware. Selecting the so-called performance governor is the safe option, which guarantees that no frequency transition ever happens, keeping the CPUs at their maximum processing speed.\nIn other words, if CONFIG_CPU_FREQ has to be enabled in your configuration, enabling CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE and CONFIG_CPU_FREQ_GOV_PERFORMANCE exclusively is most often the best way to prevent unexpectedly high latency peaks.\nDisable CONFIG_SMP for best latency on single-core systems On single-core hardware, some out-of-line code may still be executed for dealing with various types of spinlock with a SMP build, which translates into additional CPU branches and cache misses. On low end hardware, this overhead may be noticeable.\nTherefore, if you neither need SMP support nor kernel debug options which depend on instrumenting the spinlock constructs (e.g. CONFIG_DEBUG_PREEMPT), you may want to disable all the related kernel options, starting with CONFIG_SMP.\nArchitecture-specific issues x86   GCC 10.x might generate code causing the SMP boot process to break early, as reported by this post. As a work-around, you can disable CONFIG_STACKPROTECTOR_STRONG from your kernel configuration.\n  CONFIG_ACPI_PROCESSOR_IDLE may increase the latency upon wakeup on IRQ from idle on some SoC (up to 30 us observed) on x86. This option is implicitly selected by the following configuration chain: CONFIG_SCHED_MC_PRIO → CONFIG_INTEL_PSTATE → CONFIG_ACPI_PROCESSOR. If out-of-range latency figures are observed on your x86 hardware, turning off this chain may help.\n  When the HPET is disabled, the watchdog which monitors the sanity of the current clocksource for the kernel may use refined-jiffies as the reference clocksource to compare with. Unfortunately, such clocksource is fairly imprecise for timekeeping since timer interrupts might be missed. This could in turn trigger false positives with the watchdog, which would end up declaring the TSC clocksource as \u0026lsquo;unstable\u0026rsquo;. For instance, it has been observed that enabling CONFIG_FUNCTION_GRAPH_TRACER on some legacy hardware would systematically cause such behavior at boot. The following warning splat appearing in the kernel log is symptomatic of this problem:\nclocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc-early' as unstable because the skew is too large: clocksource: 'refined-jiffies' wd_now: fffb7018 wd_last: fffb6e9d mask: ffffffff clocksource: 'tsc-early' cs_now: 68a6a7070f6a0 cs_last: 68a69ab6f74d6 mask: ffffffffffffffff tsc: Marking TSC unstable due to clocksource watchdog This is a problem because the TSC is the best-rated clocksource and directly accessible from the vDSO, which speeds up timestamping operations. If the TSC on your hardware is known to be fine and face this issue nevertheless, you may want to pass tsc=nowatchdog to the kernel to prevent it, or even tsc=reliable if all TSCs are reliable enough to be synchronized across CPUs. If the TSC is really unstable on some legacy hardware and you cannot ignore the watchdog alert, you can still leave it to other clocksources such as acpi_pm. Calls to evl_read_clock() would be slower compared to a direct syscall-less readout from the vDSO, but the EVL core would nevertheless manage to get timestamps from its built-in clocks at the expense of an out-of-band system call, without involving the in-band stage though. You definitely want to make sure everything is right on your platform with respect to reading timestamps by running the latmus test, which can detect any related issue.\nYou can retrieve the current clocksource used by the kernel as follows:\n  # cat /sys/devices/system/clocksource/clocksource0/current_clocksource tsc  NMI-based perf data collection may cause the kernel to execute utterly sluggish ACPI driver code at each event. Since disabling CONFIG_PERF is not an option, passing nmi_watchodg=0 on the kernel command line at boot may help.  Passing nmi_watchodg=0 turns off the hard lockup detection for the in-band kernel. However, EVL will still detect runaway EVL threads stuck in out-of-band execution if CONFIG_EVL_WATCHDOG is enabled.\n  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/",
	"title": "Using libevl",
	"tags": [],
	"description": "",
	"content": "What is an \u0026ldquo;EVL application process\u0026rdquo;? An EVL application process is composed of one or more EVL threads, running along with any number of regular POSIX threads.\n  an EVL thread is initially a plain regular POSIX thread spawned by a call to pthread_create(3) or the main() context which has issued the evl_attach_self() system call. This service binds the caller to the EVL core, which enables it to invoke the real-time, ultra-low latency services the latter delivers.\n  once attached, such thread may call EVL core services, until it detaches from the core by a call to evl_detach_self(), or exits, whichever comes first.\n  whenever an EVL thread has to meet real-time requirements, it must rely on the services provided by libevl exclusively. If such a thread invokes a common C library service while in the middle of a time-critical code, the EVL core does keep the system safe by transparently demoting the caller to in-band context. However, the calling thread would loose any real-time guarantee in the process, meaning that unwanted jitter would happen.\n  To sum up, the lifetime of an EVL application usually looks like this:\n  When a new process initializes, a regular thread - often the main() one - invokes routines from the C library in order to get common resources, like opening files, allocating memory buffers and so on. At some point later on, this thread calls evl_attach_self() in order to bind itself to the EVL core, which in turn allows it to create other EVL objects (e.g. evl_create_mutex(), evl_create_event(), evl_create_xbuf()). If the application needs more EVL threads, it simply spawns additional POSIX threads using pthread_create(3), then ensures those threads bind themselves to the core with a call to evl_attach_self().\n  Each EVL thread runs its time-critical work loop, only calling EVL services which operate from the out-of-band context, therefore guaranteeing bounded, ultra-low latency. The pivotal EVL service from such loop has to be a blocking call, waiting for the next real-time event to process. For instance, such call could be evl_wait_flags(), evl_get_sem(), evl_poll(), oob_read() and so on.\n  Eventually, EVL threads may call common C library services in order to cleanup/unwind the application context when their time-critical loop is over and time has come to exit.\n  This page is an index of all EVL system calls available to applications, which should help you finding out which call is legit from which context. In order to use the ultra-low latency EVL services, you need to link your application code against the libevl library which provides the EVL system call wrappers.\nIs libevl a replacement for my favourite C library? (glibc, musl, uClibc etc.) NO, not even remotely. This is a drop-in complement to the common C library and NPTL support you may be using, which enables your thread(s) of choice to be scheduled with ultra-low latency guarantee by the EVL core. As it should be clear now from the above section, you may - and actually have to - use a combination of these libraries into a single application, but you must do this in a way that ensures your time-critical code only relies on either:\n  libevl\u0026rsquo;s well-defined set of low-latency services which operate from the out-of-band context.\n  a (very) small subset of the common C library which is known not to depend on regular in-band kernel services. In other words, only routines which do not issue common Linux system calls directly or indirectly are fine in a time-critical execution context. For instance, routines from the string(3) section may be fine in a time-critical code, like strcpy(3), memcpy(3) and friends. At the opposite, any routine which may directly or indirectly invoke malloc(3) must be banned from your time-critical code, which includes stdio(3) routines, or C++ default constructors which rely on the standard memory allocator.\n  Outside of those time-critical sections which require the EVL core to guarantee ultra-low latency scheduling for your application threads, your code may happily call whatever service from whatever C library.\nWhat about multi-process applications? libevl does not impose any policy regarding how you might want to organize your application over multiple processes. However, the design and implementation of the interface to the EVL core makes sharing EVL resources between processes a fairly simple task. EVL elements can be made visible as common devices, such as threads, mutexes, events, semaphores. Therefore, every element you may want to share can be exported to the device file system, based on a visibility attribute mentioned at creation time.\nIn addition, EVL provides a couple of additional features which come in handy for sharing data between processes:\n  a general memory-sharing mechanism based on the file proxy, which is used as an anchor point for memory-mappable devices.\n  the tube data structure, which is a lightweight FIFO working locklessly, you can also use for inter-process messaging.\n  the Observable element which gives your application a built-in support for implementing the observer design pattern among one or more processes transparently.\n  Visibility: public and private elements As hinted earlier, EVL elements created by the user API can be either publically visible to other processes, or private to the process which creates them. This is a choice you make at creation time, by passing the proper visibility attribute to any of the evl_create_*() system calls, either EVL_CLONE_PUBLIC or EVL_CLONE_PRIVATE.\nA public element is represented in the /dev/evl hierarchy by a device file, which is visible to any process. Once a file descriptor is available from opening such file, it can be used to send requests to the element it refers to.\nConversely, a private element has no presence in the /dev/evl hierarchy. Only the process which created such element receives a file descriptor referring to it, directly from the creation call.\nThe /dev/evl device file hierarchy Because of its everything-is-a-file mantra, EVL exports a number of device files in the /dev/evl hierarchy, which lives in the DEVTMPFS file system. Each device file either represents an active public element, or a special command device used internally by libevl.\nIn opening a public element device, an application receives a file descriptor which can be used to submit requests to the underlying element. For instance, the scheduling parameters of a thread running in process A could be changed by a thread running in process B by a call to evl_set_schedattr() using the proper file descriptor.\nElement device files are organized in directories, one for each element class: clock, monitor, proxy, thread, cross-buffer and observable; general command devices appear at the top level, such as control, poll and trace:\n~ # cd /dev/evl /dev/evl # ls -l total 0 drwxr-xr-x 2 root root 80 Jan 1 1970 clock crw-rw---- 1 root root 246, 0 Jan 1 1970 control drwxr-xr-x 2 root root 60 Apr 18 17:38 monitor drwxr-xr-x 2 root root 60 Apr 18 17:38 observable crw-rw---- 1 root root 246, 3 Jan 1 1970 poll drwxr-xr-x 2 root root 60 Apr 18 17:38 proxy drwxr-xr-x 2 root root 60 Apr 18 17:38 thread crw-rw---- 1 root root 246, 6 Jan 1 1970 trace drwxr-xr-x 2 root root 60 Apr 18 17:38 xbuf Inside each class directory, the live public elements of that class are visible, in addition to the special clone command device. For the curious, the role of this special device is documented in the under-the-hood section.\n/dev/evl/thread # ls -l total 0 crw-rw---- 1 root root 246, 1 Jan 1 1970 clone crw-rw---- 1 root root 244, 0 Apr 19 10:45 timer-responder:2562 Managing access permissions to EVL device files In some situations, you may want to restrict access to EVL devices files present in the /dev/evl file hierarchy to a particular user or group of users. Because a kernel device object is associated to each live EVL element in the system, you can attach rules to UDEV events generated for public EVL elements or special command devices appearing in the /dev/evl file hierarchy, in order to set up their ownership and access permissions at creation time.\nFor public elements which come and go dynamically, EVL enforces a simple rule internally to set the initial user and group ownership of any element device file, which is to inherit it from the clone device file of the class it belongs to. For instance, if you set the ownership of the /dev/evl/thread/clone device file via some UDEV rule to square.wheel, all public EVL threads will belong at creation time to user square, group wheel.\nElement names Every evl_create_*() call which creates a new element, along with evl_attach_thread() accepts a printf-like format string to generate the element name. A common way of generating unique names is to include the calling process\u0026rsquo;s pid somewhere into the format string, so that you may start multiple instances of the same application without running into naming conflicts. The requirement for a unique name does not depend on the visibility attribute: distinct elements must have distinct names, regardless of their visibility. For instance:\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; ret = evl_attach_self(\u0026quot;a-private-thread:%d\u0026quot;, getpid()); ~# ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 248, 1 Apr 17 11:59 clone The generated name is used to create a /sysfs attribute directory exporting the state information about the element. For public elements, a device file is created with the same name in the /dev/evl hierarchy, for accessing the element via the open(2) system call. Therefore, a name must contain only valid characters in the context of a file name.\nAs a shorthand, libevl forces in the EVL_CLONE_PUBLIC visibility attribute whenever the element name passed to the system call starts with a slash \u0026lsquo;/\u0026rsquo; character, in which case this leading character will be skipped to form the actual element name:\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; ret = evl_attach_self(\u0026quot;/a-publically-visible-thread:%d\u0026quot;, getpid()); ~# ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 248, 1 Apr 17 11:59 clone crw-rw---- 1 root root 246, 0 Apr 17 11:59 a-publically-visible-thread Note that when found, such shorthand overrides the EVL_CLONE_PRIVATE visibility attribute which might have been mentioned in the creation flags for the same call. The slash character is invalid in any other part of the element name, although it would be silently remapped to a placeholder for private elements without leading to an API error.\n Last modified: Sun, 06 Dec 2020 16:34:48 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/wait/",
	"title": "Wait queue",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://the-going.github.io/website/core/oob-drivers/",
	"title": "Real-time I/O drivers",
	"tags": [],
	"description": "",
	"content": "The real-time I/O support EVL provides for is based on the ability some kernel drivers have to handle out-of-band interrupts triggered by the hardware, along with out-of-band requests issued by applications. To achieve this, such driver may depend on the EVL kernel API which in turn depends on Dovetail. This way, both the incoming events and outgoing requests are expedited by the EVL core, without being delayed by the regular GPOS work which is still handled normally on the in-band execution stage.\nIntegrated vs separated real-time I/O support Over the years, the usual approach followed by dual kernel systems in order to provide for real-time I/O support has been to build their own separate, ad hoc driver stack, re-implementing a marginal portion of the common driver stack. The arguments invoked for doing so have been that:\n  a strict separation between both code bases would prevent any non-deterministic behavior from the common Linux driver stack to spill over into the real-time execution, by design.\n  such a separate implementation would be required in order to optimize for real-time performance.\n  the real-time driver stack would be easier to maintain, not being affected by conflicting updates to the original kernel code it is based on.\n  The typical way to implement such a driver is to start from a copy of a mainline kernel driver available for the target device, amending the sources heavily in order to use the real-time core API instead of the common kernel API, dropping any code which would not be required for supporting the real-time applications in the same move. At the end of the day, such real-time variant of a driver has diverged into an almost irreconcilable fork of the original code.\nUnfortunately, after two decades using this model in the RTAI and Xenomai 3 projects, we ended up with a massive issue which is a severe bit rotting of the real-time driver stack. Because the dual kernel ecosystem runs on very few contributors despite it has many industrial users, every real-time driver once merged would receive little to no maintenance effort in the long run. As a result, continuous updates to the original mainline driver which address bugs, extend support to new hardware models and versions, is lost for the real-time driver. In turn, the consequences for such dual kernel systems are bleak:\n  narrower hardware support than mainline drivers have.\n  more reliability issues than mainline drivers suffer from.\n  high cost of updating the real-time driver variant with the bug fixes and improvements available from the original mainline implementation, which in most cases discourages potential contributors from tackling such a task.\n  Instead of reinventing the wheel with a separate driver stack, the option EVL follows builds on these observations:\n  we can define the notion of operating mode for most common drivers, either in-band or out-of-band. Device probing, initialization, changes in basic device management are in-band operations by nature, which should never overlap with out-of-band I/O transactions aimed at transferring data since the latter are supposed to be expedited and time-bound. So both aspects of a real-time capable driver should be able to coexist, although active at different times, on a mutually exclusive basis.\n  we would only need a limited subset of the Linux driver stack to deliver ultra-low latency performances via out-of-band execution. DMA, SPI, GPIO, UART, network interface, acquisition cards, CAN and other fieldbus devices are among those which are typically involved in systems with real-time requirements.\n  With this in mind, the idea would be to add out-of-band capabilities to the existing mainline drivers we are interested in for dealing with real-time I/O, while keeping the original, in-band support available. These capabilities would be known from the EVL core, and exclusively accessible via its API under well-defined conditions. When sharing the implementation with an existing driver is not possible, either because adding out-of-band capabilities would be clumsly, or simply because there is no mainline driver for the target device (e.g. some custom FPGA), it should be a non-issue to implement a dedicated real-time driver from scratch, using the EVL kernel API.\nSuch approach is not at odds with the motivations which prevailed for using a strictly separated driver stack though:\n  there are two ways the GPOS kernel code can adversely affect the real-time behavior of the out-of-band code:\n  if the out-of-band code spuriously calls into the common, in-band kernel API. In such a case, Dovetail can detect whether some code running on the out-of-band stage is wrongfully calling into the non real-time code, so such fact would hardly go unnoticed. Which API calls are legit in the context of out-of-band processing, and which are not, is well-defined for EVL and such rules are applicable to any out-of-band code regardless of its location. In this respect, having a separate implementation for the driver stack brings no upside.\n  if the in-band code changes the hardware state in a way which may cause high latency peaks (e.g. lengthy cache maintenance operations). In this respect, if a real-time capable driver is a fork of a mainline kernel driver, any pre-existing issue of that kind in the latter implementation would be initially imported into the former, by definition. Therefore, a careful audit of the code is required in any case, whether it is forked off of a mainline driver or we are adding out-of-band capabilities to the original driver directly.\n    there is no reason for the out-of-band services available from a common driver to underperform if the implementation clearly gives exclusive access to the hardware to the EVL core while out-of-band requests are being processed. For instance, reserving exclusive access to a SPI bus for out-of-band transfers only until we are done with an entire session is an acceptable restriction on in-band usage for the purpose of delivering real-time performances. Once such out-of-band runtime mode is left, the driver becomes usable anew for regular, in-band operations. In some cases, I/O transaction management might also be entirely left to the out-of-band code, proxying in-band I/O requests via some low-priority queue which would be processed by the former on idle time, provided we can still meet the real-time requirements doing so.\n  although the risk of merge or logic conflicts does exist by definition as the extended driver is rebased over later kernel releases, it seems an acceptable burden compared to bit rotting of a separate code base, which is definitely not. How soundly the out-of-band support is integrated into the original driver code will make a difference when it comes to rebasing it.\n  Would such integrated approach cover all the needs for real-time I/O in a dual kernel system such as EVL? Certainly not. Typically, when drivers are endpoints of a complex protocol stack such as an IP network stack attached to network interfaces, the issue of handling such protocol within a bounded execution time would still not be solved. In that particular case, a separate real-time capable IP stack is going to be needed too. However, finding a proper way to extend existing NIC drivers to serve as endpoints in this real-time IP stack seems a more tractable problem than maintaining a truckload of functionally redundant, separate drivers like RTnet currently requires.\nWhy doesn\u0026rsquo;t EVL implement RTDM? RTDM as an abstract driver model for dual kernel systems was aimed at addressing two major issues with the latter in the early days:\n  provide a common call interface between applications and the real-time core, in order to replace the variety of ad hoc mechanisms which application developers came up with over time. Some would use FIFOs, others shared memory, others some specific system call only available from a given flavour of dual kernel system, and so on. Every application would come with its own I/O interface to the kernel, which was kind of weird. RTDM replaced all of them by a single POSIXish API, which strives to mimic the common character-based interface, and socket API.\n  establish a common kernel API which all RTDM drivers would use, so that they would be portable across multiple flavours of dual kernel systems implementing the RTDM interface.\n  RTDM enshrines the notion that a dual kernel system should provide for its own driver stack aside of the Linux driver model, as a result it opposes in principle to what EVL aims at. Achieving a closer integration of real-time I/O support into the mainline kernel code whenever possible is a fundamental goal of EVL.\nTo this end, EVL extends both the regular Linux character-based I/O and socket interfaces between applications and real-time drivers to support out-of-band operations on common files and sockets.\nAs a consequence, there is not much point in implementing the RTDM interface over EVL, except maybe as a compatibility layer for porting RTAI or Xenomai 3 Cobalt-originated drivers, although this would be easily done by converting them directly to the EVL kernel API without needing any wrapper of the kind. As a matter of fact, the EVL kernel API and the portion of the Cobalt core API which has been used as a reference when designing RTDM share all key semantics.\nOut-of-band capable I/O drivers This table gives an overview of the current support for real-time I/O in EVL, which is not much yet, but poised to improve since the infrastructure is ready:\n  #oobdrv { width: 70%; align: left; } #oobdrv td:nth-child(1) { text-align: center; } #oobdrv th:nth-child(1) { text-align: center; }   Device Type Support   DMA bcm2835-dma, imx-sdma   SPI spi-bcm2835   GPIO * gpio-mxc    * In theory, any GPIO pin controller based on the generic GPIOLIB services should be real-time ready since the latter implements the out-of-band interface for all of them, at the exception of controllers which can sleep from their -\u0026gt;get() and -\u0026gt;set() pin state handlers. In practice, each controller we may want to use in such context should still be audited though, in order to make sure that no in-band service (e.g. common non-raw spinlocks) is called under the hood by these handlers. Only the controllers which are known to work in out-of-band mode at the time of this writing are listed in the table.\n Last modified: Sun, 06 Jun 2021 18:28:51 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/",
	"title": "Dovetail interface",
	"tags": [],
	"description": "",
	"content": "Introducing Dovetail Using Linux as a host for lightweight software cores specialized in delivering very short and bounded response times has been a popular way of supporting real-time applications in the embedded space over the years.\nThis dual kernel design introduces a small real-time infrastructure into the Linux kernel, which immediately handles time-critical, out-of-band activities independently from the ongoing general purpose kernel work. Application threads co-managed by this infrastructure still benefit from the common kernel services such as virtual memory management; they can leverage the rich GPOS feature set Linux provides such as networking, data storage or GUIs too.\nThere are significant upsides to keeping the real-time core separate from the GPOS infrastructure:\n  because the autonomous core does not depend on the main kernel logic for running time-critical operations, real-time activities are not serialized with GPOS operations internally. This removes potential delays induced by the latter, by construction. As a result, there is no need for keeping all GPOS operations fine-grained and highly preemptible at any time, which may otherwise induce noticeable overhead as task priority inheritance and IRQ threading have to apply system-wide, affecting all tasks including those which have zero real-time requirement. To sum up, the lesser the effort for the kernel to maintain real-time guarantees internally, the more CPU bandwidth is available to applications.\n  when debugging a real-time issue, the functional isolation of the real-time infrastructure from the rest of the kernel code restricts bug hunting to the scope of the small autonomous core, excluding most interactions with the very large GPOS kernel base.\n  with a dedicated infrastructure providing a specific, well-defined set of real-time services, applications can unambiguously figure out which API calls are available for supporting time-critical work, excluding all the rest as being potentially non-deterministic with respect to response time.\nSaid differently, would you assume that each and every routine from the glibc becomes real-time capable solely by virtue of running on a native preemption system? Of course you would not, therefore you would carefully select the set of services your real-time application may call from its time-critical work loop in any case. For this reason, providing a compact, dedicated API which exports a set of services specifically aimed at real-time usage is clearly an asset, not a limitation.\n  This documentation presents Dovetail, a kernel interface which introduces a high-priority execution stage into the main kernel logic. At any time, out-of-band activities running on this stage can preempt the common work. A task-specific software core - such as a real-time core - can connect to this interface for gaining bounded response time to external interrupts and ultra-low latency scheduling capabilities. This translates into the Dovetail implementation as follows:\n  the interrupt pipeline which creates a high-priority execution stage for an autonomous software core to run on.\n  support for the so-called alternate scheduling between the main kernel and the autonomous software core for sharing kthreads and user tasks.\n  Although both layers are likely to be needed for implementing some autonomous core, only the interrupt pipeline has to be enabled in the early stage of porting Dovetail. Support for alternate scheduling builds upon the latter, and may - and should - be postponed until the pipeline is fully functional on the target architecture or platform. The code base is specifically maintained in a way which allows such incremental process.\nDovetail only introduces the basic mechanisms for hosting an autonomous core into the kernel, enabling the common programming model for its applications in user-space. It does not implement the software core per se, which should be provided by a separate kernel component instead, such as the EVL core.\n Why do we need this? Linux-based dual kernel systems require some interface layer to couple the secondary core (typically a real-time capable one like the EVL core) to the logic of the kernel it is embedded in, so as to benefit from the rich Linux feature set while running dedicated applications with stringent real-time requirements. The archetypical implementation of such kind of interface is the I-pipe, which served both RTAI and Xenomai 3 Cobalt over the years. For several reasons explained in this document, maintaining the I-pipe proved to be difficult as changes to the mainline kernel regularly caused non-trivial code conflicts, sometimes nasty regressions to the I-pipe maintainers downstream. Although the concept of interrupt pipelining proved to be correct in delivering short response time with reasonably limited changes to the original kernel, the way this mechanism is integrated into the mainline code shows its age.\nDovetail is the successor to the I-pipe, with the following goals:\n  introduce a high-priority execution stage for time-critical operations into the kernel logic, enabling all device interrupts to behave like NMIs from the standpoint of the kernel.\n  provide a straightforward interface to autonomous cores for running Linux tasks on this high-priority execution stage when they have to, enabling ultra-fine grained preemption of all other kernel activities in such an event.\n  enable the common Linux programming model for applications which should be controlled by the autonomous core for delivering ultra-low latency services (private user address space, multi-threading, SMP capabilities, system calls etc).\n  make it possible to maintain Dovetail (and ultimately the EVL core which is based on it) with common kernel development knowledge, at a fraction of the engineering and maintenance cost native preemption requires. Typically, Dovetail always favors extending existing kernel subsystems with the ability to deal with the new execution stage, instead of taking sideways steps. For instance, the interrupt pipeline logic is directly integrated into the generic IRQ layer.\n  Developing Dovetail Working on Dovetail is customary Linux kernel development, following the common set of rules and guidelines which prevails with the mainline kernel.\nThe Dovetail interface is maintained in the following GIT repository which tracks the mainline kernel:\n git@git.xenomai.org:Xenomai/linux-dovetail.git https://git.xenomai.org/linux-dovetail.git  Audience This document is intended to people having common kernel development knowledge, who may be interested in building an autonomous software core on Dovetail, porting it to their architecture or platform of choice. Knowing about the basics of interrupt flow, IRQ chip and clock event device drivers in the kernel tree would be a requirement for porting this code.\nHowever, this document is not suited for getting one\u0026rsquo;s feet wet with kernel development, as it assumes that the reader is already familiar with kernel fundamentals.\n Last modified: Sun, 06 Jun 2021 18:28:51 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/",
	"title": "Interrupt pipeline",
	"tags": [],
	"description": "",
	"content": "The autonomous core has to act upon device interrupts with no delay, regardless of the other kernel operations which may be ongoing when the interrupt is received by the CPU. Therefore, there is a basic requirement for prioritizing interrupt masking and delivery between the autonomous core and GPOS operations, while maintaining consistent internal serialization for the kernel.\nHowever, to protect from deadlocks and maintain data integrity, Linux hard disables interrupts around any critical section of code which must not be preempted by interrupt handlers on the same CPU, enforcing a strictly serialized execution among those contexts. The unpredictable delay this may cause before external events can be handled is a major roadblock for kernel components requiring predictable and very short response times to external events, in the range of a few microseconds.\nTo address this issue, Dovetail introduces a mechanism called interrupt pipelining which turns all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt handlers from the perspective of the main kernel activities. This is achieved by substituting real interrupt masking in a CPU by a software-based, virtual interrupt masking when the in-band stage is active on such CPU. This way, the autonomous core can receive IRQs as long as it did not mask interrupts in the CPU, regardless of the virtual interrupt state maintained by the in-band side. Dovetail monitors the virtual state to decide when IRQ events should be allowed to flow down to the in-band stage where the main kernel executes. This way, the assumptions the in-band code makes about running interrupt-free or not are still valid.\nTwo-stage IRQ pipeline Interrupt pipelining is a lightweight approach based on the introduction of a separate, high-priority execution stage for running out-of-band interrupt handlers immediately upon IRQ receipt, which cannot be delayed by the in-band, main kernel work. By immediately, we mean unconditionally, regardless of whether the in-band kernel code had disabled interrupts when the event arrived, using the common local_irq_save(), local_irq_disable() helpers or any of their derivatives. IRQs which have no handlers in the high priority stage may be deferred on the receiving CPU until the out-of-band activity has quiesced on that CPU. Eventually, the preempted in-band code can resume normally, which may involve handling the deferred interrupts.\nIn other words, interrupts are flowing down from the out-of-band to the in-band interrupt stages, which form a two-stage pipeline for prioritizing interrupt delivery. The runtime context of the out-of-band interrupt handlers is known as the out-of-band stage of the pipeline, as opposed to the in-band kernel activities sitting on the in-band stage:\nAn autonomous core can base its own activities on the out-of-band stage, interposing on specific IRQ events, for delivering real-time capabilities to a particular set of applications. Meanwhile, the main kernel operations keep going over the in-band stage unaffected, only delayed by short preemption times for running the out-of-band work.\nOptimistic interrupt protection Predictable response time of out-of-band handlers to IRQ receipts requires the in-band kernel work not to be allowed to delay them by masking interrupts in the CPU.\nHowever, critical sections delimited this way by the in-band code must still be enforced for the in-band stage, so that system integrity is not at risk. This means that although out-of-band IRQ handlers may run at any time while the out-of-band stage is accepting interrupts, in-band IRQ handlers should be allowed to run only when the in-band stage is accepting interrupts too. So we need to decouple the interrupt masking and delivery logic which applies to the out-of-band stage from the one in effect on the in-band stage, by implementing a dual interrupt control.\nIn the pipelined interrupt model, the CPU can receive interrupts most of the time, but the delivery logic of those events may be deferred by a software mechanism until the kernel actually accepts them. This approach is said to be optimistic because it is assumed that the overhead of maintaining such mechanism should be small as the in-band code seldom receives an interrupt while masking them.\nThis approach was fully described by Stodolsky, Chen \u0026amp; Bershad in [Fast Interrupt Priority Management in Operating System Kernels] (https://www.usenix.org/legacy/publications/library/proceedings/micro93/full_papers/stodolsky.txt).\nVirtual interrupt disabling To this end, a software logic managing a virtual interrupt disable flag is introduced by the interrupt pipeline between the hardware and the generic IRQ management layer. This logic can mask IRQs from the perspective of the in-band kernel work when local_irq_save(), local_irq_disable() or any lock-controlled masking operations like spin_lock_irqsave() is called, while still accepting IRQs from the CPU for immediate delivery to out-of-band handlers.\nWhen a real IRQ arrives while interrupts are virtually masked, the event is logged for the receiving CPU, kept there until the virtual interrupt disable flag is cleared at which point it is dispatched as if it just happened. The principle of deferring interrupt delivery based on a software flag coupled to an event log has been originally described as Optimistic interrupt protection in this paper. It was originally intended as a low-overhead technique for manipulating the processor interrupt state, reducing the cost of interrupt masking for the common case of absence of interrupts.\nIn Dovetail\u0026rsquo;s two-stage pipeline, the out-of-band stage protects from interrupts by disabling them in the CPU\u0026rsquo;s status register as usual, while the in-band stage disables interrupts only virtually. A stage for which interrupts are disabled is said to be stalled. Conversely, unstalling a stage means re-enabling interrupts for it.\nObviously, stalling the out-of-band stage implicitly means disabling further IRQ receipts for the in-band stage down the pipeline too.\n Interrupt deferral for the in-band stage When the in-band stage is stalled because the virtual interrupt disable flag is set, any IRQ event which was not immediately delivered to the out-of-band stage is recorded into a per-CPU log, postponing delivery to the in-band kernel handler.\nSuch delivery is deferred until the in-band kernel code clears the virtual interrupt disable flag by calling local_irq_enable() or any of its variants, which unstalls the in-band stage. When this happens, the interrupt state is resynchronized by playing the log, firing the in-band handlers for which an IRQ event is pending.\n/* Both stages unstalled on entry */ local_irq_save(flags); \u0026lt;IRQx received: no out-of-band handler\u0026gt; (pipeline logs IRQx event) ... local_irq_restore(flags); (pipeline plays IRQx event) handle_IRQx_interrupt(); If the in-band stage is unstalled at the time of the IRQ receipt, the in-band handler is immediately invoked, just like with the non-pipelined IRQ model.\nAll interrupts are (seemingly) NMIs From the standpoint of the in-band kernel code (i.e. the one running over the in-band interrupt stage) , the interrupt pipelining logic virtually turns all device IRQs into NMIs, for running out-of-band handlers.\nFor this reason, out-of-band code may generally not re-enter in-band code, for preventing creepy situations like this one:\n/* in-band context */ spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;IRQx received: out-of-band handler installed\u0026gt; handle_oob_event(); /* attempted re-entry to in-band from out-of-band. */ in_band_routine(); spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;DEADLOCK\u0026gt; ... ... ... ... spin_unlock irqrestore(\u0026amp;lock, flags); Even in absence of an attempt to get a spinlock recursively, the outer in-band code in the example above is entitled to assume that no access race can occur on the current CPU while interrupts are (virtually) masked for the in-band stage. Re-entering in-band code from an out-of-band handler would invalidate this assumption.\nIn rare cases, we may need to fix up the in-band kernel routines in order to allow out-of-band handlers to call them. Typically, generic atomic helpers are such routines, which serialize in-band and out-of-band callers.\nFor all other cases, the IRQ work API is available for scheduling the execution of a routine from the out-of-band stage, which will be invoked later from a safe place on the in-band stage as soon as it gets back in control on the current CPU (i.e. typically when in-band interrupts are allowed again).\n Last modified: Fri, 27 Nov 2020 16:26:53 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/mutex/",
	"title": "Kernel mutex",
	"tags": [],
	"description": "",
	"content": "void evl_init_kmutex(struct evl_kmutex *kmutex)  void evl_destroy_kmutex(struct evl_kmutex *kmutex)  int evl_trylock_kmutex(struct evl_kmutex *kmutex)  int evl_lock_kmutex(struct evl_kmutex *kmutex)  void evl_unlock_kmutex(struct evl_kmutex *kmutex)  DEFINE_EVL_MUTEX(__name)  Last modified: Thu, 28 Nov 2019 18:52:38 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/mutex/",
	"title": "Mutex",
	"tags": [],
	"description": "",
	"content": "Serializing threads with mutexes EVL provides common mutexes for serializing thread access to a shared resource from out-of-band context, with semantics close to the POSIX specification for the basic operations.\nMutex services   int evl_create_mutex(struct evl_mutex *mutex, int clockfd, unsigned int ceiling, int flags, const char *fmt, ...)  This call creates a mutex, returning a file descriptor representing the new object upon success. This is the generic call form; for creating a mutex with common pre-defined settings, see [evl_new_mutex()}(#evl_new_mutex).\nmutexAn in-memory mutex descriptor is constructed by evl_create_mutex(), which contains ancillary information other calls will need. mutex is a pointer to such descriptor of type struct evl_mutex.\n\nclockfdSome mutex-related calls are timed like evl_timedlock_mutex which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\nceilingWhen non-zero, EVL interprets this value as the priority ceiling of the new mutex. In this case, the behavior is similar to POSIX\u0026rsquo;s PRIO_PROTECT protocol. The priority must be in the [1-99] range, so that priority ceiling can apply to any scheduling policy EVL supports.\nWhen zero, EVL applies a priority inheritance protocol such as described for POSIX\u0026rsquo;s PRIO_INHERIT protocol.\n\nFor priority protection/ceiling, EVL actually changes the priority of the lock owner to the ceiling value only when/if the rescheduling procedure is invoked while such thread is current on the CPU. Since mutex-protected critical sections should be short and the lock owner is unlikely to schedule out while holding such lock, this reduces the overhead of dealing with priority protection, whilst providing the same guarantees compared to changing the priority immediately upon acquiring that lock.\n flagsA set of creation flags ORed in a mask which defines the new mutex type and visibility:\n  EVL_MUTEX_NORMAL for a normal, non-recursive mutex.\n  EVL_MUTEX_RECURSIVE for a recursive mutex a thread may lock multiple times in a nested way.\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new mutex in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the mutex name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_mutex() returns the file descriptor of the newly created mutex on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither flags is wrong, clockfd does not refer to a valid EVL clock, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL mutex.\n  #include \u0026lt;evl/mutex.h\u0026gt; static struct evl_mutex mutex; void create_new_mutex(void) { int fd; /* Create a (private) non-recursive mutex with priority inheritance enabled. */ fd = evl_create_mutex(mutex, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL, \u0026quot;name_of_mutex\u0026quot;); /* skipping checks */ return fd; }   int evl_new_mutex(struct evl_mutex *mutex, const char *fmt, ...)  This call is a shorthand for creating a normal (non-recursive) mutex enabling the priority inheritance protocol, timed on the built-in EVL monotonic clock. It is identical to calling:\n\tevl_create_mutex(mutex, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL|EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_MUTEX_INITIALIZER((const char *) name, (int) clockfd, (int) ceiling, (int) flags)  The static initializer you can use with events. All arguments to this macro refer to their counterpart in the call to evl_create_mutex().\n/* Create a (public) recursive mutex with priority ceiling enabled (prio=90). */ struct evl_mutex mutex = EVL_MUTEX_INITIALIZER(\u0026quot;name_of_mutex\u0026quot;, EVL_CLOCK_MONOTONIC, 90, EVL_MUTEX_RECURSIVE|EVL_CLONE_PUBLIC); /* which is strictly equivalent to: */ struct evl_mutex mutex = EVL_MUTEX_INITIALIZER(\u0026quot;/name_of_mutex\u0026quot;, EVL_CLOCK_MONOTONIC, 90, EVL_MUTEX_RECURSIVE);   int evl_open_mutex(struct evl_mutex *mutex, const char *fmt, ...)  You can open an existing mutex, possibly from a different process, by calling evl_open_mutex().\nmutexAn in-memory mutex descriptor is constructed by evl_open_mutex(), which contains ancillary information other calls will need. mutex is a pointer to such descriptor of type struct evl_mutex. The information is retrieved from the existing mutex which was opened.\n\nfmtA printf-like format string to generate the name of the mutex to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_mutex() returns the file descriptor referring to the opened mutex on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to a mutex.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_lock_mutex(struct evl_mutex *mutex)  The current thread may lock a mutex by calling this service, which guarantees exclusive access to the resource it protects on success, until it eventually releases mutex.\nIf mutex is unlocked, or it is of recursive type and the current thread already owns it on entry to the call, the lock nesting count is incremented by one then evl_lock_mutex() returns immediately with a success status.\nOtherwise, the caller blocks until the current owner releases it by a call to evl_unlock_mutex(). If multiple threads wait for acquiring the lock, the one with the highest scheduling priority which has been waiting for the longest time is served first.\nAs long as the caller holds an EVL mutex, switching to in-band mode is wrong since this would introduce a priority inversion. For this reason, EVL threads which undergo the SCHED_WEAK policy are kept running out-of-band by the core until the last mutex they have acquired is dropped.\n mutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() is issued for mutex before a locking operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_lock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN mutex is recursive and the lock nesting count would exceed 2^31.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf mutex was statically initialized with EVL_MUTEX_INITIALIZER, then any error returned by evl_create_mutex() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedlock_mutex(struct evl_mutex *mutex, const struct timespec *timeout)  This call is a variant of evl_lock_mutex() which allows specifying a timeout on the locking operation, so that the caller is unblocked after a specified delay without being able to acquire the lock.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() is issued for mutex before a locking operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\ntimeoutA time limit to wait for the caller to be unblocked before the call returns on error. The clock mentioned in the call to evl_create_mutex() will be used for tracking the elapsed time.\n\nThe possible return values include any status from evl_lock_mutex(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_trylock_mutex(struct evl_mutex *mutex)  This call attempts to lock the mutex like evl_lock_mutex() would do, but returns immediately both on success or failure to do so, without waiting for the lock to be available in the latter case.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() for mutex is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_trylock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN mutex is already locked by another thread.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf mutex was statically initialized with EVL_MUTEX_INITIALIZER, then any error returned by evl_create_mutex() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_unlock_mutex(struct evl_mutex *mutex)  This routine releases a mutex previously acquired by evl_lock_mutex(), evl_trylock_mutex() or evl_timedlock_mutex() by the current EVL thread.\nOnly the thread which acquired an EVL mutex may release it.\n mutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nIf mutex is recursive, the lock nesting count is decremented by one until it reaches zero, at which point the lock is actually released.\nevl_unlock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EPERM\tThe caller does not own the mutex.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_set_mutex_ceiling(struct evl_mutex *mutex, unsigned int ceiling)  Change the priority ceiling value for a mutex. If the mutex is currently owned by a thread, the change may not apply immediately, depending on whether the priority ceiling was committed for that thread already (see this note).\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nceilingThe new priority ceiling in the [1-99] range.\n\nevl_set_mutex_ceiling returns zero on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tmutex does not enable priority protection.\n-EINVAL\tceiling is out of range.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_get_mutex_ceiling(struct evl_mutex *mutex)  Retrieve the current priority ceiling value for mutex. This call also succeeds for a mutex which does not enable priority protection (but priority inheritance instead), returning zero in such a case.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nevl_get_mutex_ceiling returns the current priority ceiling value on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_close_mutex(struct evl_mutex *mutex)  You can use evl_close_mutex to dispose of an EVL mutex, releasing the associated file descriptor, at which point mutex will not be valid for any subsequent operation from the current process. However, this mutex is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_mutex() have been released, whether from the current process or any other process.\nmutexThe in-memory descriptor of the mutex to dismantle.\n\nevl_close_mutex returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized mutex descriptor which has never been locked always returns zero.\n Last modified: Mon, 17 May 2021 16:48:45 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/under-the-hood/abi/",
	"title": "The EVL ABI",
	"tags": [],
	"description": "",
	"content": "Since we have service calls between the applications and the EVL core, we depend on a certain format of request codes, arguments and return values when exchanging information between both ends, like the contents of the argument structure passed to the various ioctl(2) and oob_ioctl() requests. This convention forms the EVL ABI. As hard as we might try to keep the ABI stable over time, there are times when this cannot be achieved, at the very least because we may want to make new features added to the EVL core visible to applications. Generally speaking, EVL being a new kid in the old dual kernel town, it is too early - for the time being - to write the current convention in stone. This fact of life calls for a way to unambiguously:\n  determine which (range of subsequent) revision(s) of the ABI is implemented by the running EVL core, from the application side.\n  state which revision of the ABI an application requires.\n  In some instance, the EVL core may also be able to provide a backward-compatible interface to applications which spans multiple subsequent ABI revisions. Because ABI stability is a strong goal, backward compatibility may become the norm as the EVL core feature set stabilizes.\nTo this end, the following apply (since ABI revision #19):\n  an ABI is versioned. Each version is represented by a non-null integer starting at 1, incremented by one after each revision.\n  the EVL core defines two symbols, namely EVL_ABI_BASE and EVL_ABI_LEVEL, which represent the range of ABI revisions the core supports (inclusively), from oldest (EVL_ABI_BASE) to current (EVL_ABI_LEVEL) respectively. Differing values means that the EVL core honors multiple subsequent revisions by way of backward compatible support of some sort.\n  By convention, changes to EVL_ABI_BASE and/or EVL_ABI_LEVEL belong to the same GIT commit which triggered the ABI revision. This way, it is easy to map a revision to a particular change unambiguously. For the same reason, non-related changes to the ABI appear in separate commits, each one forming a particular ABI revision.\n   libevl states the ABI revision it follows by defining the EVL_ABI_PREREQ symbol. When libevl initializes, this prerequisite is checked against the range of ABI revisions the running EVL core advertises. If a mismatch is detected, the initialization fails and -ENOEXEC is returned to the caller; otherwise the execution proceeds normally. An easy way to figure out which ABI a libevl installation requires is to ask the evl command about it as follows:\n~ # evl -V evl.0 -- #1c6115c (2020-03-06 16:24:00 +0100) [requires ABI 19]   Locating ABI boundaries and prereqs {git-evlabi} When developing (with) the EVL core, it may sometimes be useful to find which code changes in the GIT commit history correspond to ABI revisions and conversely. It may also be useful to determine the history of ABI prerequisites in libevl, so that you can map any particular point the commit history with the ABI revision it is compatible with. For this, you can use a simple script called git-evlabi, which is available in the linux-evl repository. You just need to make sure this script is executable and reachable from the $PATH variable.\ngit-evlabi looks for EVL ABI definitions or prereqs in GIT trees cloned from either linux-evl or libevl respectively. It does so depending on the value and type of the GIT object mentioned (or not) in the command:\n$ git evlabi -h usage: git-evlabi [-r \u0026lt;rev\u0026gt;][--git-dir \u0026lt;dir\u0026gt;][-s][-h] [\u0026lt;object\u0026gt;]   if \u0026lt;object\u0026gt; is a regular commit (SHA-1) hash, it extracts the ABI information for that particular commit.\n  if \u0026lt;object\u0026gt; matches a GIT refname (typically a branch name), it scans the commit history backward from that point, displaying the ABI information for each revision encountered during the traversal.\n  if \u0026lt;object\u0026gt; is omitted, it defaults to HEAD, which falls back to the refname case above.\n  The output is of the form:\n\u0026lt;start\u0026gt; \u0026lt;range\u0026gt; \u0026lt;revision\u0026gt; [\u0026lt;shortlog\u0026gt;], where:\n  \u0026lt;start\u0026gt; is the predecessor of the earliest commit implementing/requesting the ABI revision\n  \u0026lt;range\u0026gt; is the span of the ABI revision in the history (usable with git log for instance).\n  \u0026lt;revision\u0026gt; is the ABI revision number\n  \u0026lt;shortlog\u0026gt; is the subject line describing \u0026lt;start\u0026gt;\n   e.g. asking for all EVL ABI revisions in the kernel tree\n {rpm@cobalt} cd ~/git/linux-evl \u0026amp;\u0026amp; git evlabi evl/master 4bc9b71134f0 96c899ef3a6c..bc7ca4ad0b5c 19 02ae0b8783cb 7f082bc5bef1..96c899ef3a6c 18 c2a51a7e68a7 56e217799766..7f082bc5bef1 17 5266e26ca729 857c199beead..56e217799766 16 54559813955b 91f7d9d3bc73..857c199beead 15 fb0de980dac9 7795aa5979a1..91f7d9d3bc73 14 437fee2b84ee 1b627b1977a1..7795aa5979a1 13 60c937d72a72 03f35888596c..1b627b1977a1 12 db431047b9bd 68b176e50081..03f35888596c 11 8f5b07b678ea 92f04516c8dd..68b176e50081 10 b0eb03db5c77 8f9bb48957ba..92f04516c8dd 9 b5f675b3e2c0 5e640e734385..8f9bb48957ba 8 7892bfe4886e 9ddba326d41b..5e640e734385 7 9ddba326d41b 61e9a6b78aa5..9ddba326d41b 6 badb3bf8cee8 ba708547b903..61e9a6b78aa5 5 ba708547b903 7d9aa309a81d..ba708547b903 4 06b78dd09bfa 3dd263191153..7d9aa309a81d 3 6170f4f1d9fd fd56dfbfdbbf..3dd263191153 2 800484643595 f8ef2539553e..fd56dfbfdbbf 1 f8ef2539553e fec5d8902f49..f8ef2539553e 0 With the information above, it is easy to figure out the span of a given ABI revision in the kernel code (here ABI 17 starting at commit #c2a51a7e68a7), as follows:\n e.g. looking for span of ABI 17 in the kernel tree\n {rpm@cobalt} git log --oneline 56e217799766..7f082bc5bef1 7f082bc5bef1 evl/work: export interface to modules 54b365d416c7 evl/stax: export interface to modules c268cadb0396 evl/stax: force the caller to in-band mode on WOSX 77d1de6add6e evl/clock: y2038: remove use of struct timex 05fc1d0e5e96 evl/thread: fix rescheduling leak c2a51a7e68a7 evl/thread: replace SIGEVL_ACTION_HOME with a RETUSER event The same logic applies to the libevl tree, this time for determining the ABI prerequisites of commits in the API.\n e.g. walking the history of ABI prerequisites\n {rpm@cobalt} git evlabi next dde33b5 1c6115c..dde33b5 19 Which means that looking at the current state of branch next in the libevl tree, ABI revision 19 in the EVL core starts being a prerequisite for the code starting at commit #dde33b5.\nConversely, you could ask for matching a particular ABI revision to a commit, using the -r switch.\n e.g. matching a particular ABI revision (asking for the subject line too)\n {rpm@cobalt} git evlabi -r 16 -s 5266e26ca729 857c199beead..56e217799766 16 evl: introduce synchronous breakpoint support  git-evlabi cannot detect ABI revisions earlier than 19 in the libevl tree, since the marker it looks for was introduced by that particular revision.\n List of ABI revisions This document summarizes the various ABI revisions and their respective purpose, matching them to the corresponding libevl release implementing the change.\n Last modified: Sat, 05 Jun 2021 16:47:53 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/",
	"title": "Writing drivers",
	"tags": [],
	"description": "",
	"content": "What is an EVL driver? An EVL driver is a regular Linux driver, either of a character device driver or a socket protocol driver, which also implements a set of out-of-band I/O operations advertised by its file operation descriptor (struct file_operations). These out-of-band I/O requests are only available to EVL threads, since only such threads may run on the out-of-band execution stage. Applications running in user-space can start these I/O operations by issuing specific system calls to the EVL core.\n Excerpt from include/linux/fs.h, as modified by Dovetail\n struct file_operations { ... ssize_t (*oob_read) (struct file *, char __user *, size_t); ssize_t (*oob_write) (struct file *, const char __user *, size_t); long (*oob_ioctl) (struct file *, unsigned int, unsigned long); long (*compat_oob_ioctl) (struct file *, unsigned int, unsigned long); __poll_t (*oob_poll) (struct file *, struct oob_poll_wait *); ... } __randomize_layout; In the particular case of the socket interface, Dovetail also connects the in-band networking core with the companion core through a simple internal interface so that the latter can provide out-of-band extensions.\nDovetail is in charge of routing the system calls received from applications to the proper recipient, either the EVL core or the in-band kernel. As mentioned earlier when describing the everything-is-a-file mantra, only I/O transfer and control requests have to run from the out-of-band context (i.e. EVL\u0026rsquo;s real-time mode), creating and dismantling the underlying file from the regular in-band context is just fine. Therefore, the execution flow upon I/O system calls looks like this:\nWhich translates as follows:\n  When an application issues the open(2) or close(2) system calls with a file descriptor referring to a file managed by an EVL driver, the request normally goes through the virtual filesystem switch (VFS) as usual, ending up into the .open() and .release() handlers (when the last reference is dropped) defined by the driver in its struct file_operations descriptor. The same goes for mmap(2), ioctl(2), read(2), write(2), and all other common file operations for character device drivers.\n  When an applications issues the oob_read(), oob_write() or oob_ioctl() system calls via the EVL library, Dovetail routes the call to the EVL core (instead of the VFS), which in turn fires the corresponding handlers defined by the driver\u0026rsquo;s struct file_operations descriptor: i.e. .oob_read(), .oob_write(), and .oob_ioctl(). Those handlers should use the EVL kernel API, or the main kernel services which are known to be usable from the out-of-band-context, exclusively. Failing to abide by this golden rule may lead to funky outcomes ranging from plain kernel crashes (lucky) to rampant memory corruption (unlucky).\n  Now, you may wonder: \u0026ldquo;what if an out-of-band operation is ongoing in the driver on a particular file, while I\u0026rsquo;m closing the last VFS-maintained reference to that file?\u0026quot; Well, with a properly written EVL driver, the close(2) call should block until the out-of-band operation finishes, at which point the .release() handler may proceed with dismantling the file. A simple rule for writing EVL drivers ensures this.\n  Last modified: Sat, 17 Apr 2021 11:55:02 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/event/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Synchronizing on arbitrary events An EVL event object has the same semantics than a POSIX condition variable. It can be used for synchronizing EVL threads on the state of some shared information in a raceless way. Proper serialization is enforced by pairing an event with a mutex. The typical usage pattern is as follows:\n#include \u0026lt;evl/mutex.h\u0026gt; #include \u0026lt;evl/event.h\u0026gt; struct evl_mutex lock = EVL_MUTEX_INITIALIZER(\u0026quot;this_lock\u0026quot;, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL|EVL_CLONE_PRIVATE); struct evl_event event = EVL_EVENT_INITIALIZER(\u0026quot;this_event\u0026quot;, EVL_CLOCK_MONOTONIC, EVL_CLONE_PRIVATE); bool condition = false; void waiter(void) { evl_lock_mutex(\u0026amp;lock); /* * Check whether the wait condition is satisfied, go * sleeping if not. EVL drops the lock and puts the thread * to sleep atomically, then re-acquires it automatically * once the event is signaled before returning to the * caller. */ while (!condition) evl_wait_event(\u0026amp;event, \u0026amp;lock); evl_unlock_mutex(\u0026amp;lock); } void poster(void) { /* * Post the condition and signal the event to the waiter, * all atomically by holding the lock. CAUTION: actual wake * up happens when dropping the lock protecting the event. */ evl_lock_mutex(\u0026amp;lock); condition = true; evl_signal_event(\u0026amp;event); evl_unlock_mutex(\u0026amp;lock); } Of course, the way you would express the condition to be satisfied is fully yours. It could be a simple boolean condition like illustrated, or the combination of multiple tests and/or computations, whatever works.\nUnlike glibc with condition variables, EVL optimizes the wake up sequence on event receipt by readying the waiter only upon release of the lock which protects accesses to the condition by the thread posting the awaited event. This spares us a useless context switch to the waiter in case it has a higher priority than the poster.\n Event services   int evl_create_event(struct evl_event *evt, int clockfd, int flags, const char *fmt, ...)  To create an event, you need to call evl_create_event() which returns a file descriptor representing the new object upon success. This is the generic call form; for creating an event with common pre-defined settings, see evl_new_event().\nevtAn in-memory event descriptor is constructed by evl_create_event(), which contains ancillary information other calls will need. evt is a pointer to such descriptor of type struct evl_event.\n\nclockfdSome event-related calls are timed like evl_timedwait_event() which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\nflagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new event in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the event name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_event() returns the file descriptor of the newly created event on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither flags is wrong, clockfd does not refer to a valid EVL clock, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO\tThe EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL event.\n  An event object cannot be used alone, it has to be paired with an EVL mutex for proper operation.\n #include \u0026lt;evl/event.h\u0026gt; static struct evl_event event; void create_new_event(void) { int evfd; evfd = evl_create_event(\u0026amp;event, EVL_CLOCK_MONOTONIC, \u0026quot;name_of_event\u0026quot;); /* skipping checks */ return evfd; } An event is based on the EVL monitor element. For this reason, the device representing the new element appears in the /dev/evl/monitor hierarchy.\n  int evl_new_event(struct evl_event *evt, const char *fmt, ...)  This call is a shorthand for creating a private event timed on the built-in EVL monotonic clock. It is identical to calling:\n\tevl_create_event(evt, EVL_CLOCK_MONOTONIC, EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_EVENT_INITIALIZER((const char *) name, (int) clockfd, (int) flags)  The static initializer you can use with events. All arguments to this macro refer to their counterpart in the call to evl_create_event().\n  int evl_open_event(struct evl_event *evt, const char *fmt, ...)  Knowing its name in the EVL device hierarchy, you may want to open an existing event, possibly from a different process, obtaining a new file descriptor on such object. You can do so by calling evl_open_event().\nevtAn in-memory event descriptor is constructed by evl_open_event(), which contains ancillary information other calls will need. evt is a pointer to such descriptor of type struct evl_event. The information is retrieved from the existing event which was opened.\n\nfmtA printf-like format string to generate the name of the event to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_event() returns the file descriptor referring to the opened event on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to an event.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_wait_event(struct evl_event *evt, struct evl_mutex *mutex)  You should use this call to wait for the condition associated with an EVL event to become true. Access to such condition must be protected by an EVL mutex. The caller is put to sleep by the core until this happens, dropping the lock protecting the condition in the same move, all atomically so that no wake up event can be missed. Waiters are queued by order of scheduling priority.\nKeep in mind that events may be subject to spurious wake ups, meaning that the condition might turn to false again before the waiter can resume execution for consuming the event. Therefore, abiding by the documented usage pattern for waiting for an event is very important.\nevtThe in-memory event descriptor constructed by either evl_create_event() or evl_open_event(), or statically built with EVL_EVENT_INITIALIZER. In the latter case, an implicit call to evl_create_event() is issued for evt before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nmutexThe in-memory mutex descriptor returned by either evl_create_mutex() or evl_open_mutex(). This lock should be used to serialize access to the shared information representing the condition.\n\nevl_wait_event() returns zero if the event was signaled by a call to evl_signal_event() or evl_broadcast_event(). Otherwise, a negated error code may be returned if:\n-EINVAL\tevt does not represent a valid in-memory event descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n-EBADFD\tA thread is currently pending on evt protected by a different mutex. When multiple threads issue concurrent wait requests on the same event, they must use the same mutex to serialize.\nIf evt was statically initialized with EVL_EVENT_INITIALIZER, then any error returned by evl_create_event() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedwait_event(struct evl_event *evt, struct evl_mutex *mutex, const struct timespec *timeout)  This call is a variant of evl_wait_event() which allows specifying a timeout on the wait operation, so that the caller is unblocked after a specified delay sleeping without the event being signaled.\nevtThe in-memory event descriptor constructed by either evl_create_event() or evl_open_event(), or statically built with EVL_EVENT_INITIALIZER. In the latter case, an implicit call to evl_create_event() for evt is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nmutexThe in-memory mutex descriptor returned by either evl_create_mutex() or evl_open_mutex().\n\ntimeoutA time limit to wait for the event to be signaled before the call returns on error. The clock mentioned in the call to evl_create_event() will be used for tracking the elapsed time.\n\nThe possible return values include any status from evl_wait_event(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_signal_event(struct evl_event *evt)  This call notifies the thread heading the wait queue of an event at the time of the call (i.e. having the highest priority among waiters), indicating that the condition it has waited for may have become true.\nYou must hold the mutex lock which protects accesses to the condition around the call to evl_signal_event(), because the notified thread will be actually readied when such lock is dropped, not at the time of the notification.\nevtThe in-memory event descriptor constructed by either evl_create_event() or evl_open_event(), or statically built with EVL_EVENT_INITIALIZER. In the latter case, an implicit call to evl_create_event() for evt is issued before the signal is sent, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_signal_event() returns zero upon success, with the signal set pending for the event. Otherwise, a negated error code is returned:\n-EINVAL\tevt does not represent a valid in-memory event descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf evt was statically initialized with EVL_EVENT_INITIALIZER but not passed to any event-related call yet, then any error status returned by evl_create_event() may be passed back to the caller in case the implicit initialization call fails.\nIf no thread was waiting on evt at the time of the call, no action is performed and evl_signal_event() returns zero.\n  int evl_signal_thread(struct evl_event *evt, int thrfd)  This service is a variant of evl_signal_event(), which directs the wake up signal to a specific waiter instead of waking up the thread heading the wait queue.\nevtThe in-memory event descriptor constructed by either evl_create_event() or evl_open_event(), or statically built with EVL_EVENT_INITIALIZER. In the latter case, an implicit call to evl_create_event() for evt is issued before the signal is sent, which may trigger a transition to the in-band execution mode for the caller.\n\nthrfdThe file descriptor of the target thread, which must be attached to the EVL core by a call to evl_attach_self().\n\nThe possible return values include any status from evl_signal_event(), plus:\n-EINVAL\tthrfd is not a valid file descriptor referring to an EVL thread.\nIf the target thread was not waiting on evt at the time of the call, no action is performed and evl_signal_thread() returns zero.\n  int evl_broadcast(struct evl_event *evt)  This service is yet another variant of evl_signal_event(), which wakes up all waiters currently sleeping on the event wait queue at the time of the call.\nevtThe in-memory event descriptor constructed by either evl_create_event() or evl_open_event(), or statically built with EVL_EVENT_INITIALIZER. In the latter case, an implicit call to evl_create_event() for evt is issued before the signal is sent, which may trigger a transition to the in-band execution mode for the caller.\n\nThe possible return values include any status from evl_signal_event().\nIf no thread was waiting on evt at the time of the call, no action is performed and evl_broadcast_event() returns zero.\n  int evl_close_event(struct evl_event *evt)  You can use evl_close_event() to dispose of an EVL event, releasing the associated file descriptor, at which point evt will not be valid for any subsequent operation from the current process. However, this event object is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_event() have been released, whether from the current process or any other process.\nevtThe in-memory descriptor of the event to dismantle.\n\nevl_close_event() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tevt does not represent a valid in-memory event descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized event descriptor which has never been used in wait or signal operations always returns zero.\n Last modified: Mon, 27 Apr 2020 19:01:01 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/under-the-hood/",
	"title": "Under the hood",
	"tags": [],
	"description": "",
	"content": "This (work-in-progress) section will help you navigate the design and implementation of the EVL core. It is written from a developer perspective as a practical example of implementing a companion core living in the Linux kernel, emphasizing on details about the way the Dovetail interface is leveraged for this purpose. This information is intended to help anyone interested in or simply curious about the \u0026ldquo;other path to Linux real-time\u0026rdquo;, whether it is useful for developing your own Linux-based dual kernel system, contributing to EVL, or educational purpose.\nHow do applications request services from the EVL core? An EVL application interacts with so-called EVL elements. Each element usually exports a mix of in-band and out-of-band file operations implemented by the EVL core (in a few cases, only either side is implemented). Therefore, an EVL application sees every core element as a file, and uses a regular file descriptor to interact with it, either directly or indirectly by calling routines from the standard glibc or libevl whenever there is a real-time requirement.\nTo sum up, an application (indirectly) issues file I/O requests on element devices to obtain services from the EVL core. The system calls involved in the interface between an application and the EVL core are exclusively open(2), close(2), read(2), write(2), ioctl(2), mmap(2) for the in-band side, and oob_ioctl(), oob_read(), oob_write() for the out-of-band side.\nlibevl hides the nitty-gritty details of forming these I/O requests, presenting a high-level API to be used in applications.\nWhere are the EVL core services implemented? The EVL core can be described as a multi-device driver, each device type representing an EVL element. From the standpoint of the Linux device driver model, the EVL core is composed of a set of character-based device drivers, one per element type. The core is implemented under the kernel/evl hierarchy.\nElement creation The basic operation every element driver implements is cloning, which creates a new instance of the element type upon request from the application. For instance, a thread element is created each time evl_attach_self() is invoked from libevl. To do so, create_evl_element() sends a request to the special clone device exported by the core at /dev/evl/thread/clone. Upon return, the new thread element is live, and can be accessed by opening the newly created device at /dev/evl/thread/\u0026lt;name\u0026gt;, where \u0026lt;name\u0026gt; is the thread name passed to evl_attach_self(). Eventually, the file descriptor obtained is usable for issuing requests for core services to that particular element instance via file I/O requests.\nThe same logic applies to all other types of named elements, such as proxies, cross-buffers or monitors which underlie mutexes, events, semaphores and flags.\nElement factory In order to avoid code duplication, the EVL core implements a so-called element factory. The factory refers to EVL class descriptors of type struct evl_factory, which describes how a particular element type should be handled by the generic factory code.\nThe factory performs the following tasks:\n  it populates the initial device hierarchy under /dev/evl so that applications can issue requests to the EVL core. The main aspect of this task is to register a Linux device class for each element type, creating the related clone device.\n  it implements the generic portion of the ioctl(EVL_IOC_CLONE) request, eventually calling the proper type-specific handler, such as thread_factory_build() for threads, monitor_factory_build() for monitors and so on.\n  it maintains a reference count on every element instantiated in the system, so as to automatically remove elements when they have no more referrers. Typically, closing the last file descriptor referring to the file underlying an element would cause such removal (unless some kernel code is still withholding references to the same element).\n  Unlike other elements, a thread may exist in absence of any file reference. The disposal still happens automatically when the thread exits or voluntarily detaches by calling evl_detach_self().\n "
},
{
	"uri": "https://the-going.github.io/website/core/user-api/flags/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Synchronizing on a group of flags An event flag group is an efficient and lightweight mechanism for synchronizing multiple threads, based on a 32bit value, in which each individual bit represents the state of a particular event defined by the application. Therefore, each group defines 32 individual events, from bit #0 to bit #31. Threads can wait for bits to be posted by other threads. Although EVL\u0026rsquo;s event flag group API is somewhat reminiscent of the eventfd interface, it is much simpler:\n  on the send side, posting the bits event mask to an event group is equivalent to performing atomically a bitwise OR operation such as group |= bits.\n  on the receive side, waiting for events means blocking until group becomes non-zero, at which point this value is returned to the thread heading the wait queue and the group is reset to zero in the same move. In other words, all bits set are immediately consumed by the first waiter and cleared in the group atomically. Threads wait on a group by priority order, which is determined by the scheduling policy they undergo.\n  Unlike with the eventfd, there is no semaphore semantics associated to an event flag group, you may want to consider the EVL semaphore feature instead if this is what you are looking for.\n Event flag group services   int evl_create_flags(struct evl_flags *flg, int clockfd, int initval, int flags, const char *fmt, ...)  This call creates a group of event flags, returning a file descriptor representing the new object upon success. This is the generic call form; for creating an event group with common pre-defined settings, see [evl_new_flags()}(#evl_new_flags).\nflgAn in-memory flag group descriptor is constructed by evl_create_flags(), which contains ancillary information other calls will need. flg is a pointer to such descriptor of type struct evl_flags.\n\nclockfdSome flag group-related calls are timed like evl_timewait_flags() which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\ninitvalThe initial value of the flag group. You can use this parameter to pre-set some bits in the received event mask at creation time.\n\nflagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new flag group in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the flag group name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_flags() returns the file descriptor of the newly created flag group on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither flags is wrong, clockfd does not refer to a valid EVL clock, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL flag group.\n  #include \u0026lt;evl/flags.h\u0026gt; static struct evl_flags flags; void create_new_flags(void) { int fd; fd = evl_create_flags(\u0026amp;flags, EVL_CLOCK_MONOTONIC, 0, \u0026quot;name_of_group\u0026quot;); /* skipping checks */ return fd; }   int evl_new_flags(struct evl_flags *flg, const char *fmt, ...)  This call is a shorthand for creating a zero-initialized group of event flags, timed on the built-in EVL monotonic clock. It is identical to calling:\n\tevl_create_flags(flg, EVL_CLOCK_MONOTONIC, 0, EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_FLAGS_INITIALIZER((const char *) name, (int) clockfd, (int) initval, (int) flags)  The static initializer you can use with flag groups. All arguments to this macro refer to their counterpart in the call to evl_create_flags().\n  int evl_open_flags(struct evl_flags *flg, const char *fmt, ...)  You can open an existing flag group, possibly from a different process, by calling evl_open_flags().\nflgAn in-memory flag group descriptor is constructed by evl_open_flags(), which contains ancillary information other calls will need. flg is a pointer to such descriptor of type struct evl_flags. The information is retrieved from the existing flag group which was opened.\n\nfmtA printf-like format string to generate the name of the flag group to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_flags() returns the file descriptor referring to the opened group on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to a group.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_wait_flags(struct evl_flags *flg, int *r_bits)  This service waits for events to be posted to the flag group. The caller is put to sleep by the core until this happens. Waiters are queued by order of scheduling priority.\nWhen the group receives some event(s), its value is passed back to the waiter leading the wait queue, then reset to zero before the latter returns.\nIf the flag group value is non-zero on entry to this service, the caller returns immediately without blocking with the current set of posted flags.\nflgThe in-memory flag group descriptor constructed by either evl_create_flags() or evl_open_flags(), or statically built with EVL_FLAGS_INITIALIZER. In the latter case, an implicit call to evl_create_flags() is issued for flg before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nr_bitsThe address of an integer which contains the set of flags received on successful return from the call.\n\nevl_wait_flags() returns zero on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tflg does not represent a valid in-memory flag group descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf flg was statically initialized with EVL_FLAGS_INITIALIZER, then any error returned by evl_create_flags() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedwait_flags(struct evl_flags *flg, const struct timespec *timeout, int *r_bits)  This call is a variant of evl_wait_flags() which allows specifying a timeout on the wait operation, so that the caller is unblocked after a specified delay sleeping without any flag being posted.\nflgThe in-memory flag group descriptor constructed by either evl_create_flags() or evl_open_flags(), or statically built with EVL_FLAGS_INITIALIZER. In the latter case, an implicit call to evl_create_flags() for flg is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\ntimeoutA time limit to wait for the group to be posted before the call returns on error. The clock mentioned in the call to evl_create_flags() will be used for tracking the elapsed time.\n\nr_bitsThe address of an integer which contains the set of flags received on successful return from the call.\n\nThe possible return values include any status from evl_wait_flags(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_post_flags(struct evl_flags *flg, int bits)  This call posts a (non-null) set of events to a flag group. The core adds the posted bits to the group\u0026rsquo;s current value using a bitwise OR operation. Afterwards, it unblocks the thread heading the wait queue of the flag group at the time of the call if any (i.e. having the highest priority among waiters).\nflgThe in-memory flag group descriptor constructed by either evl_create_flags() or evl_open_flags(), or statically built with EVL_FLAGS_INITIALIZER. In the latter case, an implicit call to evl_create_flags() for flg is issued before the event mask is posted, which may trigger a transition to the in-band execution mode for the caller.\n\nbitsThe non-zero bitmask to add to the flag group value.\n\nevl_post_flags() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tflg does not represent a valid in-memory flag group descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n-EINVAL\tbits is zero.\nIf flg was statically initialized with EVL_FLAGS_INITIALIZER but not passed to any flag group-related call yet, then any error status returned by evl_create_flags() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_trywait_flags(struct evl_flags *flg, int *r_bits)  This call attempts to consume the event flags currently posted to a group without blocking the caller if there are none. If some events were pending at the time of the call, the group value is reset to zero before returning to the caller.\nflgThe in-memory flag group descriptor constructed by either evl_create_flags() or evl_open_flags(), or statically built with EVL_FLAGS_INITIALIZER. In the latter case, an implicit call to evl_create_flags() for flg is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nr_bitsThe address of an integer which contains the set of flags received on successful return from the call.\n\nevl_trywait_flags() returns zero on success and the non-zero set of pending events. Otherwise, a negated error code may be returned if:\n-EAGAIN flg had no event pending at the time of the call.\n-EINVAL\tflg does not represent a valid in-memory flag group descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf flg was statically initialized with EVL_FLAGS_INITIALIZER, then any error returned by evl_create_flags() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_peek_flags(struct evl_flags *flg, int *r_bits)  This call is a variant of evl_trywait_flags() which does not consume the flags before returning to the caller. In other words, the group value is not reset to zero before returning a non-zero set of pending events, allowing the group value to be read multiple times with no side-effect.\nflgThe in-memory flag group descriptor constructed by either evl_create_flags() or evl_open_flags(), or statically built with EVL_FLAGS_INITIALIZER. In the latter case, the flag group becomes valid for a call to evl_peek_flags() only after a post or [try]wait operation was issued for it.\n\nr_bitsThe address of an integer which contains the group value on successful return from the call.\n\nevl_peek_flags() returns zero on success along with the current group value. Otherwise, a negated error code may be returned if:\n-EINVAL\tflg does not represent a valid in-memory flag group descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_close_flags(struct evl_flags *flg)  You can use evl_close_flags() to dispose of an EVL flag group, releasing the associated file descriptor, at which point flg will not be valid for any subsequent operation from the current process. However, this flag group is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_flags() have been released, whether from the current process or any other process.\nflgThe in-memory descriptor of the flag group to dismantle.\n\nevl_close_flags() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tflg does not represent a valid in-memory flag group descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized flag group descriptor which has never been used in wait or post operations always returns zero.\n Events pollable from an event flag group descriptor The evl_poll() interface can monitor the following events occurring on an event flag group descriptor:\n  POLLIN and POLLRDNORM are set whenever the flag group value is non-zero, which means that a subsequent attempt to read it by a call to evl_wait_flags(), evl_trywait_flags() or evl_timedwait_flags() might be successful without blocking (i.e. unless another thread sneaks in in the meantime and collects the pending flags).\n  POLLOUT and POLLWRNORM are set whenever the flag group value is zero, which means that no flag is pending at the time of the call. As a result, polling for such status waits for all pending events to have been read by the receiving side.\n   Last modified: Mon, 17 May 2021 16:48:45 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/semaphore/",
	"title": "Kernel semaphore",
	"tags": [],
	"description": "",
	"content": "void evl_init_ksem(struct evl_ksem *ksem, unsigned int value)  void evl_destroy_ksem(struct evl_ksem *ksem)  int evl_down_timeout(struct evl_ksem *ksem, ktime_t timeout)  int evl_down(struct evl_ksem *ksem)  int evl_trydown(struct evl_ksem *ksem)  void evl_up(struct evl_ksem *ksem)  void evl_broadcast(struct evl_ksem *ksem)  Last modified: Sat, 15 Feb 2020 19:38:30 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/semaphore/",
	"title": "Semaphore",
	"tags": [],
	"description": "",
	"content": "Synchronizing on a semaphore EVL implements the classic Dijkstra semaphore construct, with an API close to the POSIX specification for the basic operations.\nSemaphore services   int evl_create_sem(struct evl_sem *sem, int clockfd, init initval, int flags, const char *fmt, ...)  This call creates a semaphore, returning a file descriptor representing the new object upon success. This is the generic call form; for creating a semaphore with common pre-defined settings, see [evl_new_sem()}(#evl_new_sem).\nsemAn in-memory semaphore descriptor is constructed by evl_create_sem(), which contains ancillary information other calls will need. sem is a pointer to such descriptor of type struct evl_sem.\n\nclockfdSome semaphore-related calls are timed like evl_timedget_sem() which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\ninitvalThe initial value of the semaphore count.\n\nflagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new semaphore in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the semaphore name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_sem() returns the file descriptor of the newly created semaphore on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither clockfd does not refer to a valid EVL clock, or the generated semaphore name is badly formed, likely containing invalid character(s), such as a slash. Keep in mind that it should be usable as a basename of a device element\u0026rsquo;s file path.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL semaphore.\n  #include \u0026lt;evl/sem.h\u0026gt; static struct evl_sem sem; void create_new_sem(void) { int fd; fd = evl_create_sem(sem, EVL_CLOCK_MONOTONIC, 1, EVL_CLONE_PRIVATE, \u0026quot;name_of_semaphore\u0026quot;); /* skipping checks */ return fd; }   int evl_new_sem(struct evl_sem *sem, const char *fmt, ...)  This call is a shorthand for creating a zero-initialized private semaphore, timed on the built-in EVL monotonic clock. It is identical to calling:\n\tevl_create_sem(sem, EVL_CLOCK_MONOTONIC, 0, EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_SEM_INITIALIZER((const char *) name, (int) clockfd, (int) initval, (int) flags)  The static initializer you can use with semaphores. All arguments to this macro refer to their counterpart in the call to evl_create_sem().\nstruct evl_sem sem = EVL_SEM_INITIALIZER(\u0026quot;name_of_semaphore\u0026quot;, EVL_CLOCK_MONOTONIC, 1, EVL_CLONE_PUBLIC);   int evl_open_sem(struct evl_sem *sem, const char *fmt, ...)  You can open an existing semaphore, possibly from a different process, by calling evl_open_sem().\nsemAn in-memory semaphore descriptor is constructed by evl_open_sem(), which contains ancillary information other calls will need. sem is a pointer to such descriptor of type struct evl_sem. The information is retrieved from the existing semaphore which was opened.\n\nfmtA printf-like format string to generate the name of the semaphore to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_sem() returns the file descriptor referring to the opened semaphore on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to a semaphore.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_get_sem(struct evl_sem *sem)  This service decrements a semaphore by one. If the resulting semaphore value is negative, the caller is put to sleep by the core until a subsequent call to evl_put_sem() eventually releases it. Otherwise, the caller returns immediately. Waiters are queued by order of scheduling priority.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before a get operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_get_sem() returns zero on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER, then any error returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedget_sem(struct evl_sem *sem, const struct timespec *timeout)  This call is a variant of evl_get_sem() which allows specifying a timeout on the get operation, so that the caller is unblocked after a specified delay sleeping without being unblocked by a subsequent call to evl_put_sem().\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() is issued for sem before a get operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\ntimeoutA time limit to wait for the caller to be unblocked before the call returns on error. The clock mentioned in the call to evl_create_sem() will be used for tracking the elapsed time.\n\nThe possible return values include any status from evl_get_sem(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_put_sem(struct evl_sem *sem)  This call posts a semaphore by incrementing its count by one. If a thread is sleeping on the semaphore as a result of a previous call to evl_get_sem() or evl_timedget_sem(), the thread heading the wait queue is unblocked.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before the semaphore is posted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_put_sem() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER but not passed to any semaphore-related call yet, then any error status returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_tryget_sem(struct evl_sem *sem, int *r_bits)  This call attempts to decrement the semaphore provided the result does not lead to a negative count.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_tryget_sem() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN sem count value is zero or negative at the time of the call.\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER, then any error returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_peek_sem(struct evl_sem *sem, int *r_val)  This call returns the count value of the semaphore. If a negative count is returned in *r_val, its absolute value can be interpreted as the number of waiters sleeping on the semaphore\u0026rsquo;s wait queue (at the time of the call). A zero or positive value means that the semaphore is not contended.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, the semaphore becomes valid for a call to evl_peek_sem() only after a put or [try]get operation was issued for it.\n\nr_valThe address of an integer which contains the semaphore value on successful return from the call.\n\nevl_peek_sem() returns zero on success along with the current semaphore count. Otherwise, a negated error code may be returned if:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_close_sem(struct evl_sem *sem)  You can use evl_close_sem() to dispose of an EVL semaphore, releasing the associated file descriptor, at which point sem will not be valid for any subsequent operation from the current process. However, this semaphore is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_sem() have been released, whether from the current process or any other process.\nsemThe in-memory descriptor of the semaphore to dismantle.\n\nevl_close_sem() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller\u0026rsquo;s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized semaphore descriptor which has never been used in get or put operations always returns zero.\n Events pollable from a semaphore file descriptor The evl_poll() interface can monitor the following events occurring on a semaphore file descriptor:\n POLLIN and POLLRDNORM are set whenever the semaphore count is strictly positive, which means that a subsequent attempt to deplete it by a call to evl_get_sem(), evl_tryget_sem() or evl_timedget_sem() might be successful without blocking (i.e. unless another thread sneaks in in the meantime and fully depletes the semaphore).   Last modified: Mon, 27 Apr 2020 19:01:01 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/ports/",
	"title": "Status of EVL ports",
	"tags": [],
	"description": "",
	"content": "The table below summarizes the current status of the EVL ports to particular SoCs which we are aware of. It is definitely not meant to be exhaustive: many SoCs which are not listed here may just work out of the box with a recent EVL-enabled kernel release, especially in the x86 and arm64 ecosystems (arm32 might require more work in some cases). New SoCs sporting interrupt chip controllers, timer or clock sources which have not been made pipeline-aware yet may need the corresponding kernel drivers to be marginally fixed up for that purpose. See the porting guide for more.\nIf you are interested in porting your own autonomous core to a particular kernel release Dovetail supports, you certainly need the IRQ pipeline column matching the target platform to be checked, and likely the Alternate scheduling column as well. Running the EVL core requires both features to be available. If you ported EVL to a SoC which does not appear in this list and want to let people know about it, please drop me a note at rpm@xenomai.org.\nThe Xenomai 4 project primarily tracks the mainline Linux kernel. In absence of any specific information, all kernel releases mentioned below refer to mainline Linux.\n To get EVL running on a platform, we need the following software to be ported in the following sequence:\n  the Dovetail interface. This task is composed of two incremental milestones: first getting the interrupt pipeline to work, then enabling the alternate scheduling. Porting Dovetail is where most of the work takes place, the other porting tasks are comparatively quite simple.\n  the EVL core which is mostly composed of architecture-independent code, therefore only a few bits need to be ported (a FPU test helper for the most part).\n  the EVL library. Likewise, this code has very little dependencies on the underlying CPU architecture and platform. A port boils down to resolving the address of the clock_gettime(3) in the vDSO.\n   Current target kernel release\n Linux v5.14\n ARM64 SoC\n   #ports { width: 70%; } #ports td { text-align: center; }   SoC (Board) IRQ pipeline1 Alternate scheduling EVL base2 EVL stress3 Test kernel   Amlogic S905X3 (Odroid C4)     v5.12   Broadcom BCM2711 (Raspberry PI 4 Model B)     v5.12   Broadcom BCM2837 (Raspberry PI 3 Model B)     v5.7-rc7   Qualcomm QCS404     v5.1-rc3   Qualcomm Snapdragon 410E (DragonBoard 410c)     v5.0   HiSilicon Kirin 620 (HiKey LeMaker)     v5.5-rc7   NXP i.MX8M Mini (Variscite DART-MX8M-MINI)     v5.4.40 *   QEMU virt     v5.13-rc6   Xilinx Zynq UltraScale+ (ZCU102)     v5.4.40   * Mainline kernel with SoC-specific bits picked from the vendor tree.\n ARM SoC\n  SoC (Board) IRQ pipeline1 Alternate scheduling EVL base2 EVL stress3 Test kernel   TI AM335x-GP (BeagleBone Black)     v5.13   Broadcom BCM2835 (Raspberry PI Zero)     v5.7-rc6   Broadcom BCM2836 (Raspberry PI 2 Model B)     v5.9   STMicro Cannes2-STiH410 (B2260)     v5.2-rc7   Altera Cyclone V SoC FPGA (DevKit)     v5.7   Exynos 5422 (Odroid XU4Q)     v5.13   AllWinner H3 (NanoPI NEO)     v5.5-rc2   NXP i.MX6QP (SabreSD)     v5.13   NXP i.MX6Q (phyBOARD-Mira)     v5.12   NXP i.MX7D (SabreSD)     v5.8    X86_64\n  Chipset (Module) IRQ pipeline1 Alternate scheduling EVL base2 EVL stress3 Test kernel   QEMU KVM     v5.13-rc6   Intel Atom x5-E3940 (TQMxE39M)     v5.12   Intel C236 core i7 quad (DFI SD631)     v5.9   Intel Desktop Board DQ45CB (Legacy)     v5.11    1 Means that the pipeline torture tests pass (see CONFIG_IRQ_PIPELINE_TORTURE_TEST). This milestone guarantees that we can deliver high-priority interrupt events immediately to a guest core, regardless of the work ongoing for the main kernel.\n2 When this box is checked, EVL\u0026rsquo;s basic functional test suite runs properly on the platform, which is a good starting point. So far so good.\n3 When this box is checked, the EVL core passes a massive stress test involving the hectic and latmus applications running in parallel along with the full test suite for 24 hrs, all glitchlessly. This denotes a reliable state, including flawless alternate scheduling of threads between the main kernel and EVL. On the contrary, a problem with sharing the FPU unit properly between the in-band and out-of-band execution contexts is most often the reason for keeping this box unchecked until the situation is fixed.\n Last modified: Tue, 10 Aug 2021 10:52:41 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/clock/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Clock element The target platform can provide particular clock chips and/or clock source drivers in addition to the architecture-specific ones. For instance, some device on a PCI bus could implement a timer chip which the application wants to use for timing its threads, in addition to the architected timer found on ARM64 and some ARM-based SoCs. In this case, we would need a specific clock driver, binding the timer hardware to the EVL core.\nEVL\u0026rsquo;s clock element ensures all clock drivers present the same interface to applications in user-space. In addition, the clock element can export individual software timers to applications which comes in handy for running periodic loops or waiting for oneshot events on a specific time base.\nSome built-in clocks are pre-defined by the EVL core for reading the monotonic and wallclock time of the system.\nEVL abstracts clock event devices and clock sources (timekeeping hardware) into a single clock element.\n Clock services  int evl_read_clock(int clockfd, struct timespec *tp)  This call is the EVL equivalent of the POSIX clock_gettime(3) service, for reading the time from a specified EVL clock. It reads the current time maintained by the EVL clock, which is returned as counts of seconds and microseconds since the clock\u0026rsquo;s epoch.\nclockfdThe clock file descriptor, which can be:\n  any valid file descriptor received as a result of opening a clock device in /dev/evl/clock.\n  the identifier of a built-in EVL clock, such as EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME.\n  \ntpA pointer to a timespec structure which should receive the time stamp.\n\nevl_read_clock() writes the timestamp to tp then returns zero on success, otherwise:\n  -EBADF if clockfd is neither a built-in clock identifier or a valid file descriptor.\n  -ENOTTY if clockfd does not refer to an EVL clock device.\n  -EFAULT if tp points to invalid memory.\n    int evl_set_clock(int clockfd, const struct timespec *tp)  This call is the EVL equivalent of the POSIX clock_settime(3) service, for setting the time of a specified EVL clock.\nclockfdThe clock file descriptor, which can be:\n  any valid file descriptor received as a result of opening a clock device in /dev/evl/clock.\n  the identifier of a built-in EVL clock, such as EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME.\n  \ntpA pointer to a timespec structure which should receive the time stamp.\n\nevl_set_clock() returns zero and the clock is set to the specified time on success, otherwise:\n  -EINVAL if the time specification referred to by ts is invalid (e.g. ts-\u0026gt;tv_nsec not in the [0..1e9] range).\n  -EBADF if clockfd is neither a built-in clock identifier or a valid file descriptor.\n  -EPERM if the caller does not have the permission to set some built-in clock via clock_settime(3). See note.\n  -ENOTTY if clockfd does not refer to an EVL clock device.\n  -EFAULT if tp points to invalid memory.\n  Setting EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME may imply a transition to the in-band execution stage as clock_settime() is called internally for carrying out the request.\n   int evl_get_clock_resolution(int clockfd, struct timespec *tp)  This call is the EVL equivalent of the POSIX clock_getres(3) service, for reading the resolution of a specified EVL clock. It returns this value as counts of seconds and microseconds. The resolution of clocks depends on the implementation and cannot be configured.\nclockfdThe clock file descriptor, which can be:\n  any valid file descriptor received as a result of opening a clock device in /dev/evl/clock.\n  the identifier of a built-in EVL clock, such as EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME.\n  \ntpA pointer to a timespec structure which should receive the resolution.\n\nevl_get_clock_resolution() writes the resolution to tp then returns zero on success, otherwise:\n  -EBADF if clockfd is neither a built-in clock identifier or a valid file descriptor.\n  -ENOTTY if clockfd does not refer to an EVL clock device.\n  -EFAULT if tp points to invalid memory.\n  Setting EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME may imply a transition to the in-band execution stage as clock_getres(3) is called internally for carrying out the request.\n   int evl_sleep_until(int clockfd, const struct timespec *timeout)  This call is the EVL equivalent of the POSIX clock_nanosleep(3) service, for blocking the caller until an arbitrary date expires on a specified EVL clock. Unlike its POSIX counterpart, evl_sleep_until() only accepts absolute timeout specifications though.\nclockfdThe clock file descriptor, which can be:\n  any valid file descriptor received as a result of opening a clock device in /dev/evl/clock.\n  the identifier of a built-in EVL clock, such as EVL_CLOCK_MONOTONIC or EVL_CLOCK_REALTIME.\n  \ntimeoutA pointer to a timespec structure which specifies the wake up date.\n\nevl_sleep_until() blocks the caller until the wake up date expires then returns zero on success, otherwise:\n  -EBADF if clockfd is neither a built-in clock identifier or a valid file descriptor.\n  -ENOTTY if clockfd does not refer to an EVL clock device.\n  -EFAULT if timeout points to invalid memory.\n  -EINTR if the call was interrupted by an in-band signal, or forcibly unblocked by the EVL core.\n    int evl_usleep(useconds_t usecs)  This call puts the caller to sleep until a count of microseconds has elapsed. evl_usleep() invokes evl_sleep_until() for sleeping until the delay expires on the EVL_CLOCK_MONOTONIC clock.\nusecsThe count of microseconds to sleep for.\n\nevl_usleep() blocks the caller until the delay expires then returns zero on success, otherwise:\n -EINTR if the call was interrupted by an in-band signal, or forcibly unblocked by the EVL core.  Pre-defined clocks EVL defines two built-in clocks, you can pass any of the following identifiers to EVL calls which ask for a clock file descriptor (usually noted as clockfd):\n  EVL_CLOCK_MONOTONIC is identical to the CLOCK_MONOTONIC POSIX clock, which is a monotonically increasing clock that cannot be set and represents time since some unspecified starting point (aka the epoch). This identifier has the same meaning than a file descriptor opened on /dev/evl/clock/monotonic.\n  EVL_CLOCK_REALTIME is identical to the CLOCK_REALTIME POSIX clock, which is a non-monotonic wall clock which can be manually set to an arbitrary value with proper privileges, and can also be subject to dynamic adjustements by the NTP system. This identifier has the same meaning than a file descriptor opened on /dev/evl/clock/realtime.\n  If you are to measure the elapsed time between two events, you definitely want to use EVL_CLOCK_MONOTONIC.\n Last modified: Tue, 14 Apr 2020 18:39:50 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/flag/",
	"title": "Kernel flag",
	"tags": [],
	"description": "",
	"content": "void evl_init_flag(struct evl_flag *wf)  void evl_destroy_flag(struct evl_flag *wf)  int evl_wait_flag_timeout(struct evl_flag *wf, ktime_t timeout, enum evl_tmode timeout_mode)  int evl_wait_flag(struct evl_flag *wf)  struct evl_thread *evl_wait_flag_head(struct evl_flag *wf)  void evl_raise_flag_nosched(struct evl_flag *wf)  void evl_raise_flag(struct evl_flag *wf)  void evl_pulse_flag_nosched(struct evl_flag *wf)  void evl_pulse_flag(struct evl_flag *wf)  void evl_flush_flag_nosched(struct evl_flag *wf, int reason)  void evl_flush_flag(struct evl_flag *wf, int reason)  void evl_clear_flag(struct evl_flag *wf)  DEFINE_EVL_FLAG(__name)  Last modified: Sat, 15 Feb 2020 19:38:30 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/spinlock/",
	"title": "EVL Spinlock",
	"tags": [],
	"description": "",
	"content": "evl_spin_lock_init(__lock)  evl_spin_lock(__lock)  evl_spin_lock_irqsave(__lock, __flags)  evl_spin_unlock(__lock)  evl_spin_unlock_irqrestore(__lock, __flags)  DEFINE_EVL_SPINLOCK(__lock)  Last modified: Thu, 28 Nov 2019 18:52:38 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/clock/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "The target platform can provide particular clock chips and/or clock source drivers in addition to the architecture-specific ones. For instance, some device on a PCI bus could implement a timer chip which the application wants to use for timing its threads, in addition to the architected timer found on ARM64 and some ARM-based SoCs. In this case, we would need a specific clock driver, binding the timer hardware to the EVL core.\nThe EVL clock device interface gives you a framework for integrating such hardware into the real-time core, which in turns enables applications to use it.\nEVL abstracts clock event devices and clock sources (timekeeping hardware) into a single clock element.\n   int evl_init_clock(struct evl_clock *clock, const struct cpumask *affinity)  Initializes a new clock which may be used to drive EVL core timers.\nclockA descriptor describing the clock properties, which the core is also going to use for maintaining some runtime information. See below.\n\naffinityThe set of CPUs we may expect the backing clock device to tick on, usually denoting a per-CPU device. As a special case, passing a NULL affinity mask means that we have to deal with a global device which does not issue per-CPU events, in which case outstanding timers based on this clock will be maintained into a single global queue instead of per-CPU timer queues. At least one out-of-band CPU mentioned in evl_oob_cpus should be present in the clock affinity mask.\n\nThe clock descriptor should be filled in by the caller with the following information on entry to evl_init_clock():\n  .name: name of the clock. If the clock has public visibility, the file representing this clock in the /dev/evl hierarchy is named after it.\n  .resolution: the resolution of the clock in nanoseconds.\n  .flags: a set of creation flags for the new clock, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public clock which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to application processes for sharing.\n  EVL_CLONE_PRIVATE denotes a clock which is private to the kernel. No device file appears for it in the /dev/evl file hierarchy.\n    .gravity: the clock gravity value, which is a set of static adjustment values to account for the basic latency of the target system for responding to timer events, as described in this document.\n  .ops: a set of operation handlers which should be implemented by the caller:\n  ktime_t (*read)(struct evl_clock *clock)\n  u64 (*read_cycles)(struct evl_clock *clock)\n  int (*set)(struct evl_clock *clock, ktime_t date)\n  void (*program_local_shot)(struct evl_clock *clock)\n  void (*program_remote_shot)(struct evl_clock *clock, struct evl_rq *rq)\n  int (*set_gravity)(struct evl_clock *clock, const struct evl_clock_gravity *p)\n  void (*reset_gravity)(struct evl_clock *clock)\n  void (*adjust)(struct evl_clock *clock)\n     A typical example: the EVL core monotonic clock\n struct evl_clock evl_mono_clock = { .name = EVL_CLOCK_MONOTONIC_DEV, /* i.e. \u0026quot;monotonic\u0026quot; */ .resolution = 1, /* nanosecond. */ .flags = EVL_CLONE_PUBLIC, .ops = { .read = read_mono_clock, .read_cycles = read_mono_clock_cycles, .program_local_shot = evl_program_proxy_tick, #ifdef CONFIG_SMP .program_remote_shot = evl_send_timer_ipi, #endif .set_gravity = set_coreclk_gravity, .reset_gravity = reset_coreclk_gravity, }, }; evl_init_clock() returns zero on success, or a negated error code otherwise:\n  -EEXIST\tThe generated name is conflicting with an existing clock name.\n  -EINVAL\tEither clock-\u0026gt;flags or affinity are wrong.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -ENOMEM\tNot enough memory available.\n   evl_init_slave_clock(struct evl_clock *clock, struct evl_clock *master)  int evl_register_clock(struct evl_clock *clock, const struct cpumask *affinity)  void evl_unregister_clock(struct evl_clock *clock)  void evl_announce_tick(struct evl_clock *clock)  void evl_adjust_timers(struct evl_clock *clock, ktime_t delta)  ktime_t evl_read_clock(struct evl_clock *clock)  ktime_t evl_ktime_monotonic(void)  u64 evl_read_clock_cycles(struct evl_clock *clock)  int evl_set_clock_time(struct evl_clock *clock, const struct timespec *ts)  void evl_set_clock_resolution(struct evl_clock *clock, ktime_t resolution)  ktime_t evl_get_clock_resolution(struct evl_clock *clock)  int evl_set_clock_gravity(struct evl_clock *clock, const struct evl_clock_gravity *gravity)  struct evl_clock *evl_get_clock_by_fd(int efd)  void evl_put_clock(struct evl_clock *clock)  Last modified: Sun, 13 Dec 2020 19:00:43 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/timer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Running oneshot or periodic timers At some point, you will most likely need to synchronize some thread with ultra-low latency clock events, either once or according to a recurring period. The EVL core gives you timers for this purpose, which can be obtained on any existing EVL clock. EVL timers support synchronous delivery, there is no asynchronous notification mechanism of clock events. This means that a client thread must explicitly wait for the next expiry by either reading or polling for such event. Would you choose to poll for clock events on a timer, you would still have to collect these events once they have occurred by reading the timer file descriptor.\nEach EVL timer is referred to by a file descriptor. The way you would use those timers mimics the usage of the timerfd API for the most part, which consists of:\n creating a timer 2. setting up a timeout date, either oneshot or recurring 3. waiting for the (next) timeout to elapse by calling oob_read() for the timer file descriptor. The value collected by reading a timer is the number of elapsed ticks since the last readout. Therefore, any value greater than 1 would denote an overrun condition.   A simple periodic loop using an EVL timer\n \t#include \u0026lt;evl/thread.h\u0026gt; #include \u0026lt;evl/timer.h\u0026gt; #include \u0026lt;evl/clock.h\u0026gt; #include \u0026lt;evl/proxy.h\u0026gt; void timespec_add_ns(struct timespec *__restrict r, const struct timespec *__restrict t, long ns) { long s, rem; s = ns / 1000000000; rem = ns - s * 1000000000; r-\u0026gt;tv_sec = t-\u0026gt;tv_sec + s; r-\u0026gt;tv_nsec = t-\u0026gt;tv_nsec + rem; if (r-\u0026gt;tv_nsec \u0026gt;= 1000000000) { r-\u0026gt;tv_sec++; r-\u0026gt;tv_nsec -= 1000000000; } } int main(int argc, char *argv[]) { struct itimerspec value, ovalue; int tfd, tmfd, ret, n = 0; struct timespec now; __u64 ticks; /* Attach to the core. */ tfd = evl_attach_self(\u0026quot;periodic-timer:%d\u0026quot;, getpid()); check_this_fd(tfd); /* Create a timer on the built-in monotonic clock. */ tmfd = evl_new_timer(EVL_CLOCK_MONOTONIC); check_this_fd(tmfd); /* Set up a 1 Hz periodic timer. */ ret = evl_read_clock(EVL_CLOCK_MONOTONIC, \u0026amp;now); check_this_status(ret); /* EVL always uses absolute timeouts, add 1s to the current date */ timespec_add_ns(\u0026amp;value.it_value, \u0026amp;now, 1000000000ULL); value.it_interval.tv_sec = 1; value.it_interval.tv_nsec = 0; ret = evl_set_timer(tmfd, \u0026amp;value, \u0026amp;ovalue); check_this_status(ret); for (;;) { /* Wait for the next tick to be notified. */ ret = oob_read(tmfd, \u0026amp;ticks, sizeof(ticks)); check_this_status(ret); if (ticks \u0026gt; 1) { fprintf(stder, \u0026quot;timer overrun! %lld ticks late\\n\u0026quot;, ticks - 1); break; } evl_printf(\u0026quot;TICKED, loops=%d\\n\u0026quot;, n++); } /* Disable the timer (not required if closing). */ value.it_interval.tv_sec = 0; value.it_interval.tv_nsec = 0; ret = evl_set_timer(tmfd, \u0026amp;value, NULL); check_this_status(ret); return 0; } Timer services  int evl_new_timer(int clockfd)  This call creates an EVL timer on the clock referred to by clockfd, returning a file descriptor representing the new object upon success. There is no arbitrary limit on the number of timers an application can create, which is only limited to the available system resources.\nclockfdThe file descriptor of the reference clock the new timer should be synchronized on. clockfd refer to any valid clock element known from the EVL core, including one of the pre-defined clocks.\n\nevl_new_timer() returns the file descriptor of the newly created timer on success. Otherwise, a negated error code is returned:\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO\tThe EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL timer.\n    int evl_set_timer(int efd, struct itimerspec *value, struct itimerspec *ovalue)  You should use this call to arm or disarm an EVL timer. It sets the next expiration date and reload value for the timer referred to by efd. If ovalue is not NULL, the current expiration date and reload value are stored at this location prior to updating them, as with evl_get_timer.\nefdA file descriptor referring to the EVL timer to set up.\n\nvalueA pointer to the interval timer specification structure which contains the new absolute expiration date and reload value. If both value-\u0026gt;it_value.tv_nsec and value-\u0026gt;it_value.tv_sec are zero, the timer is disarmed and value-\u0026gt;it_interval is ignored. Otherwise, the timer is program to timeout at the specified date mentioned in value-\u0026gt;it_value. If at least on field of value-\u0026gt;it_interval is non-zero, the EVL will reprogram the timer after each timeout according to this period automatically.\n\novalueIf non-NULL, the previous expiration date and reload value of the timer are copied to this memory location.\n\nZero is returned on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL timer.\n  -EINVAL some field in *value is not valid, likely it_value.tv_nsec and/or it_interval.tv_nsec are out of the valid range, which is between 0 and 1e9 - 1.\n  -EFAULT\teither value or ovalue (if non-NULL) point to invalid memory.\n    int evl_get_timer(int efd, struct itimerspec *value)  Retrieve the current timeout and reload values of an EVL timer.\nefdA file descriptor referring to the EVL timer to query.\n\nvalueThe current expiration date and reload value of the timer are copied to this memory location. The expiration date is always given in absolute form based on the reference clock for the timer.\n\nZero is returned on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL timer.\n  -EFAULT\tif value points to invalid memory.\n   Events pollable from a timer file descriptor The evl_poll() interface can monitor the following events occurring on a timer file descriptor:\n POLLIN and POLLRDNORM are set whenever a timeout event has fired for the timer, which is yet to be collected by the parent process. Timer events should be awaited for and collected by a call to oob_read().   Last modified: Mon, 27 Apr 2020 19:01:01 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/contributing/",
	"title": "Contributing to Xenomai 4",
	"tags": [],
	"description": "",
	"content": "We obviously welcome contribution to this effort, small or large. If you are interested in contributing to Xenomai 4 here are some helpful hints on how to get started. Start small, then progress as you get your feet wet. There is no such thing as a minor contribution, there is no shame in making mistakes. A contributor who submits a perfectible one-liner surely advances the project further than any smart lurker. Things you will need:\n  to build EVL from source for your platform of choice, unless you precisely want to port EVL to that platform.\n  access to the Xenomai mailing list\n  You can contribute to Dovetail, the EVL core, libevl, and/or the documentation effort, there is a lot to be done in all areas.\nSubmitting patches Xenomai 4 follows the kernel coding style for Dovetail, the EVL core, and libevl too.\nPlease send individual patch or series to the Xenomai mailing list for review.\nGetting the basics right Understanding the two key concepts of interrupt pipelining and alternate scheduling implemented by Dovetail is a must. This is what enables the kernel to hand over high priority events and tasks to a companion core such as EVL (and any other Dovetail client for that purpose) to get them processed with bounded, ultra-low latency.\nPorting EVL to a new platform This basically means porting Dovetail to that platform, in the sense that most machine and architecture-specific code we may need involves running the interrupt pipeline the EVL core depends on. Luckily, a Dovetail porting guide is available to help you in this task, explaining the fundamentals of a port, and what changes should be applied to the vanilla mainline kernel.\nDigging into the EVL core internals First, the documentation on this website tends to explicitly refer to particular pieces of code via Permalinks to GIT commits when discussing a matter. This should give you direct pointers to the implementation in many occasions. For instance, you can find some of these in the discussion about the alternate scheduling technique Dovetail implements and which the EVL core uses, or in the discussion about out-of-band GPIO services.\nSecond, there is a work-in-progress section called \u0026ldquo;Under the hood\u0026rdquo;.\nLast but not least, there is always the mailing list to ask for implementation details.\nUse the rules of thumb This document gives you a few but important hints, tips and tricks when developing in a dual kernel environment based on Dovetail.\n Last modified: Sun, 06 Jun 2021 12:03:24 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/observable/",
	"title": "Observable",
	"tags": [],
	"description": "",
	"content": "Using the observer pattern for event-driven applications The EVL core provides a simple yet flexible support for implementing the observer design pattern in your application, based on the Observable element. The Observable receives a stream of so-called notices, which may be events, state changes or any data which fits within a 64bit word, delivering them as notifications (with additional meta-data) to subscribed threads called observers, when they ask for it. Subscribers do not have to be attached to the EVL core, any thread can observe an Observable element. Whether the threads which produce the events, the Observable and the observers live in the same or different processes is transparent.\nBroadcast mode By default, an Observable broadcasts a copy of every event it receives to every subscribed observer:\n    Master mode In some cases, you may want to use an Observable to dispatch work submitted as events to the set of observers which forms a pool of worker threads. Each message would be sent to a single worker, each worker would be picked on a round-robin basis so as to implement a naive load-balancing strategy. Setting the Observable in the so-called master mode (see EVL_CLONE_MASTER) at creation time enables this behavior:\n    Creating an Observable   int evl_create_observable(int flags, const char *fmt, ...)  This call creates an Observable element, returning a file descriptor representing the new object upon success. This is the generic call form; for creating an Observable with common pre-defined settings, see evl_new_observable().\nAn Observable can operate either in broadcast or master mode:\n  in broadcast mode, a copy of every event received by the Observable is sent to every observer.\n  in master mode, each message received by the Observable is sent to a single observer. The subscriber to send a message to is picked according to a simple round-robin strategy among all observers.\n  flagsA set of creation flags for the new element, defining its visibility and operation mode:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_MASTER enables the master mode for the Observable.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new Observable in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the Observable name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_observable() returns the file descriptor of the newly created Observable on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing Observable name.\n  -EINVAL\tEither flags is wrong, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL Observable.\n  #include \u0026lt;evl/observable.h\u0026gt; void create_new_observable(void) { int efd; efd = evl_create_observable(EVL_CLONE_PUBLIC|EVL_CLONE_MASTER, \u0026quot;name_of_observable\u0026quot;); /* skipping checks */ return efd; }   int evl_new_observable(const char *fmt, ...)  This call is a shorthand for creating a private observable operating in broadcast mode. It is identical to calling:\n\tevl_create_observable(EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n  Working with an Observable Once an Observable is created, using it entails the following steps, performed by either the thread(s) issuing the stream of events to the Observable, or those observing these events:\nObservers need to subscribe to the Observable. Observers can subscribe and unsubscribe at will, come and go freely during the Observable\u0026rsquo;s lifetime. Depending on its subscription flags, the observer may ask for merging consecutive identical notices or receiving all of them unfiltered (See EVL_NOTIFY_ONCHANGE).\n    Event streamers can send notices to the Observable by calling evl_update_observable().\n    Observers can read notifications from the Observable by calling evl_read_observable().\n    Once an Observable has become useless, you only need to close all the file descriptors referring to it in order to release it, like for any ordinary file.\n  int evl_update_observable(int efd, const struct evl_notice *ntc, int nr)  This call sends up to nr notices starting at address ntc in memory to the Observable referred to by efd. This call never blocks the caller. It may be used by any thread, including non-EVL ones.\nA notice contains a tag, and an opaque event value. It is defined by the following C types:\nunion evl_value { int32_t val; int64_t lval; void *ptr; }; struct evl_notice { uint32_t tag; union evl_value event; }; All notices sent to the Observable should carry a valid issuer tag in the evl_notice.tag field. For applications, a valid tag is an arbitrary value greater or equal to EVL_NOTICE_USER (lower tag values are reserved to the core for HM diag codes).\nevl_update_observable() returns the number of notices which have been successfully queued. Each notice which have been successfully queued for consumption by at least one observer counts for one in the return value. Zero is returned whenever no observer was subscribed to the Observable at the time of the call, or no buffer space was available for queuing any notification for any observer. Otherwise, a negated error code is returned on error:\n  -EBADF\tefd is not a valid file descriptor.\n  -EPERM\tefd does not refer to an Observable element. This may happen if efd refers to a valid EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\n  -EINVAL\tSome notice has an invalid tag.\n  -EFAULT\tif ntc points to invalid memory.\n   Sending a notice to an observable\n void send_notice(int ofd) { struct evl_notice notice; int ret; notice.tag = EVL_NOTICE_USER; notice.event.val = 42; ret = evl_update_observable(ofd, \u0026amp;notice, 1); /* ret should be 1 on success. */ }   int evl_read_observable(int efd, struct evl_notification *nf, int nr)  This call receives up to nr notifications starting at address nf in memory from the Observable referred to by efd. It may only be used by observers subscribed to this particular Observable. If O_NONBLOCK is clear for efd, the caller might block until at least one notification arrives.\nA notification contains the tag and the event value sent by the issuer of the corresponding notice, plus some meta-data. A notification is defined by the following C type:\nunion evl_value { int32_t val; int64_t lval; void *ptr; }; struct evl_notification { uint32_t tag; uint32_t serial; int32_t issuer; union evl_value event; struct timespec date; }; The meta-data is defined as follows:\n  serial is a monotonically increasing counter of notices sent by the Observable referred to by efd. In broadcast mode, this serial number is identical in every copy of a given original notice forwarded to the observers present.\n  issuer is the pid of the thread which issued the notice, zero if it was sent by the EVL core, such as with HM diagnostics.\n  date is a timestamp at receipt of the original notice, based on the built-in EVL_CLOCK_MONOTONIC clock.\n  evl_read_observable() returns the number of notifications which have been successfully copied to nf. This count may be lower than nr. Otherwise, a negated error code is returned on error:\n  -EAGAIN\tO_NONBLOCK is set for efd and no notification is pending at the time of the call.\n  -EBADF\tefd is not a valid file descriptor.\n  -EPERM\tefd does not refer to an Observable element. This may happen if efd refers to a valid EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\n  -ENXIO\tthe calling thread is not subscribed to the Observable referred to by efd.\n  -EFAULT\tif nf points to invalid memory.\n   Receiving a notification from an observable\n void receive_notification(int ofd) { struct evl_notification notification; int ret; ret = evl_read_observable(ofd, \u0026amp;notification, 1); /* ret should be 1 on success. */ }  Events pollable from an Observable The evl_poll() and poll(2) interfaces can monitor the following events occurring on an Observable:\n  POLLIN and POLLRDNORM are set whenever at least one notification is pending for the calling observer thread. This means that a subsequent call to evl_read_observable() by the same thread would return at least one valid notification immediately.\n  POLLOUT and POLLWRNORM are set whenever at least one observer subscribed to the Observable has enough room in its backlog to receive at least one notice. This means that a subsequent call to evl_update_observable() would succeed in queuing at least one notice to one observer. In case multiple threads may update the Observable concurrently, which thread might succeed in doing so cannot be determined (typically, there would be no such guarantee for the caller of evl_poll() and poll(2)).\n  In addition to these flags, POLLERR might be returned in case the caller did not subscribe to the Observable, or some file descriptor from the polled set refer to an EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\nObserving EVL threads An EVL thread is in and of itself an Observable element. Observability of a thread can be enabled by passing the EVL_CLONE_OBSERVABLE flag when attaching a thread to the EVL core. In this case, the file descriptor obtained from evl_attach_thread() may be subsequently used in Observable-related operations. If the thread was also made public (EVL_CLONE_PUBLIC), then there is a way for remote processes to monitor it via an access to its device.\nThreads can monitor events sent to an observable thread element by subscribing to it. An observable thread typically relays health monitoring information to subscribers. An observable thread can also do introspection, by subscribing to itself then reading the HM diagnostics it has received.\n Last modified: Wed, 27 Jan 2021 19:00:28 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/xbuf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Bi-directional out-of-band ⇔ in-band messaging A most typical requirement in dual kernel systems is to exchange data between threads running in separate execution stages, i.e. out-of-band vs in-band, without incuring any delay for the out-of-band side. The EVL core implements a feature called a cross-buffer (aka xbuf), which connects the sending out-of-band end of a communication channel to its receiving in-band end, and conversely. As a result, data sent to a cross-buffer by calling oob_write() are received by calling read(2), data sent by calling write(2) are received by calling oob_read(). A cross-buffer can be used for transferring fixed-size or variable-size data, supports message-based and byte-oriented streams. For instance, such a mechanism would enable a GUI front-end to receive monitoring data from a real-time work loop.\nxbuf-specific semantics for sending and receiving The following rules apply to cross-buffer transfers, in either directions (inbound and outbound):\n If O_NONBLOCK is clear on the cross-buffer file descriptor, there are no short reads on the receiving end: a reader always gets a complete message of the requested length, blocking if necessary except if a sender is currently blocked on the other end of the channel, waiting for some space to be available into the ring buffer to complete a write operation. In this case, a short read is performed to prevent a deadlock, which means the reader may receive less data than requested in the read call. This situation arises when the size picked for the ring buffer does not fit the transfer pattern, such as follows:  If you plan to send fixed-size messages through a cross-buffer and keeping the message boundaries intact matters, you should pick a buffer size which is a multiple of the basic message size, reading and writing complete messages at each transfer.\n Otherwise, if O_NONBLOCK is set and not enough bytes are immediately available from the ring buffer for satisfying the request, the write operation fails with the EAGAIN error status.\n There are no short or scattered writes to the ring buffer: the operation either succeeds immediately or blocks until enough space is available into the buffer for copying the entire message atomically, unless O_NONBLOCK is set on the cross-buffer file descriptor (EAGAIN).  Cross-buffer services  int evl_create_xbuf(size_t i_bufsz, size_t o_bufsz, int flags, const char *fmt, ...)  This call creates a new cross-buffer, then returns a file descriptor representing the communication channel upon success. Internally, a cross-buffer is implemented as a pair of ring buffers conveying data from in-band to out-of-band context (i.e. inbound traffic), and conversely (i.e. outbound traffic). oob_write() and oob_read() should be used by callers running on the out-of-band stage for sending/receiving data through this channel respectively. Conversely, write(2) and read(2) should be used for accessing the channel from the in-band stage.\ni_bufszThe size in bytes of the ring buffer conveying inbound traffic. If zero, the cross-buffer is intended to relay outbound messages exclusively, i.e. from the in-band to the out-of-band context. i_bufsz must not exceed 2^30.\n\no_bufszThe size in bytes of the ring buffer conveying outbound traffic. If zero, the cross-buffer is intended to relay inbound messages exclusively, i.e. from the out-of-band to the in-band context. i_bufsz must not exceed 2^30.\n\nflagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new cross-buffer in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the cross-buffer name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_xbuf() returns the file descriptor of the new cross-buffer on success. If the call fails, a negated error code is returned instead:\n  -EEXIST\tThe generated name is conflicting with an existing cross-buffer name.\n  -EINVAL\tEither flags is wrong, i_bufsz and/or o_bufsz are wrong, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO\tThe EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating a cross-buffer.\n    int evl_new_xbuf(size_t io_bufsz, const char *fmt, ...)  This call is a shorthand for creating a private cross-buffer with identically-sized I/O buffers. It is identical to calling:\n\tevl_create_xbuf(io_bufsz, io_bufsz, EVL_CLONE_PRIVATE, fmt, ...);  Note that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n  Events pollable from a cross-buffer descriptor The evl_poll() interface can monitor the following events occurring on a cross-buffer file descriptor:\n  POLLIN and POLLRDNORM are set whenever data coming from the in-band side is available for reading by a call to oob_read().\n  POLLOUT and POLLWRNORM are set whenever there is still room in the output ring buffer for sending more data to the in-band side using oob_write().\n  Conversely, you can also use the in-band poll(2) on a cross-buffer file descriptor, monitoring the following events:\n  POLLIN and POLLRDNORM are set whenever data coming from the out-of-band side is available for reading, by a call to read(2).\n  POLLOUT and POLLWRNORM are set whenever there is still room in the output ring buffer for sending more data to the out-of-band side, using write(2).\n   Last modified: Mon, 27 Apr 2020 19:01:01 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/timer/",
	"title": "Timer",
	"tags": [],
	"description": "",
	"content": "evl_init_timer(__timer, __clock, __handler, __rq, __flags)  evl_init_core_timer(__timer, __handler)  evl_init_timer_on_cpu(__timer, __cpu, __handler)  void evl_destroy_timer(struct evl_timer *timer)  void evl_start_timer(struct evl_timer *timer, ktime_t value, ktime_t interval)  void evl_stop_timer(struct evl_timer *timer)  ktime_t evl_get_timer_date(struct evl_timer *timer)  ktime_t evl_get_interval(struct evl_timer *timer)  ktime_t evl_get_timer_delta(struct evl_timer *timer)  ktime_t evl_get_stopped_timer_delta(struct evl_timer *timer)  Last modified: Sun, 13 Dec 2020 17:55:13 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/xbuf/",
	"title": "Cross-buffer access",
	"tags": [],
	"description": "",
	"content": "struct evl_xbuf *evl_get_xbuf(int efd, struct evl_file **efilpp)  void evl_put_xbuf(struct evl_file *efilp)  ssize_t evl_read_xbuf(struct evl_xbuf *xbuf, void *buf, size_t count, int f_flags)  ssize_t evl_write_xbuf(struct evl_xbuf *xbuf, const void *buf, size_t count, int f_flags)  Last modified: Thu, 28 Nov 2019 18:52:38 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/proxy/",
	"title": "File proxy",
	"tags": [],
	"description": "",
	"content": "Zero-latency I/O to in-band files A common issue with dual kernel systems stems from the requirement not to issue in-band system calls while running time-critical code in out-of-band context. Problem is that sometimes, you may need - for instance - to write to regular files such as for logging information to the console or elsewhere from an out-of-band work loop. Doing so by calling common stdio(3) routines directly is therefore not an option for latency reasons.\nThe EVL core solves this problem with the file proxy feature, which can push data to an arbitrary target file, and/or conversely read data pulled from such target file, providing an out-of-band I/O interface to the data producer and consumer threads. This works by offloading the I/O transfers to internal worker threads running in-band which relay the data to/from the target file. For this reason, I/O transfers may be delayed until the in-band stage resumes on a worker\u0026rsquo;s CPU. For instance, evl_printf() formats then emits the resulting string to fileno(stdout) via an internal proxy created by libevl.so at initialization.\n      Typically, out-of-band writers would send data through the proxy file descriptor using EVL\u0026rsquo;s oob_write() system call, which in-band readers could receive using the read(2) system call. Conversely, out-of-band readers would receive data through the proxy file descriptor using EVL\u0026rsquo;s oob_read() system call, which in-band writers could send using the write(2) system call. You can associate any type of file with a proxy, including a socket(2), eventfd(2), signalfd(2), pipe(2) and so on (see also the discussion below about the transfer granularity). This means that you could also use a proxy for signaling an in-band object from the out-of-band context, if doing so can be done using a regular write(2) system call, like eventfd(2) and signalfd(2).\nThe proxy also handles input and output operations from callers running on the in-band stage transparently.\n  Starting from kernel v5.10, a restricted set of file types supports proxying compared to earlier releases, specifically those for which the corresponding driver implements the read_iter(), write_iter() file operations. Regular files, sockets, pipes, tty and eventfd can still be used as proxy targets though.\n Logging debug messages via a proxy For instance, you may want your application to dump debug information to some arbitray file as it runs, including when running time-critical code out-of-band. Admittedly, this would add some overhead, but still low enough to keep the system happy, while giving you precious debug hints. Obviously, you cannot get away with calling the plain printf(3) service for this, since it would downgrade the calling EVL thread to in-band mode. However, you may create a proxy to the debug file:\n\t#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;evl/proxy.h\u0026gt; int proxyfd, debugfd; debugfd = open(\u0026quot;/tmp/debug.log\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0600); ... /* Create a proxy offering a 1 MB buffer. */ proxyfd = evl_new_proxy(debugfd, 1024*1024, \u0026quot;log:%d\u0026quot;, getpid()); ... Then channel debug output from anywhere in your code you see fit through the proxy this way:\n\t#include \u0026lt;evl/proxy.h\u0026gt; evl_print_proxy(proxyfd, \u0026quot;some useful debug information\u0026quot;); Synchronizing in-band threads on out-of-band events with via a proxy There are a couple of ways which you could use in order to wake up an in-band thread waiting for some event to occur on the out-of-band side of your application, while preventing the waker from being demoted as a result of sending such a signal. One would involve running the in-band thread in the SCHED_WEAK scheduling class, waiting on some EVL synchronization object, such as a semaphore or event flag group. Another one would use a cross-buffer for sending some wakeup datagram from the out-of-band caller to the in-band waiter.\nThe file proxy can also help in implementing such construct, by connecting it to a regular eventfd(2), which can be signaled using the common write(2) system call:\n\t#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/eventfd.h\u0026gt; #include \u0026lt;evl/proxy.h\u0026gt; int evntfd; evntfd = eventfd(0, EFD_SEMAPHORE); ... /* Create a private proxy for output, allow up to 3 notifications to pile up. */ proxyfd = evl_create_proxy(evntfd, sizeof(uint64_t) * 3, sizeof(uint64_t), 0, \u0026quot;event-relay\u0026quot;); ... Note the specific granularity value mentioned in the creation call above: we do want the proxy to write 64-bit values at each transfer, in order to cope with the requirements of the eventfd(2) interface. Once the proxy is created, set up for routing all write requests to the regular event file descriptor by 64-bit chunks, the in-band thread can wait for out-of-band events reading from the other side of the channel as follows:\n\t#include \u0026lt;stdint.h\u0026gt; #include \u0026lt;evl/proxy.h\u0026gt; void oob_waker(int proxyfd) { uint64 val = 1; ssize_t ret; ret = oob_write(proxyfd, \u0026amp;val, sizeof(val)); ... } void inband_sleeper(int evntfd) { ssize_t ret; uint64 val; for (;;) { /* Wait for the next wakeup signal. */ ret = read(evntfd, \u0026amp;val, sizeof(val)); ... } } Export of memory mappings EVL proxies can be also be used for carrying over memory mapping requests to a final device which actually serves the mapping. The issuer of such request does not have to know which device driver will actually be handling that request: the proxy acts as an anchor point agreed between peer processes in order to locate a particular memory-mappable resource, and the proxy simply redirects the mapping requests it receives to the device driver handling the target file it is proxying for.\nExporting process-private memfd memory to the outer world For instance, this usage of a public proxy comes in handy when you need to export a private memory mapping like those obtained by memfd_create(2) to a peer process, assuming you don\u0026rsquo;t want to deal with the hassle of the POSIX shm_open(3) interface for sharing memory objects. In this case, all this peer has to know is the name of the proxy which is associated with the memory-mappable device. It can open(2) that proxy device in /dev/evl/proxy, then issue the mmap(2) system call for receiving a mapping to the backing device\u0026rsquo;s memory. From the application standpoint, creating then sharing a 1 KB RAM segment with other peer processes may be as simple as this:\n\t#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/memfd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;evl/proxy.h\u0026gt; int memfd, ret; void *ptr; memfd = memfd_create(\u0026quot;whatever\u0026quot;, 0); ... /* Set the size of the underlying segment then map it locally. */ ret = ftruncate(memfd, 1024); ptr = mmap(NULL, 1024, PROT_READ|PROT_WRITE, MAP_SHARED, memfd, 0); ... /* Create a public proxy others can find in the /dev/evl/proxy hierarchy. */ proxyfd = evl_new_proxy(memfd, 0, \u0026quot;/some-fancy-proxy\u0026quot;); ... Any remote process peer could then do:\n\t#include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; void *ptr; int fd; /* Open the proxy device relaying mapping requests to memfd(). */ fd = open(\u0026quot;/dev/evl/proxy/some-fancy-proxy\u0026quot;, O_RDWR); ... /* Map the private RAM segment created by our peer. */ ptr = mmap(NULL, 1024, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); ... The other way you could do this is by passing the memfd file descriptor via a socket-based control message, but this would require significantly more logic in both peers than using a proxy.\nProxy services  int evl_create_proxy(int targetfd, size_t bufsz, size_t granularity, int flags, const char *fmt, ...)  This call creates a new proxy to the target file referred to by targetfd, then returns a file descriptor for accessing the proxy upon success. oob_write() should be used to send data through the proxy to the target file. Conversely, oob_read() should be used to receive data through the proxy from the target file.\ntargetfdA file descriptor referring to the target file which should be associated with the proxy.\n\nbufszThe size in bytes of the I/O ring buffer where the relayed data is kept. bufsz must not exceed 2^30. A ring is allocated for each direction enabled in flags (see EVL_CLONE_OUTPUT, EVL_CLONE_INPUT). If granularity is non-zero, bufsz must be a multiple of this value. Out-of-band readers/writers may block on an out-of-band file operation if the buffer is either full on output or empty on input, unless the proxy descriptor is set to non-blocking mode (O_NONBLOCK). Zero is an acceptable value if you plan to use this proxy exclusively for exporting a shared memory mapping, in which case there is no point in reserving I/O ring buffers.\n\ngranularityIn some cases, the target file may have special semantics, which requires a fixed amount of data to be submitted at each read/write operation, like the eventfd(2) file which requires 64-bit words to be read or written from/to it at each transfer. When granularity is less than 2, the proxy is free to read or write as many bytes as possible from/to the target file at each transfer performed by the worker. Conversely, a granularity value greater than 1 is used as the exact count of bytes which may be read from or written to the target file by the in-band worker at each transfer. For instance, in the eventfd(2) use case, we would use sizeof(uint64_t). You may pass zero for a memory mapping proxy since no granularity is applicable in this case.\n\nflagsA set of creation flags for the new element, defining how it should be operated, along with its visibility:\n  EVL_CLONE_OUTPUT enables the output side of the proxy, so that writing to the proxy file descriptor pushes data to targetfd. If busfz is non-zero and flags does not mention EVL_CLONE_INPUT, EVL_CLONE_OUTPUT is assumed (default behavior).\n  EVL_CLONE_INPUT enables the input side of the proxy, so that reading from the proxy file descriptor pulls data from targetfd.\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new proxy in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the proxy name. See this description of the [naming convention] (/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_proxy() returns the file descriptor of the new proxy on success. If the call fails, a negated error code is returned instead:\n  -EEXIST\tThe generated name is conflicting with an existing proxy name.\n  -EINVAL\tEither flags is wrong, bufsz and/or granularity are wrong, or the generated name is badly formed.\n  -EINVAL\tbufsz is zero but flags mentions any of EVL_CLONE_INPUT or EVL_CLONE_OUTPUT.\n  -EBADF\ttargetfd is not a valid file descriptor.\n  -ENAMETOOLONG\tThe overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL proxy.\n    int evl_new_proxy(int targetfd, size_t bufsz, const char *fmt, ...)  This call is a shorthand for creating a private proxy with no particular transfer granularity. It is identical to calling:\n\tevl_create_proxy(targetfd, bufsz, 0, EVL_CLONE_PRIVATE, fmt, ...); If bufsz is non-zero, the output side of the proxy is implicitly enabled. If busfz is zero, then a mapping-only proxy is created.\nNote that if the [generated name] (/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   ssize_t evl_send_proxy(int proxyfd, const void *buf, size_t count)  This is a shorthand checking the current execution stage before sending the output through a proxy channel via the proper system call, i.e. write(2) if in-band, or oob_write() if out-of-band. You can use this routine to emit output from a portion of code which may be used from both stages.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nbufA buffer containing the data to be written.\n\ncountThe number of bytes to write starting from buf. Zero is acceptable and leads to a null-effect.\n\nevl_send_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which corresponds to the errno value received from either write(2) or oob_write() depending on the calling stage.\n  int evl_vprint_proxy(int proxyfd, const char *fmt, ...)  A routine which formats a printf(3)-like input string before sending the resulting output through a proxy channel.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nfmtThe format string.\n\n...The optional variable argument list completing the format.\n\nevl_print_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which may correspond to either a formatting error, or to a sending error in which case the error codes returned by evl_send_proxy() apply.\n  int evl_vprint_proxy(int proxyfd, const char *fmt, va_list ap)  This call is a variant of evl_print_proxy() which accepts format arguments specified as a pointer to a variadic parameter list.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nfmtThe format string.\n\napA pointer to a variadic parameter list.\n\nevl_vprint_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which may correspond to either a formatting error, or to a sending error in which case the error codes returned by evl_send_proxy() apply.\n  int evl_printf(const char *fmt, ...)  A shorthand which sends formatted output through an internal proxy targeting fileno(stdout). This particular proxy is built by the EVL library automatically when it initializes as a result of a direct or indirect call to evl_init().\n Events pollable from a proxy file descriptor The evl_poll() interface can monitor the following events occurring on a proxy file descriptor:\n  POLLOUT and POLLWRNORM are set whenever the output ring buffer some polled file descriptor refers to is empty AND allocated, which means that a proxy initialized with a zero-sized output buffer never raises these events. You would typically monitor the POLLOUT condition in order to wait for all of the buffered output to have been sent to the target file.\n  POLLIN and POLLRDNORM are set whenever some polled file descriptor refers to a proxy for which data is available on input.\n  POLLERR is returned whenever some polled file descriptor refers to a proxy which may not be either read or written (see EVL_CLONE_OUTPUT, EVL_CLONE_INPUT from (#evl_create_proxy)).\n   Last modified: Sun, 15 Nov 2020 18:06:56 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/irq_handling/",
	"title": "IRQ handling",
	"tags": [],
	"description": "",
	"content": "Requesting an out-of-band IRQ Dovetail introduces the new interrupt type flag IRQF_OOB, denoting an out-of-band handler to the generic interrupt API routines:\n setup_irq() for early registration of special interrupts request_irq() for device interrupts __request_percpu_irq() for per-CPU interrupts  An IRQ action handler bearing this flag will run from out-of-band context over the out-of-band stage, regardless of the current interrupt state of the in-band stage. If no out-of-band stage is present, the flag will be ignored, with the interrupt handler running on the in-band stage as usual.\nConversely, out-of-band handlers can be dismissed using the usual calls, such as:\n free_irq() for device interrupts free_percpu_irq() for per-CPU interrupts  Out-of-band IRQ handling has the following constraints:\n If the IRQ is shared, with multiple action handlers registered for the same event, all other handlers on the same interrupt channel must bear the IRQF_OOB flag too, or the request will fail.  If meeting real-time requirements is your goal, sharing an IRQ line among multiple devices operating from different execution stages (in-band vs out-of-band) can only be a bad idea design-wise. You should resort to this in desparate hardware situations only.\n  Obviously, out-of-band handlers cannot be threaded (IRQF_NO_THREAD is implicit, IRQF_ONESHOT is ignored).   Installing an out-of-band handler for a device interrupt\n #include \u0026lt;linux/interrupt.h\u0026gt; static irqreturn_t oob_interrupt_handler(int irq, void *dev_id) { ... return IRQ_HANDLED; } init __init driver_init_routine(void) { int ret; ... ret = request_irq(DEVICE_IRQ, oob_interrupt_handler, IRQF_OOB, \u0026quot;Out-of-band device IRQ\u0026quot;, device_data); if (ret) goto fail; return 0; fail: /* Unwind upon error. */ ... } Telling the companion kernel about entering, leaving the IRQ context Your companion core will most likely want to be notified each time a new interrupt context is entered, typically in order to block any further task rescheduling on its end. Conversely, this core will also want to be notified when such context is exited, so that it can start its rescheduling procedure, applying any change to the scheduler state which occurred during the execution of the interrupt handler(s), such as waking up a thread which was waiting for the incoming event.\nTo provide such support, Dovetail calls irq_enter_pipeline() on entry to the pipeline when it receives an IRQ from the hardware, then irq_exit_pipeline() right before it leaves the interrupt frame. It defines empty placeholders for these hooks as follows, which are picked in absence of a companion core in the kernel tree:\n linux/include/dovetail/irq.h\n /* SPDX-License-Identifier: GPL-2.0 */ #ifndef _DOVETAIL_IRQ_H #define _DOVETAIL_IRQ_H /* Placeholders for pre- and post-IRQ handling. */ static inline void irq_enter_pipeline(void) { } static inline void irq_exit_pipeline(void) { } #endif /* !_DOVETAIL_IRQ_H */ As an illustration, the EVL core overrides the placeholders by interposing the following file which comes earlier in the inclusion order of C headers, providing its own set of hooks as follows:\n linux-evl/include/asm-generic/evl/irq.h\n /* SPDX-License-Identifier: GPL-2.0 */ #ifndef _ASM_GENERIC_EVL_IRQ_H #define _ASM_GENERIC_EVL_IRQ_H #include \u0026lt;evl/irq.h\u0026gt; static inline void irq_enter_pipeline(void) { #ifdef CONFIG_EVL evl_enter_irq(); #endif } static inline void irq_exit_pipeline(void) { #ifdef CONFIG_EVL evl_exit_irq(); #endif } #endif /* !_ASM_GENERIC_EVL_IRQ_H */ Eventually, the EVL core implements the evl_enter_irq() and evl_exit_irq() routines in a final support header like this:\n linux-evl/include/evl/irq.h\n /* * SPDX-License-Identifier: GPL-2.0 * * Copyright (C) 2017 Philippe Gerum \u0026lt;rpm@xenomai.org\u0026gt; */ #ifndef _EVL_IRQ_H #define _EVL_IRQ_H #include \u0026lt;evl/sched.h\u0026gt; /* hard irqs off. */ static inline void evl_enter_irq(void) { struct evl_rq *rq = this_evl_rq(); rq-\u0026gt;local_flags |= RQ_IRQ; } /* hard irqs off. */ static inline void evl_exit_irq(void) { struct evl_rq *this_rq = this_evl_rq(); this_rq-\u0026gt;local_flags \u0026amp;= ~RQ_IRQ; /* * CAUTION: Switching stages as a result of rescheduling may * re-enable irqs, shut them off before returning if so. */ if ((this_rq-\u0026gt;flags|this_rq-\u0026gt;local_flags) \u0026amp; RQ_SCHED) { evl_schedule(); if (!hard_irqs_disabled()) hard_local_irq_disable(); } } #endif /* !_EVL_IRQ_H */  Last modified: Tue, 26 Jun 2018 19:27:55 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/poll/",
	"title": "Polling file descriptors",
	"tags": [],
	"description": "",
	"content": "Waiting for events on file descriptors Every EVL element is represented by a device file accessible in the /dev/evl file hierarchy, and requests can be sent to any of those elements by writing to common file descriptors obtained on the corresponding files. Conversely, information is returned to the caller by reading from those file descriptors. EVL applications can poll for out-of-band events occurring on a set of file descriptors opened on elements, just like in-band applications can use poll(2) with other files.\nTo access the out-of-band polling services, an application first needs to obtain a file descriptor on a new polling set by a call to evl_poll(). Such descriptor is an access point for registering other file descriptors to monitor, then sensing events occurring on them. evl_poll() or evl_timedpoll() fill an array of structures indicating which events have been received on each notified file descriptor. The type of this structure is as follows:\nstruct evl_poll_event { int32_t fd; int32_t events; }; Applications can wait for any event which poll(2) can monitor such as POLLIN, POLLOUT, POLLRDNORM, POLLWRNORM and so on. Which event can occur on a given file descriptor is defined by the out-of-band driver managing the file it refers to.\n Pollable elements   Semaphore   Event flag group   Timer   Proxy   Cross-buffer   Polling services  int evl_new_poll(void)  This call creates a polling set, returning a file descriptor representing the new object upon success. There is no arbitrary limit on the number of polling sets an application can create, which is only limited to the available system resources.\nevl_new_poll() returns the file descriptor of the newly created polling set on success. Otherwise, a negated error code is returned:\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO\tThe EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL polling set.\n    int evl_add_pollfd(int efd, int newfd, unsigned int events)  Adds a file descriptor to a polling set. This service registers the newfd file descriptor for being polled for events by the efd polling set.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\nnewfdA file descriptor to monitor for events. Once newfd is added to the polling set, it becomes a source of events which is monitored when reading from efd. You can add as many file descriptor as you need to a polling set in order to perform synchronous I/O multiplexing over multiple data sources or sinks.\n\neventsA bitmask representing the set of events to monitor for newfd. These events are common to the in-band poll(2) interface. POLLERR and POLLHUP are always implicitly polled, even if they are not mentioned in events. POLLNVAL cannot be awaited for. Which event is meaningful in the context of the call depends on the driver managing the file, which defines the logic for raising them. For instance, if newfd refers to an EVL semaphore, you could monitor POLLIN and POLLRDNORM, no other event would occur for this type of file.\n\nevl_add_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EBADF\tnewfd is not a valid file descriptor to an EVL file. Note that you cannot register a file descriptor referring to a common file (i.e. managed by an in-band driver) into an EVL polling set. An EVL file is created by [out-of-band drivers] (/core/user-api/io/) for referring to resources which can be accessed from the [out-of-band execution] (/dovetail/altsched/) stage.\n  -ELOOP\tAdding newfd to the polling set would lead to a cyclic dependency (such as newfd referring to the polling set).\n  -ENOMEM\tNo memory available.\n    int evl_del_pollfd(int efd, int delfd)  Removes a file descriptor from a polling set. As a result, reading this polling set does not monitor delfd anymore. This change applies to subsequent calls to evl_poll() or evl_timedpoll() for such set; ongoing waits are not affected.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\ndelfdThe file descriptor to stop monitoring for events. This descriptor should have been registered by an earlier call to evl_add_pollfd().\n\nevl_del_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -ENOENT\tdelfd is not a file descriptor present into the polling set referred to by efd.\n    int evl_mod_pollfd(int efd, int modfd, unsigned int events)  Modifies the events being monitored for a file descriptor already registered in a polling set. This change applies to subsequent calls to evl_poll() or evl_timedpoll() for such set; ongoing waits are not affected.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\nmodfdThe file descriptor to update the event mask for. This descriptor should have been registered by an earlier call to evl_add_pollfd().\n\nevl_mod_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -ENOENT\tpollfd is not a file descriptor present into the polling set referred to by efd.\n    int evl_poll(int efd, struct evl_poll_event *pollset, int nrset)  Waits for events on the file descriptors present in the polling set referred to by efd.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\npollsetThe start of an array of structures of type evl_poll_event containing the set of file descriptors which have received at least one event when the call returns.\n\nnrsetThe number of structures in the pollset array. If zero, evl_poll() returns immediately with a zero count.\n\nOn success, evl_poll() returns the count of file descriptors from the polling set for which an event is pending, each of them will appear in a evl_poll_event entry in pollset. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EINVAL\tnrset is negative.\n  -EAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and no file descriptor from the polling set has any event pending.\n  -EFAULT\tpollset points to invalid memory.\n    int evl_timedpoll(int efd, struct evl_poll_event *pollset, int nrset, struct timespec *timeout)  This call is a variant of evl_poll() which allows specifying a timeout on the polling operation, so that the caller is unblocked after a specified delay sleeping without receiving any event.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\npollsetThe start of an array of structures of type evl_poll_event containing the set of file descriptors which have received at least one event when the call returns.\n\nnrsetThe number of structures in the pollset array. If zero, evl_poll() returns immediately with a zero count.\n\ntimeoutA time limit to wait for any event to be notified before the call returns on error. Timeouts are always based on the built-in EVL_CLOCK_MONOTONIC clock. If both the tv_sec and tv_nsec members of timeout are set to zero, evl_timedpoll() behaves identically to evl_poll().\n\nOn success, evl_timedpoll() returns the count of file descriptors from the polling set for which an event is pending, each of them will appear in a evl_poll_event entry in pollset. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EINVAL\tnrset is negative.\n  -EFAULT\tpollset or timeout point to invalid memory.\n  -EINVAL\tany of tv_sec or tv_nsec in timeout is invalid.\n  -ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n   Last modified: Sun, 03 May 2020 11:13:49 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/synthetic/",
	"title": "Synthetic IRQs",
	"tags": [],
	"description": "",
	"content": "The pipeline introduces an additional type of interrupts, which are purely software-originated, with no hardware involvement. These IRQs can be triggered by any kernel code. Synthetic IRQs are inherently per-CPU events. Because the common pipeline flow applies to synthetic interrupts, it is possible to attach them to out-of-band and/or in-band handlers, just like device interrupts.\nSynthetic interrupts abide by the normal rules with respect to interrupt masking: such IRQs may be deferred until the stage they should be handled from is unstalled.\nSynthetic interrupts and softirqs differ in essence: the latter only exist in the in-band context, and therefore cannot trigger out-of-band activities. Synthetic interrupts used to be called virtual IRQs (or virq for short) by the legacy I-pipe implementation, Dovetail\u0026rsquo;s ancestor; such rename clears the confusion with the way abstract interrupt numbers defined within interrupt domains may be called elsewhere in the kernel code base (i.e. virtual interrupts too).\n Allocating synthetic interrupts Synthetic interrupt vectors are allocated from the synthetic_irq_domain, using the irq_create_direct_mapping() routine.\nA synthetic interrupt handler can be installed for running on the in-band stage upon a scheduling request (i.e. being posted) from an out-of-band context as follows:\n#include \u0026lt;linux/irq_pipeline.h\u0026gt; static irqreturn_t sirq_handler(int sirq, void *dev_id) { do_in_band_work(); return IRQ_HANDLED; } static struct irqaction sirq_action = { .handler = sirq_handler, .name = \u0026#34;In-band synthetic interrupt\u0026#34;, .flags = IRQF_NO_THREAD, }; unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; setup_percpu_irq(sirq, \u0026amp;sirq_action); return sirq; } A synthetic interrupt handler can be installed for running from the oob stage upon a trigger from an in-band context as follows:\nstatic irqreturn_t sirq_oob_handler(int sirq, void *dev_id) { do_out_of_band_work(); return IRQ_HANDLED; } unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; ret = __request_percpu_irq(sirq, sirq_oob_handler, IRQF_OOB, \u0026#34;Out-of-band synthetic interrupt\u0026#34;, dev_id); if (ret) { irq_dispose_mapping(sirq); return 0; } return sirq; } Scheduling in-band execution of a synthetic interrupt handler The execution of sirq_handler() in the in-band context can be scheduled (or posted) from the out-of-band context in two different ways:\nUsing the common injection service irq_pipeline_inject(sirq); Using the lightweight injection method (requires interrupts to be disabled in the CPU) unsigned long flags = hard_local_irqsave(); irq_post_inband(sirq); hard_local_irqrestore(flags);  Assuming that no interrupt may be pending in the event log for the oob stage at the time this code runs, the second method relies on the invariant that in a pipeline interrupt model, IRQs pending for the in-band stage will have to wait for the oob stage to quiesce before they can be handled. Therefore, it is pointless to check for synchronizing the interrupts pending for the in-band stage from the oob stage, which the irq_pipeline_inject() service would do systematically. irq_post_inband() simply marks the event as pending in the event log of the in-band stage for the current CPU, then returns. This event would be played as a result of synchronizing the log automatically when the current CPU switches back to the in-band stage.\n It is also valid to post a synthetic interrupt to be handled on the in-band stage from an in-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the in-band stage: the IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nTriggering out-of-band execution of a synthetic interrupt handler Conversely, the execution of sirq_handler() on the oob stage can be triggered from the in-band context as follows:\nirq_pipeline_inject(sirq); Since the oob stage has precedence over the in-band stage for execution of any pending event, this IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nIt is also valid to post a synthetic interrupt to be handled on the oob stage from an out-of-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the oob stage.\nCalling irq_post_oob(sirq) from the in-band stage to trigger an out-of-band event is most often not the right way to do this, because this service would not synchronize the interrupt log before returning. In other words, the sirq event would still be pending for the oob stage despite the fact that it should have preempted the in-band stage before returning to the caller.\n  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/pipeline_inject/",
	"title": "IRQ injection",
	"tags": [],
	"description": "",
	"content": "Sending out-of-band IPIs to remote CPUs Although the pipeline does not directly use IPIs internally, it exposes two generic IPI vectors which autonomous cores may use in SMP configuration for signaling the following events across CPUs:\n  RESCHEDULE_OOB_IPI, the cross-CPU task reschedule request. This is available to the core\u0026rsquo;s scheduler for kicking the task rescheduling procedure on remote CPUs, when the state of their respective runqueue has changed. For instance, a task sleeping on CPU #1 may be unblocked by a system call issued from CPU #0: in this case, the scheduler code running on CPU #0 is supposed to tell CPU #1 that it should reschedule. Typically, the EVL core does so from its test_resched() routine.\n  TIMER_OOB_IPI, the cross-CPU timer reschedule request. Because software timers are in essence per-CPU beasts, this IPI is available to the core\u0026rsquo;s timer management code for kicking the hardware timer programming procedure on remote CPUs, when the state of some software timer has changed. Typically, stopping a timer from a remote CPU, or migrating a timer from a CPU to another should trigger such signal. The EVL core does so from its evl_program_remote_tick() routine, which is called whenever the timer with the earliest timeout date enqueued on a remote CPU, may have changed.\n  As their respective name suggests, those two IPIs can be sent from out-of-band context (as well as in-band), by calling the irq_send_oob_ipi() service.\n  void irq_send_oob_ipi(unsigned int ipi, const struct cpumask *cpumask)  ipiThe IPI number to send. There are only two legit values for this argument: either RESCHEDULE_OOB_IPI, or TIMER_OOB_IPI. This is a low-level service with not much parameter checking, so any other value is likely to cause havoc.\n\ncpumaskA CPU bitmask defining the target CPUs for the signal. The current CPU is allowed to send a signal to itself, although this may not be the faster path to running a local handler.\n\nIn order to receive these IPIs, an out-of-band handler must have been set for them, mentioning the [IRQF_OOB flag]({{ \u0026lt; relref \u0026ldquo;dovetail/pipeline/irq_handling.md\u0026rdquo; \u0026gt;}}).\nirq_send_oob_ipi() serializes callers internally so that it may be used from either stages: in-band or out-of-band.\n Injecting an IRQ event for the current CPU In some very specific cases, we may need to inject an IRQ into the pipeline by software as if such hardware event had happened on the current CPU. irq_inject_pipeline() does exactly this.\n  int irq_inject_pipeline(unsigned int irq)  irqThe IRQ number to inject. A valid interrupt descriptor must exist for this interrupt.\n\nirq_inject_pipeline() fully emulates the receipt of a hardware event, which means that the common interrupt pipelining logic applies to the new event:\n  first, any out-of-band handler is considered for delivery,\n  then such event may be passed down the pipeline to the common in-band handler(s) in absence of out-of-band handler(s).\n  The pipeline priority rules apply accordingly:\n  if the caller is in-band, and an out-of-band handler is registered for the IRQ event, and the out-of-band stage is unstalled, the execution stage is immediately switched to out-of-band for running the later, then restored to in-band before irq_inject_pipeline() returns.\n  if the caller is out-of-band and there is no out-of-band handler, the IRQ event is deferred until the in-band stage resumes execution on the current CPU, at which point it is delivered to any in-band handler(s).\n  in any case, should the current stage receive the IRQ event, the virtual interrupt state of that stage is always considered before deciding whether this event should be delivered immediately to its handler by irq_inject_pipeline() (unstalled case), or deferred until the stage is unstalled (stalled case).\n  This call returns zero on successful injection, or -EINVAL if the IRQ has no valid descriptor.\nIf you look for a way to schedule the execution of a routine in the in-band interrupt context from the out-of-band stage, you may want to consider the extended irq_work API which provides a high level interface to this feature.\n  Direct logging of an IRQ event Sometimes, running the full interrupt delivery logic irq_inject_pipeline() implements for feeding an interrupt into the pipeline may be overkill when we may make assumptions about the current execution context, and which stage should handle the event. The following fast helpers can be used instead in this case:\n  void irq_post_inband(unsigned int irq)  irqThe IRQ number to inject into the in-band stage. A valid interrupt descriptor must exist for this interrupt.\n\nThis routine may be used to mark an interrupt as pending directly into the current CPU\u0026rsquo;s log for the in-band stage. This is useful in either of these cases:\n  you know that the out-of-band stage is current, therefore this event has to be deferred until the in-band stage resumes on the current CPU later on. This means that you can simply post it to the in-band stage directly.\n  you know that the in-band stage is current but stalled, therefore this event can\u0026rsquo;t be immediately delivered, so marking it as pending into the in-band stage is enough.\n  Interrupts must be hard disabled in the CPU before calling this routine.\n  void irq_post_oob(unsigned int irq)  irqThe IRQ number to inject into the out-of-band stage. A valid interrupt descriptor must exist for this interrupt.\n\nThis routine may be used to mark an interrupt as pending directly into the current CPU\u0026rsquo;s log for the out-of-band stage. This is useful in only one situation: you know that the out-of-band stage is current but stalled, therefore this event can\u0026rsquo;t be immediately delivered, so marking it as pending into the out-of-band stage is enough.\nInterrupts must be hard disabled in the CPU before calling this routine. If the out-of-band stage is stalled as expected on entry to this helper, then interrupts must be hard disabled in the CPU as well anyway.\n Extended IRQ work API Due to the NMI-like nature of interrupts running out-of-band code from the standpoint of the main kernel, such code might preempt in-band activities in the middle of a critical section. For this reason, it would be unsafe to call any in-band routine from an out-of-band context.\nHowever, we may schedule execution of in-band work handlers from out-of-band code, using the regular irq_work_queue() service which has been extended by the IRQ pipeline core. Such work request from the out-of-band stage is scheduled for running on the in-band stage on the issuing CPU as soon as the out-of-band activity quiesces on this processor. As its name implies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline forces the use of a synthetic IRQ as a notification signal for the IRQ work machinery, instead of a hardware-specific interrupt vector. This special IRQ is labeled in-band work when reported by /proc/interrupts. irq_work_queue() may invoke the work handler immediately only if called from the in-band stage with hard irqs on. In all other cases, the handler execution is deferred until the in-band log is synchronized.\n  Last modified: Sun, 10 Jan 2021 12:43:10 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/interrupt_protection/",
	"title": "Interrupt protection",
	"tags": [],
	"description": "",
	"content": "Disabling interrupts in the CPU The local_irq_save() and local_irq_disable() helpers are no more disabling interrupts in the CPU when interrupt pipelining is enabled, but only disable interrupt events virtually for the in-band stage.\nA set of helpers is provided for manipulating the interrupt disable flag in the CPU instead. When CONFIG_IRQ_PIPELINE is disabled, this set maps 1:1 over the regular local_irq_*() API.\n   Original/Virtual Non-virtualized call     local_save_flags(flags) flags = hard_local_save_flags()   local_irq_disable() hard_local_irq_disable()   local_irq_enable() hard_local_irq_enable()   local_irq_save(flags) flags = hard_local_irq_save()   local_irq_restore(flags) hard_local_irq_restore(flags)   irqs_disabled() hard_irqs_disabled()   irqs_disabled_flags(flags) hard_irqs_disabled_flags(flags)    Stalling the out-of-band stage Just like the in-band stage is affected by the state of the virtual interrupt disable flag, the interrupt state of the oob stage is controlled by a dedicated stall bit flag in the oob stage\u0026rsquo;s status. In combination with the interrupt disable bit in the CPU, this software bit controls interrupt delivery to the oob stage.\nWhen this stall bit is set, interrupts which might be pending in the oob stage\u0026rsquo;s event log of the current CPU are not played. Conversely, the out-of-band handlers attached to pending IRQs are fired when the stall bit is clear. The following table represents the equivalent calls affecting the stall bit for each stage:\n   In-band stage operation OOB stage operation     local_save_flags(flags) -none-   local_irq_disable() oob_irq_disable()   local_irq_enable() oob_irq_enable()   local_irq_save(flags) flags = oob_irq_save()   local_irq_restore(flags) oob_irq_restore(flags)   irqs_disabled() oob_irqs_disabled()   irqs_disabled_flags(flags) -none-     Last modified: Sun, 08 Mar 2020 13:06:41 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/interrupts/",
	"title": "Managing IRQs",
	"tags": [],
	"description": "",
	"content": "EVL has no specific API for managing out-of-band interrupts. You can use the regular kernel API as updated by Dovetail for this purpose.\n Last modified: Tue, 26 Jun 2018 19:27:55 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/io/",
	"title": "Out-of-band I/O services",
	"tags": [],
	"description": "",
	"content": "Talking to real-time capable device drivers Using the EVL kernel API, you can extend an existing driver for supporting out-of-band I/O operations, or even write one from scratch. Both character-based I/O and socket protocol drivers are supported.\nOn the user side, application can exchange data with, send requests to these real-time capable drivers from the out-of-band execution stage with the a couple of additional services libevl provides.\nYou may notice that several POSIX file I/O services such as open(2), socket(2), close(2), fcntl(2), mmap(2) and so on have no out-of-band counterpart in the following list. The reason is that we don\u0026rsquo;t need them: opening, closing or mapping a file are inherently non-deterministic operations, which may block for an unspecified amount of time for a number of reasons, depending on the underlying file and current runtime conditions. Besides, those are hardly useful in a time-critical loop.\nHowever, issuing data transfers and control requests to the driver is definitely something we may want to happen within a bounded time, hence directly from the out-of-band execution stage.\nSince the EVL core exports every public element as a character device which can be accessed from /dev/evl, libevl can interface with elements from other processes through the out-of-band I/O requests documented here, which are sent to the corresponding devices.\n Opening an out-of-band capable I/O channel Since the EVL core does not redefine the open(2) and socket(2) calls, there has to be a way to tell the kernel code managing the device and/or protocol that we want to enable out-of-band operations.\nIn most cases, we don\u0026rsquo;t have to do so though, because the purpose of the corresponding device driver is all about providing out-of-band services, so enabling them for any connecting file is implicit. For instance, most of the drivers accessed through the /dev/evl file hierarchy turn on out-of-band services automatically.\nHowever, some drivers might distinguish between out-of-band capable files and others, providing a different set of services. Typically, a regular in-band driver which is extended in order to handle out-of-band requests too should be told when to do so for any given file.\nTo meet this requirement, Dovetail introduces the additional open flag O_OOB, which can be passed to open(2) ORed into the flags argument. Similarly, it defines the SOCK_OOB flag which can be passed to socket(2) ORed into the type argument for the same purpose. If the receiving driver implements opt-in out-of-band services, passing this flag when opening a file/socket should enable them.\nNot all devices drivers may support out-of-band operations (the overwhelming majority does not). Whether passing either O_OOB or SOCK_OOB to them when opening a file/socket would cause an error, or the flag would just be ignored depends on the driver code.\n Out-of-band I/O services  ssize_t oob_read(int efd, void *buf, size_t count)  This is the strict equivalent to the standard read(2) system call, for sending the request from the out-of-band stage to an EVL driver. In other words, oob_read() attempts to read up to count bytes from file descriptor fd into the buffer starting at buf, from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to read from.\n\nbufA buffer to receive the data.\n\ncountThe number of bytes to read at most, which should fit into buf.\n\noob_read() returns the actual number of bytes read, copied to buf on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver, or fd was not opened for reading.\nEINVAL if fd does not support the .oob_read operation.\nEFAULT\tif buf points to invalid memory.\nEAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and the read operation would block.\nOther driver-specific error codes may be returned, such as:\nENOBUFS fd is a cross-buffer file descriptor, and there is no ring buffer space associated with the outbound traffic (i.e. o_bufsz parameter was zero when creating the cross-buffer).\nEINVAL fd is a cross-buffer file descriptor, and count is greater than the size of the ring buffer associated with the traffic direction. (i.e. either the i_bufsz or o_bufsz parameter given when creating the cross-buffer).\nENXIO\tfd is a proxy file descriptor which is not available for input. See EVL_CLONE_INPUT.\n  ssize_t oob_write(int efd, const void *buf, size_t count)  This is the strict equivalent to the standard write(2) system call, for sending the request from the out-of-band stage to an EVL driver. In other words, oob_write() attempts to write up to count bytes to file descriptor fd from the buffer starting at buf, from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to read from.\n\nbufA buffer containing the data to be written.\n\ncountThe number of bytes to write starting from buf.\n\noob_write() returns the actual number of bytes written from buf on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver, or fd was not opened for writing.\nEINVAL if fd does not support the .oob_write operation.\nEFAULT\tif buf points to invalid memory.\nEAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and the write operation would block.\nOther driver-specific error codes may be returned, such as:\nEFBIG\tfd is a proxy file descriptor, and count is larger than the size of the output buffer as specified in the call to [evl_create_proxy()] (/core/user-api/proxy/#evl_create_proxy).\nEINVAL\tfd is a proxy file descriptor, and count is not a multiple of the output granularity as specified in the call to [evl_create_proxy()] (/core/user-api/proxy/#evl_create_proxy).\nENOBUFS fd is a cross-buffer file descriptor, and there is no ring buffer space associated with the inbound traffic (i.e. i_bufsz parameter was zero when creating the cross-buffer).\nEINVAL fd is a cross-buffer file descriptor, and count is greater than the size of the ring buffer associated with the traffic direction. (i.e. either the i_bufsz or o_bufsz parameter given when creating the cross-buffer).\nENXIO\tfd is a proxy file descriptor which is not available for output. See EVL_CLONE_OUTPUT.\n  int oob_ioctl(int efd, unsigned long request, ...)  This is the strict equivalent to the standard ioctl(2) system call, for sending the I/O control request from the out-of-band stage to an EVL driver. In other words, oob_ioctl() issues request to file descriptor fd from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to send a request to.\n\nrequestThe I/O control request code.\n\n...An optional variable argument list which applies to request.\n\noob_ioctl() returns zero on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver.\nENOTTY if fd does not support the .oob_ioctl operation, or the driver does not implement request.\nEFAULT\tif buf points to invalid memory.\nEAGAIN fd is marked as non-blocking (O_NONBLOCK), and the control request would block.\nOther driver-specific error codes may be returned.\n  ssize_t oob_recvmsg(int s, struct oob_msghdr *msghdr, const struct timespec *timeout, int flags)  This is an equivalent to the standard recvmsg(2) system call, for sending the request from the out-of-band stage to an EVL driver with a socket-based interface. In other words, oob_recvmsg() is used to receive messages from an out-of-band capable EVL socket from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nsA socket descriptor obtained from a regular socket(2) call, with the SOCK_OOB flag set in the type argument, denoting that out-of-band services are enabled for the socket.\n\nmsghdrA pointer to a structure containing the multiple arguments to this call, which is described below.\n\ntimeoutA time limit to wait for a message before the call returns on error. The built-in clock EVL_CLOCK_MONOTONIC is used for tracking the elapsed time. If NULL is passed, the call is allowed to wait indefinitely for a message.\n\nflagsA set of flags further qualifying the operation. Only the following flags should be recognized for out-of-band requests:\n  MSG_DONTWAIT causes the call to fail with the error EAGAIN if no message is immediately available at the time of the call. MSG_DONTWAIT is implied if O_NONBLOCK was set for the socket descriptor via the fcntl(2) F_SETFL operation.\n  MSG_PEEK causes the receive operation to return data from the beginning of the receive queue without removing that data from the queue. Thus, a subsequent receive call will return the same data.\n  \noob_recvmsg() returns the actual number of bytes received on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF if s does not refer to a valid socket opened with the SOCK_OOB type flag set, or s was not opened for reading.\nEINVAL if s does not support the .oob_ioctl operation.\nEFAULT\tif msghdr, or any buffer it refers to indirectly points to invalid memory.\nEAGAIN\ts is marked as non-blocking (O_NONBLOCK), or MSG_DONTWAIT is set in flags, and the receive operation would block.\nETIMEDOUT the timeout fired before the operation could complete successfully.\n  ssize_t oob_sendmsg(int s, const struct oob_msghdr *msghdr, const struct timespec *timeout, int flags)  This call is equivalent to the standard sendmsg(2) system call, for sending the request from the out-of-band stage to an EVL driver with a socket-based interface. In other words, oob_sendmsg() is used to send messages to an out-of-band capable EVL socket from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nsA socket descriptor obtained from a regular socket(2) call, with the SOCK_OOB flag set in the type argument, denoting that out-of-band services are enabled for the socket.\n\nmsghdrA pointer to a structure containing the multiple arguments to this call, which is described below.\n\ntimeoutA time limit to wait for an internal buffer to be available for sending the message before the call returns on error. The built-in clock EVL_CLOCK_MONOTONIC is used for tracking the elapsed time. If NULL is passed, the call is allowed to wait indefinitely for a buffer.\n\nflagsA set of flags further qualifying the operation. Only the following flag should be recognized for out-of-band requests:\n MSG_DONTWAIT causes the call to fail with the error EAGAIN if no buffer is immediately available at the time of the call for sending the message. MSG_DONTWAIT is implied if O_NONBLOCK was set for the socket descriptor via the fcntl(2) F_SETFL operation.  \noob_sendmsg() returns the actual number of bytes sent on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif s does not refer to a valid socket opened with the SOCK_OOB type flag set, or s was not opened for writing.\nEINVAL if s does not support the .oob_ioctl operation.\nEFAULT\tif msghdr, or any buffer it refers to indirectly points to invalid memory.\nEAGAIN\ts is marked as non-blocking (O_NONBLOCK), or MSG_DONTWAIT is set in flags, and the send operation would block.\nETIMEDOUT the timeout fired before the operation could complete successfully.\n\u0026ndash;\nThe out-of-band message header The structure oob_msghdr which is passed to the oob_recvmsg() and oob_sendmsg() calls is defined as follows:\n struct oob_msghdr { void *msg_name; /* Optional address */ socklen_t msg_namelen; /* Size of address */ struct iovec *msg_iov; /* Scatter/gather array */ size_t msg_iovlen; /* # elements in msg_iov */ void *msg_control; /* Ancillary data, see below */ size_t msg_controllen; /* Ancillary data buffer len */ int msg_flags; /* Flags on received message */ struct timespec msg_time; /* Optional time, see below */ }; struct iovec { /* Scatter/gather array items */ void *iov_base; /* Starting address */ size_t iov_len; /* Number of bytes to transfer */ };  The msg_name field points to a caller-allocated buffer that is used to return the source address if the socket is unconnected. The caller should set msg_namelen to the size of this buffer before this call. On success, oob_recvmsg() updates msg_namelen to contain the length of the returned address. If the application does not need to know the source address, msg_name can be specified as NULL.\nThe fields msg_iov and msg_iovlen describe scatter-gather locations pointing at the message data being sent or received, as discussed in readv(2).\nThe field msg_control points to a buffer for other protocol control-related messages or miscellaneous ancillary data. When either oob_recvmsg() or oob_sendmsg() is called, msg_controllen should contain the length of the available buffer in msg_control. On success, oob_recvmsg() updates msg_controllen to contain the actual length of the control message sequence returned by the call.\nThe msg_flags field is only set on return of oob_recvmsg(). It can contain any of the flags which may be returned by recvmsg(2).\nmsg_time may be used to send or receive timestamping information to/from the protocol driver implementing out-of-band operations.\nProtocol drivers should no attach any meaning to MSG_OOB when operating in out-of-band mode, so that no additional confusion arises with the common usage of this flag with recvmsg(2).\n  Last modified: Sun, 22 Aug 2021 11:37:58 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/kernel-api/stax/",
	"title": "Stage exclusion lock",
	"tags": [],
	"description": "",
	"content": "The STAge eXclusion lock (aka stax) serializes in-band vs out-of-band thread activities for accessing an arbitrary resource. Such lock can be nested so that multiple threads which run on the same execution stage may \u0026lsquo;own\u0026rsquo; the stax guarding the resource, excluding any access from the converse stage until the last thread drops the innermost lock. In other words, at any point in time, the resource guarded by a stax is either owned by out-of-band threads exclusively, or by in-band threads exclusively, or by no thread at all. A stax is definitely not designed for guaranteeing bounded low latency to out-of-band threads: if the application allows in-band threads to compete for the stax, the out-of-band work serialized on the same object may be delayed at some point by definition.\nNevertheless, since we may assume that there will be no stage concurrency when accessing the resource guarded by a stax, as a consequence we may rely on stage-specific serializers such as mutexes (either regular in-band ones or EVL) to enforce mutual exclusion among them while they own the stax, without having to care further about threads running on the converse stage. This solves a tricky issue about sharing the implementation of a common driver between in-band and out-of-band users in a safe way.\n  int evl_init_stax(struct evl_stax *stax)    int evl_destroy_stax(struct evl_stax *stax)    int evl_lock_stax(struct evl_stax *stax)    int evl_trylock_stax(struct evl_stax *stax)    void evl_unlock_stax(struct evl_stax *stax)   Last modified: Sun, 19 Jan 2020 18:54:09 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/scheduling/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "The out-of-band scheduler EVL defines five scheduling policies for running out-of-band threads. These policies are hierarchized: every time the core needs to pick the next eligible thread to run on the current CPU, it queries each policy module for a runnable thread in the following order:\n  SCHED_FIFO, which is the common first-in, first-out real-time policy, also dealing with the SCHED_RR round-robin policy internally.\n  SCHED_TP, which enforces temporal partitioning of multiple sets of threads in a way which prevents those sets from overlapping time-wise on the CPU which runs such policy.\n  SCHED_QUOTA, which enforces a limitation on the CPU consumption of threads over a fixed period of time.\n  SCHED_WEAK, which is a non real-time policy allowing its members to run in-band most of the time, while retaining the ability to request EVL services.\n  SCHED_IDLE, which is the fallback option the EVL core considers only when other policies have no runnable task on the CPU.\n  The SCHED_QUOTA and SCHED_TP policies are optionally supported by the core, make sure to enable CONFIG_EVL_SCHED_QUOTA or CONFIG_EVL_SCHED_TP respectively in the kernel configuration if you need them.\nOnly SCHED_FIFO, SCHED_QUOTA and SCHED_TP are real-time scheduling policies.\nBefore a thread can be assigned to any EVL class, its has to attach itself to the core by a call to evl_attach_thread.\n Scheduler services  int evl_set_schedattr(int efd, const struct evl_sched_attrs *attrs)  This call changes the scheduling attributes for the thread referred to by efd in the EVL core.\nefdA file descriptor referring to the target thread, as returned by evl_attach_thread(), evl_get_self(), or opening a thread element device in /dev/evl/thread using open(2).\n\nattrsA structure defining the new set of attributes, which depends on the scheduling policy mentioned in attrs-\u0026gt;sched_policy. EVL currently implement the following policies:\n\n  SCHED_FIFO, which is the common first-in, first-out real-time policy.\n  SCHED_RR, defining a real-time, round-robin policy in which each member of the class is allotted an individual time quantum before the CPU is given to the next thread.\n  SCHED_TP, which enforces temporal partitioning of multiple sets of threads based on a cycle-based scheduling.\n  SCHED_QUOTA, which enforces a limitation on the CPU consumption of threads over a fixed period of time, known as the global quota period. Threads undergoing this policy are pooled in groups, with each group being given a share of the period.\n  SCHED_WEAK, which is a non real-time policy allowing its members to run in-band most of the time, while retaining the ability to request EVL services, at the expense of briefly switching to the out-of-band execution stage on demand.\n  evl_set_schedattr() returns zero on success, otherwise a negated error code is returned:\n-EBADF\tefd is not a valid thread descriptor.\n-EINVAL\tSome of the parameters in attrs are wrong. Check attrs-\u0026gt;sched_policy, and the policy-specific information may EVL expect for more.\n-ESTALE\tefd refers to a stale thread, see these notes.\nevl_set_schedattr() immediately applies the changes to the scheduling attributes of the target thread when the latter runs in out-of-band context. Later on, the next time such thread transitions from out-of-band to in-band context, the in-band kernel will apply an extrapolated version of those changes to its own scheduler as well.\nThe extrapolation of the out-of-band scheduling attributes passed to evl_set_schedattr() to the in-band ones applied by the mainline kernel works as follows:\n   out-of-band policy in-band policy     SCHED_FIFO, prio SCHED_FIFO, prio   SCHED_RR, prio SCHED_FIFO, prio   SCHED_TP, prio SCHED_FIFO, prio   SCHED_QUOTA, prio SCHED_FIFO, prio   SCHED_WEAK, prio \u0026gt; 0 SCHED_FIFO, prio   SCHED_WEAK, prio == 0 SCHED_OTHER    Calling pthread_setschedparam(3) from the C library does not affect the scheduling attributes of an EVL thread. It only affects the scheduling parameters of such thread from the standpoint of the in-band kernel. Because the C library may cache the current scheduling attributes for the in-band context of a thread - glibc does so typically - the cached value may not reflect the actual scheduling attributes of the thread after this call.\n   int evl_get_schedattr(int efd, struct evl_sched_attrs *attrs)  This is the call for retrieving the current scheduling attributes of the thread referred to by efd.\nefdA file descriptor referring to the target thread, as returned by evl_attach_thread(), evl_get_self(), or opening a thread element device in /dev/evl/thread using open(2).\n\nattrsA pointer to a structure where the EVL core should write back the scheduling attributes.\n\n#include \u0026lt;evl/sched.h\u0026gt; #include \u0026lt;evl/thread.h\u0026gt; int retrieve_self_schedparams(void) { struct evl_sched_attrs attrs; return evl_get_schedattr(evl_get_self(), \u0026amp;attrs); } The value returned in attrs.sched_priority is the base priority level of the thread within its scheduling class, which does not reflect any priority inheritance/ceiling boost that might be ongoing.\nevl_get_schedattr() returns zero on success, otherwise a negated error code is returned:\n-EBADF\tefd is not a valid thread descriptor.\n-ESTALE\tefd refers to a stale thread, see these notes.\n  int evl_control_sched(int policy, const union evl_sched_ctlparam *param, union evl_sched_ctlinfo *info, int cpu)  Some policies require specific configuration for each CPU, evl_control_sched() passes such information to the core. Which parameters should be filled into the param union depends on the policy. For instance, you would need to call this routine for defining the SCHED_TP schedule on a given CPU.\npolicyEither SCHED_QUOTA or SCHED_TP.\n\nparamA pointer to the scheduling parameters which should be applied to cpu for the policy mentioned. These parameters are described in the SCHED_QUOTA and SCHED_TP documentation.\n\ninfoA pointer to a control information block where the core may write some useful data about the current settings. Except for the get operations, this pointer is optional and may be passed as NULL.\n\ncpuA CPU which belongs to the set of CPUs EVL runs out-of-band activity on (see the evl.oobcpus kernel parameter).\n\nThis call returns zero on success, or a negated error code:\n  -EINVAL\tEither policy, cpu, or some information in param is wrong.\n  -EFAULT\tEither param or info are invalid addresses.\n  -ENOMEM\tThe core could not allocate memory for carrying out the operation. Scary.\n  -EOPNOTSUPP policy is valid but not available from the core. CONFIG_EVL_SCHED_TP or CONFIG_EVL_SCHED_QUOTA are likely disabled in the kernel configuration.\n    int evl_yield(void)  In some cases, EVL threads undergoing the SCHED_FIFO or SCHED_RR policies might need to perform manual round-robin, like sched_yield() allows for plain POSIX threads. evl_yield() can be used for that purpose.\nManual round-robin moves the caller at the end of its priority group, yielding the CPU to other threads with the same priority. If there are none, then evl_yield() returns immediately with no effect.\nAlthough that would work, calling evl_yield() for a thread undergoing the SCHED_WEAK policy would make no sense. In that case, the caller would be immediately demoted to the in-band stage on return from the call, leaving the EVL scheduler right after it was promoted to the out-of-band stage for executing evl_yield(). Pretty much a useless CPU burner with no upside.\n On success, evl_yield() returns zero, otherwise a negated error code:\n-EPERM\tThe caller is not attached to the EVL core.\n Scheduling policies SCHED_FIFO policy The first-in, first-out policy, fixed priority, preemptive scheduling policy. If you really need a refresher about this one, you can still have a look at this inspirational piece of post-modern poetry for background info. Or you can just go for the short version: with SCHED_FIFO, the scheduler always picks the runnable thread with the highest priority which spent the longest time waiting for the CPU to be available.\nEVL provides 99 fixed priority levels starting a 1, which maps 1:1 to the in-band kernel\u0026rsquo;s SCHED_FIFO implementation as well.\nSetting the SCHED_FIFO parameters Switching a thread to FIFO scheduling is achieved by calling evl_set_schedattr() with the file descriptor of the target thread. The evl_sched_attrs attribute structure should be filled in as follows:\n#include \u0026lt;evl/sched.h\u0026gt; struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_FIFO; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ ret = evl_set_schedattr(efd, \u0026amp;attrs);  SCHED_RR policy The round-robin policy is based on SCHED_FIFO internally. Additionally, it limits the execution time of its members to a given timeslice, moving a thread which fully consumed its current timeslice to the tail of the scheduling queue for its priority level. This is designed as a simple way to prevent threads from over-consuming the CPU within their own priority level.\nUnlike the in-band kernel which defines a global timeslice value for all members of the SCHED_RR class, EVL defines a per-thread quantum instead. Since EVL is tickless, this quantum may be any valid duration, and may differ among threads from the same priority group.\nSetting the SCHED_RR parameters Switching a thread to round-robin scheduling is achieved by calling evl_set_schedattr() with the file descriptor of the target thread. The evl_sched_attrs attribute structure should be filled in as follows:\n#include \u0026lt;evl/sched.h\u0026gt; struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_RR; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ attrs.sched_rr_quantum = (struct timespec){ .tv_sec = \u0026lt;seconds\u0026gt;, .tv_nsec = \u0026lt;nanoseconds\u0026gt;, }; ret = evl_set_schedattr(efd, \u0026amp;attrs); SCHED_QUOTA policy The quota-based policy enforces a limitation on the CPU consumption of threads over a fixed period of time, known as the global quota period. Threads undergoing this policy are pooled in groups, with each group being given a share of the period (expressed as a percentage). Within a SCHED_QUOTA group, the SCHED_FIFO policy applies to all its members.\n    For instance, say that we have five distinct thread groups, each of which is given a runtime budget which represents a portion of the global period: 35%, 25%, 15%, 10% and finally 5% of a global period set to one second. The first group would be allotted 350 milliseconds over a second, the second group would get 250 milliseconds from the same period and so on.\nEvery time a thread undergoing the SCHED_QUOTA policy is given the CPU, the time it consumes is charged to the group it belongs to. Whenever the group as a whole reaches the alloted time budget, all its members stall until the next period starts, at which point the runtime budget of every group is replenished for the next round of execution, resuming all its members in the process.\nYou may attach as many threads as you need to a single group, and the number of threads may vary among groups. The alloted runtime quota for a group is decreased by the execution time of every thread in that group. Therefore, a group with no thread does not consume its quota.\nRuntime budget and peak quota Each thread group is given its full quota every time the global period starts, according to the configuration set for this group. If the group did not consume its quota entirely by the end of the current period, the remaining budget is added to the group\u0026rsquo;s quota for the next period, up to a limit defined as the peak quota. If the accumulated budget would cause the quota to exceed the peak value, the extra time is spread over multiple subsequent periods until the budget is fully consumed.\nManaging quota scheduling groups Creating, modifying and removing thread groups is achieved by calling evl_control_sched().\nCreating a quota group EVL supports up to 1024 distinct thread groups for quota-based scheduling system-wide. Each thread group is assigned to a specific CPU by the application code which creates it, among the set of CPUs EVL runs out-of-band activity on (see the evl.oobcpus kernel parameter).\nA thread group is represented by a unique integer returned by the core upon creation, aka the group identifier.\nCreating a new thread group is achieved by calling evl_sched_contol(). Some information including the new group identifier is returned in the ancillary evl_sched_ctlinfo structure passed to the request. The evl_sched_ctlparams control structure should be filled in as follows:\n#include \u0026lt;evl/sched.h\u0026gt; union evl_sched_ctlparam param; union evl_sched_ctlinfo info; int ret; param.quota.op = evl_quota_add; ret = evl_control_sched(SCHED_QUOTA, \u0026amp;param, \u0026amp;info, \u0026lt;cpu-number\u0026gt;); On success, the following information is received from the core regarding the new group:\n  info.quota.tgid contains the new group identifier.\n  info.quota.quota_percent is the current percentage of the global period allotted to the new group. At creation time, this value is set to 100%. You may want to change it to reflect the final value.\n  info.quota.quota_peak_percent reflects the current peak percentage of the global period allotted to the new group, which is also set to 100% at creation time. This means that by default, a new group might double its quota value by accumulating runtime budget, consuming up to 100% of the CPU time during the next period. Likewise, you may want to change it to reflect the final value.\n  info.quota.quota_sum is the sum of the quota values of all groups assigned to the CPU specified in the evl_control_sched() request. This gives the overall CPU business as far as SCHED_QUOTA is concerned. This sum should not exceed 100% for a CPU in a properly configured system.\n  Modifying a quota group Removing a quota group Setting the SCHED_QUOTA parameters Switching a thread to quota-based scheduling is achieved by calling evl_set_schedattr() with the file descriptor of the target thread, in order to attach such thread to a quota group along with setting its priority. To do so, the evl_sched_attrs attribute structure should be filled in as follows:\n#include \u0026lt;evl/sched.h\u0026gt; struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_QUOTA; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ attrs.sched_quota_group = \u0026lt;grpid\u0026gt;; /* Quota group id. */ ret = evl_set_schedattr(efd, \u0026amp;attrs);  SCHED_TP policy This policy enforces a so-called temporal partitioning, which is a way to schedule thread activities on a given CPU so that they cannot overlap time-wise. To this end, the policy defines a global, major time frame of fixed duration repeating cyclically, which is divided into smaller minor frames or time windows also of fixed durations, but not necessarily equal. Therefore, a minor frame is bounded by its time offset from the beginning of the major frame, and its own duration. The sum of all minor frame durations defines the duration of the global time frame.\nEach minor frame contributes to the run-time allotted within the global time frame to a group of thread called a partition. The maximum number of partitions in the system is defined by CONFIG_EVL_TP_NR_PART in the kernel configuration. Each thread undergoing this policy is attached to one of such partitions.\n    When a minor frame elapses on a CPU, the threads attached to its partition are immediately suspended, and threads from the partition assigned to the next minor frame may be picked for scheduling. When the last minor frame elapses, the process repeats from the minor frame leading the major time frame. You may assign as many threads as you need to a single partition, and the number of threads may vary among partitions. A partition with no thread simply runs no SCHED_TP thread until the next minor frame assigned a non-empty partition starts.\nValid partition numbers range from 0 to CONFIG_EVL_TP_NR_PART - 1. The special partition number EVL_TP_IDLE can be used when configuring the scheduler to designate the idle partition when assigning it to a minor frame, creating a time hole in the schedule which will not run any SCHED_TP thread until this minor frame elapses.\n Setting the temporal partitioning information for a CPU Temporal partitioning is defined on a per-CPU basis, you have to configure each CPU involved in this policy independently, enumerating the minor frames which compose the major one as follows:\n\tunion evl_sched_ctlparam param; int ret; param.tp.op = evl_tp_install; param.tp.nr_windows = \u0026lt;number_of_minor_frames\u0026gt;; param.tp.windows[0].offset = \u0026lt;offset_from_start_of_major_frame\u0026gt;; param.tp.windows[0].duration = \u0026lt;duration_of_minor_frame\u0026gt;; param.tp.windows[0].ptid = \u0026lt;assigned_partition_id\u0026gt;; ... param.tp.windows[\u0026lt;number_of_minor_frames\u0026gt; - 1].offset = \u0026lt;offset_from_start_of_major_frame\u0026gt;; param.tp.windows[\u0026lt;number_of_minor_frames\u0026gt; - 1].duration = \u0026lt;duration_of_minor_frame\u0026gt;; param.tp.windows[\u0026lt;number_of_minor_frames\u0026gt; - 1].ptid = \u0026lt;assigned_partition_id\u0026gt;; ret = evl_control_sched(SCHED_TP, \u0026amp;param, NULL, \u0026lt;cpu-number\u0026gt;); If the special value EVL_TP_IDLE is assigned to a minor time frame (ptid), no thread undergoing the SCHED_TP policy will run until such frame elapses. This is a way to create a time hole within the major time frame.\nThis operation implicitly overrides the previous TP settings for the CPU, leaving the TP scheduling in stopped state. Generally speaking, once a TP scheduling plan is installed for a CPU, it is left in standby mode until explicitly started.\nNo information is passed back by the core when installing a TP schedule, you may pass the info parameter as NULL to evl_control_sched().\nStarting the TP scheduling on a CPU The following request enables the TP scheduler for the given CPU; you have to issue this request for activating a TP scheduling plan:\n\tunion evl_sched_ctlparam param; int ret; param.tp.op = evl_tp_start; ret = evl_control_sched(SCHED_TP, \u0026amp;param, NULL, \u0026lt;cpu-number\u0026gt;); No information is passed back by the core when starting a TP scheduling plan, you may pass the info parameter as NULL to evl_control_sched().\nStopping the TP scheduling on a CPU You can disable the scheduling of threads undergoing the SCHED_TP policy on a CPU by the following call:\n\tunion evl_sched_ctlparam param; int ret; param.tp.op = evl_tp_stop; ret = evl_control_sched(SCHED_TP, \u0026amp;param, NULL, \u0026lt;cpu-number\u0026gt;); Upon success, the target CPU won\u0026rsquo;t be considered for running SCHED_TP threads, until the converse request evl_tp_start is received for that CPU.\nNo information is passed back by the core when stopping a TP scheduling plan, you may pass the info parameter as NULL to evl_control_sched().\nRetrieving the temporal partitioning information from a CPU The following request fetches the current scheduling plan for a given CPU:\n\tunion evl_sched_ctlparam param; struct { union evl_sched_ctlinfo info; struct __sched_tp_window windows[\u0026lt;max_number_of_minor_frames\u0026gt;]; } result; int ret; param.tp.op = evl_tp_get; param.tp.nr_windows = \u0026lt;max_number_of_minor_frames\u0026gt;; ret = evl_control_sched(SCHED_TP, \u0026amp;param, \u0026amp;result.info, \u0026lt;cpu-number\u0026gt;); param.nr_windows[] specifies the maximum number of minor frames the EVL core should dump into the result.info.nr_windows[] output variable array. This value is automatically capped to the actual number of minor frames defined for the target CPU at the time of the call.\nThe minor frames active for the target CPU are readable from the result.windows[] array, up to param.nr_windows[] . The number of valid elements in this array is given by result.info.nr_windows[].\nRemoving the temporal partitioning information from a CPU This is the converse operation to evl_tp_install, removing temporal partitioning support for the target CPU, releasing all related resources. This request implicitly stops the TP scheduling on such CPU prior to uninstalling.\nSetting the SCHED_TP parameters Switching a thread to temporal partitioning is achieved by calling evl_set_schedattr() with the file descriptor of the target thread. The evl_sched_attrs attribute structure should be filled in as follows:\n#include \u0026lt;evl/sched.h\u0026gt; struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_TP; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ attrs.sched_tp_partition = \u0026lt;ptid\u0026gt;; /* Partition id. */ ret = evl_set_schedattr(efd, \u0026amp;attrs); ptid should be a valid partition identifier between 0 and CONFIG_EVL_TP_NR_PART - 1.\nWithin a minor time frame, threads undergo the SCHED_FIFO policy according to their fixed priority value.\n SCHED_WEAK policy You may want to run some POSIX threads in-band most of the time, except when they need to call some EVL services occasionally. Occasionally here means either non-repeatedly, or at any rate not from a high frequency loop.\nMembers of this class are picked second to last in the hierarchy of EVL scheduling classes, right before the sole member of the SCHED_IDLE class which stands for the in-band execution stage of the kernel. The priority such threads have in the in-band context are preserved by this policy when they (normally briefly) run on the out-of-band execution stage. For this reason, SCHED_WEAK threads have a fixed priority ranging from 0 to 99 included, which maps to in-band SCHED_OTHER (0), SCHED_FIFO and SCHED_RR (1-99) priority ranges.\nA thread scheduled in the SCHED_WEAK class may invoke any EVL service, including blocking ones for waiting for out-of-band events (e.g. depleting a semaphore), which will certainly switch it to the out-of-band execution stage. Before returning from the EVL system call, the thread will be automatically switched back to the in-band execution stage by the core. This means that each and every EVL system call issued by a thread assigned to the SCHED_WEAK class is going to trigger two execution stage switches back and forth, which is definitely costly. So make sure not to use this feature in any high frequency loop.\nIn a specific case the EVL core will keep the thread running on the out-of-band execution stage though: whenever this thread has returned from a successful call to evl_lock_mutex(), holding an EVL mutex. This ensures no in-band activity on the same CPU can preempt the lock owner, which would certainly lead to a priority inversion would that lock be contended later on by another EVL thread. The lock owner is eventually switched back to in-band mode by the core as a result of releasing the last EVL mutex it was holding.\n There are only a few legitimate use cases for assigning an EVL thread to the SCHED_WEAK scheduling class:\n  as part of some initialization, cleanup or any non real-time phase of your application, a thread needs to synchronize with another EVL thread which belongs to a real-time class like SCHED_FIFO.\n  some in-band thread which purpose is to handle fairly exceptional events needs to be notified by a real-time thread to do so.\n  In all other cases where an in-band thread might need to be driven by out-of-band events which may occur at a moderate or higher rate, using a message-based mechanism such as a cross-buffer is the best way to go.\nAlso note that some EVL services may be called by regular POSIX threads. Typically, a plain POSIX thread may post an EVL semaphore for signaling an out-of-band EVL thread pending on it. You can find the allowed calling contexts for each EVL service from libevl there.\nSwitching a thread to SCHED_WEAK is achieved by calling evl_set_schedattr() with the file descriptor of the target thread. The evl_sched_attrs attribute structure should be filled in as follows:\n#include \u0026lt;evl/thread.h\u0026gt; struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_WEAK; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [0-99] */ ret = evl_set_schedattr(efd, \u0026amp;attrs);  SCHED_IDLE policy The idle class has a single task on each CPU: the low priority placeholder task.\nSCHED_IDLE has the lowest priority among policies, its sole task is picked for scheduling only when other policies have no runnable task on the CPU. A task member of the SCHED_IDLE class cannot block, it is always runnable.\nThis is an internal class, which is neither available to user applications nor drivers.\n Last modified: Tue, 22 Dec 2020 17:32:32 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/stage_push/",
	"title": "Installing the out-of-band stage",
	"tags": [],
	"description": "",
	"content": "Before you can direct the incoming interrupt flow to out-of-band handlers, you need to install the out-of-band interrupt stage. Conversely, you need to remove the out-of-band stage from the interrupt pipeline when you are done with receiving out-of-band events.\n  int enable_oob_stage(const char *name)  nameA symbolic name describing the high priority interrupt stage which is being installed. This information is merely used in kernel messages, so it should be short but descriptive enough. For instance, the EVL core installs the \u0026ldquo;EVL\u0026rdquo; stage.\n\nThis call enables the out-of-band stage context in the interrupt pipeline, which in turn allows an autonomous core to install out-of-band handlers for interrupts. It returns zero on success, or a negated error code if something went wrong:\n-EBUSY\tThe out-of-band stage is already enabled.\n  void disable_oob_stage(void)  This call disables the out-of-band stage context in the interrupt pipeline. From that point, the interrupt flow is exclusively directed to the in-band stage.\nThis call does not perform any serialization with ongoing interrupt handling on remote CPUs whatsoever. The autonomous core must synchronize with remote CPUs before calling disable_oob_stage() to prevent them from running out-of-band handlers while the out-of-band stage is being dismantled. This is particularly important if these handlers belong to a dynamically loaded module which might be unloaded right after disable_oob_stage() returns. In that case, you certainly don\u0026rsquo;t want the .text section containing interrupt handlers to vanish while they are still running.\n  Last modified: Fri, 24 Apr 2020 18:00:06 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/heap/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Dynamic memory allocation from out-of-band context EVL applications usually have to manage the RAM resource dynamically, so that the number of live objects they may need to maintain, and for how long they would have to do so, does not have to be defined or even known in advance. For this purpose, libevl implements a memory heap manager usable from the out-of-band context, from which objects can be allocated or released dynamically in a time-bound fashion.\nAn EVL heap is composed of at least one so-called memory extent, which is a contiguous RAM area chunks are allocated from and released to. Each extent is at most 4Gb - 512 bytes long. The first extent attached to any given heap is passed at creation time to evl_init_heap(). Then, you may add more space for storage by attaching more RAM extents to such heap using evl_extend_heap(). There is no arbitrary limit on the number of extents forming a heap; however, since these extents are tracked in a linked list which might have to be scanned for space when allocating chunks from a crowded heap, you may want to limit the live extents to a reasonable number (less than ten would seem appropriate). There is no arbitrary limit on the number of heaps an application can create either.\nIn other words, how many heaps you may create and how large such heaps might be is only limited to the amount of RAM available on your system.\nHow an EVL heap works The memory allocation scheme is a variation of the algorithm described in a USENIX 1988 paper called \u0026ldquo;Design of a General Purpose Memory Allocator for the 4.3BSD Unix Kernel\u0026rdquo; by Marshall K. McKusick and Michael J. Karels. You can find it at various locations on the Internet.\nAn EVL heap organizes the memory extents it has been given as a set of fixed-size pages where allocated blocks live, with each page worth 512 bytes of storage. In any given extent, pages can be either part of the free pool, or busy storing user data. A busy page either contains one or multiple blocks (aka chunks), or it may be part of a larger block which spans multiple contiguous pages in memory. Pages containing chunks (as opposed to pages representing a portion of a larger block) are grouped by common chunk size, which is always a power of 2. Every allocation request is rounded to the next power of 2, with a minimum of 16 bytes, e.g. calling evl_alloc_heap() for 60 bytes will reserve a 64-byte chunk internally.\nTo this end, the heap manager maintains the following data structures for each extent of a given heap:\n  a free page pool. This pool is maintained in an couple of AVL trees to ensure time-bound operations on inserting and removing pages (either by size or address).\n  an array of lists of free pages used as a fast block cache. Each entry in the array links pages which are available for storing chunks of the same size. The chunk size may range from 2^4 to 2^8 bytes, which gives an array of five entries for representing all the possible sizes we may allocate from the fast cache. Within a single page of 512 bytes, up to 32 chunks of 16 bytes each are available, 16 chunks for 32 bytes and so on, down to 2 chunks of 256 bytes.\n  The allocation strategy is as follows, for each available extent until the request is satisfied or impossible to satisfy by any extent:\n  if the application requests a chunk which is not larger than half the size of a page (i.e. 2^8 or 256 bytes), then the fast block cache of the current extent is searched for a free chunk. For instance, a request for allocating 24 bytes would cause a lookup into the fast cache for a free chunk of 32 bytes. If no free chunk is available from the cache for that size, a new page is pulled from the free pool, added to the free page list for the corresponding size, and the allocation is tried again.\n  if the size of requested chunk rounded up to the next power of 2 is larger than half the size of a page (i.e. \u0026gt;= 2^9 bytes), a set of contiguous pages which covers the entire allocation request is directly pulled from the free pool maintained in the current extent. In this case, the fast block cache is not used.\n  Some runtime characteristics   the implementation is thread-safe, using an EVL mutex internally to serialize callers while updating the heap state.\n  O(1) access guarantee on allocation and release of free chunks into a fast block cache, ranging from 16 to 256 bytes. This is the typical allocation pattern an EVL heap is good at handling very quickly.\n  O(log n) access guarantee on allocation and release of pages from/to a free pool.\n  the EVL heap yields limited internal fragmentation for small chunks which can fit into fast block caches. Since there is no meta-data associated to busy blocks pulled from such cache, the memory overhead is basically zero in this case. The larger the chunk, the larger the internal fragmentation since all requested sizes are aligned on the next power of 2, up to 2^9. So asking for 257 bytes would actually consume an entire 512-byte page internally, directly pulled from the corresponding free pool.\n  for precise information about the runtime performance of the EVL heap manager on your platform, you may want to have a look at the output of the heap_torture test with verbosity turned on, as follows:\n  # $(evl test -L heap-torture) -v Other options for a real-time capable memory allocator   Another option for a deterministic memory allocator would be TLSF, which would easily work on top of EVL\u0026rsquo;s out-of-band context with some limited adaptation. It is slightly faster than the EVL heap on average, but internal fragmentation for the typical use cases it was confronted to looks much higher.\n  The fragmentation issue of the original TLSF implementation seems to have led to a later implementation addressing the problem.\n  These are only examples which come to mind. There should be no shortage of memory allocators you could adapt for running EVL applications. You would only need to make sure to use EVL services exclusively in that code, like mutexes if you need a thread-safe implementation.\n Initializing a memory heap\n This is how one could create a memory heap out of a static array of characters defined in a program:\n\t#include \u0026lt;evl/heap.h\u0026gt; static char heap_storage[EVL_HEAP_RAW_SIZE(1024 * 1024)]; /* 1Mb heap */ static struct evl_heap runtime_heap; int init_runtime_heap(void) { int ret; ret = evl_init_heap(\u0026amp;runtime_heap, heap_storage, sizeof heap_storage); ... return ret; } Likewise, but this time using malloc(3) to get the raw storage for the new heap:\n\t#include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;evl/heap.h\u0026gt; static struct evl_heap runtime_heap; int init_runtime_heap(void) { const size_t raw_size = EVL_HEAP_RAW_SIZE(1024 * 1024); /* 1Mb heap */ void *heap_storage; int ret; heap_storage = malloc(raw_size); ... ret = evl_init_heap(\u0026amp;runtime_heap, heap_storage, raw_size); ... return ret; } Memory heap services  int evl_init_heap(struct evl_heap *heap, void *mem, size_t size)  This service initializes a heap, based on a memory area provided by the caller. This area should be large enough to contain both the user payload, and the meta-data the heap manager is going to need for maintaining such payload. The length of such area is called the raw size, as opposed to the lesser amount of bytes actually available for storing the user payload which is called the user size.\nheapAn in-memory heap descriptor is constructed by evl_init_heap(), which contains ancillary information other calls will need. heap is a pointer to such descriptor of type struct evl_heap.\n\nmemThe start address of the raw memory area which is given to the heap manager for serving dynamic allocation requests.\n\nsizeThe size (in bytes) of the raw memory area starting at mem. This size includes the space required to store the meta-data needed for maintaining the new heap. You should use the EVL_HEAP_RAW_SIZE(user_size) macro to determine the total amount of memory which should be available from mem for creating a heap offering up to user_size bytes of payload data. For instance, you would use EVL_HEAP_RAW_SIZE(8192) as the value of the size argument for creating a 8Kb heap.\n\nZero is returned on success. Otherwise, a negated error code is returned:\n  -EINVAL is returned if size is invalid, cannot be used to derive a proper user size. A proper user size should be aligned on a page boundary (512 bytes), cover at least one page of storage, without exceeding 4Gb - 512 bytes.\n  Since evl_init_heap() creates a mutex for protecting access to the heap meta-data, any return code returned by evl_create_mutex().\n    int evl_extend_heap(struct evl_heap *heap, void *mem, size_t size)  Add more storage space to an existing heap. The extent space should be large enough to contain both the additional user payload, and the meta-data the heap manager is going to need for maintaining such payload.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nmemThe start address of the raw memory area which is added to the heap.\n\nsizeLike with evl_init_heap(), the size (in bytes) of the raw memory area starting at mem which includes the space required to store the meta-data needed for maintaining the additional set of pages. You should use the EVL_HEAP_RAW_SIZE(user_size) macro to determine the total amount of memory which should be available from mem for creating an extent offering up to user_size bytes of additional payload data. For instance, you would use EVL_HEAP_RAW_SIZE(8192) as the value of the size argument for creating a 8Kb extent.\n\nZero is returned on success. Otherwise, a negated error code is returned:\n -EINVAL is returned if size is invalid, cannot be used to derive a proper user size. A proper user size should be aligned on a page boundary (512 bytes), cover at least one page of storage, without exceeding 4Gb - 512 bytes.    void evl_destroy_heap(struct evl_heap *heap)  This call dismantles a memory heap. The user storage is left unspoiled by the deletion process, and no specific action is taken regarding the raw storage received from either evl_init_heap() or evl_extend_heap() when the heap was active. Such storage should be further released by the caller if need be.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap() to be dismantled.\n\n  void *evl_alloc_block(struct evl_heap *heap, size_t size)  Allocate a chunk of memory from a given heap.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nsizeThe amount of bytes to allocate.\n\nA valid memory pointer is returned on success, otherwise NULL. The following is true for any chunk of memory returned by evl_alloc_block():\n  if size is smaller than 512 bytes, it is rounded up to the next power of 2, with a minimum of 16 bytes.\n  if size is larger than 512 bytes, it is rounded up to the next 512-byte boundary.\n  the address of the new chunk is aligned on a 16-byte boundary.\n    int evl_free_block(struct evl_heap *heap, void *block)  Release a previously allocated chunk of memory, returning it to the originating heap.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nblockThe address of a chunk to release which was originally obtained from evl_alloc_block().\n\n  int evl_check_block(struct evl_heap *heap, void *block)  Check if block is an active memory chunk living in heap. The thoroughness of this test depends on whether libevl was compiled with the optimizer disabled (i.e. -O0 passed to the compiler, causing the __OPTIMIZE__ macro flag not to be defined at build time). If the optimizer was enabled, this routine may not be able to detect whether block is part of a valid data page. Therefore, you would probably rely on this service only with debug builds.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nblockThe address of a block to check for sanity.\n\nZero is returned if block which was originally obtained from evl_alloc_block() and has not yat been released at the time of the call. Otherwise, a negated error code is returned:\n -EINVAL if block is not an active memory chunk previously returned by evl_alloc_block().    size_t evl_heap_raw_size(struct evl_heap *heap)  Return the raw size of the memory storage associated with heap. This size includes the storage which may have been further added to the heap by calling evl_extend_heap().\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nReturn the raw size (in bytes) of the memory storage associated with heap.\n  size_t evl_heap_size(struct evl_heap *heap)  Return the user (or payload) size of the memory storage associated with heap. This size includes the payload storage which may have been further added to the heap by calling evl_extend_heap(). This is the actual amount of bytes available for storing user data, which is lesser than the raw heap size since it does not account for the meta-data.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nReturn the user size (in bytes) of the memory storage associated with heap.\n  size_t evl_heap_used(struct evl_heap *heap)  Return the amount of space already consumed from the user (or payload) area.\nheapThe in-memory mutex descriptor previously constructed by evl_init_heap().\n\nReturn the amount of space (in bytes) consumed within heap. This value is lower or equal to the user (or payload) size.\n Last modified: Mon, 11 May 2020 11:54:16 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/tube/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "A lightweight FIFO messaging mechanism The tube is a lighweight and flexible FIFO data structure which you can use to send any type of data between threads. Since the algorithm manipulating a tube is lockless, threads can use it regardless of their respective execution stage, therefore a tube could also be used as a very simple inter-stage messaging system. In addition, the tube supports the multi-reader and multi-writer paradigms. The intent is to provide a basic mechanism which can either be used \u0026ldquo;as is\u0026rdquo; for fully non-blocking send/receive operations, or as a building block for implementing featureful message queues including sleeping wait for input and output congestion control.\nAt its core, an EVL tube is basically a one-way linked list conveying so-called canisters in a FIFO manner. Each canister is a container for a data item (or payload) a thread wants to send to another thread.\nCreating a tube The following steps are required to create a tube:\n You first need to declare the C structure type of the canister which is going to convey data through that tube. This is done by using the DECLARE_EVL_CANISTER(canister_tag, data_type) macro in your code, which should be given the name of the new canister type (used as the C structure tag), and the C type of the data to be conveyed in that canister. For instance, the following snippet would declare the j1587_canister type, conveying J1587 protocol data (some automotive diagnostic protocol, could be anything else of use of course):  #include \u0026lt;evl/tube.h\u0026gt; struct j1587_data { uint16_t pid; uint16_t ecu; unsigned char data[17]; }; DECLARE_EVL_CANISTER(j1587_canister, struct j1587_data); The macro call above expands to a complete C `struct  j1587_canisterdefinition, laying out the information needed to convey one item of typestruct j1587_data`, such as:\nstruct j1587_canister { struct j1587_data payload; struct j1587_canister *next; }; Then you need to declare the tube data structure itself, mentioning which kind of canister is going to flow through it. Note that a tube is fit for one specific type of canister, you cannot use multiple types of canister with a single tube. Such declaration is obtained by expanding the DECLARE_EVL_TUBE(tube_tag, canister_tag) macro, which receives the name of the new tube type (used as a C structure tag) and the canister tag passed earlier to DECLARE_EVL_CANISTER().  #include \u0026lt;evl/tube.h\u0026gt; DECLARE_EVL_TUBE(j1587_tube, j1587_canister);  The macro call above defines the C struct type describing a  tube conveying canisters which in turn contain a payload of type j1587_data, such as:\n struct j1587_tube { ... }; Finally, you need to initialize the tube by a call to evl_init_tube(), passing it a memory area which should be large enough to store as many canisters as required. This should be seen as the maximum number of canisters which can be queued into the tube at once. In other words, this is the maximum number of messages such queue can hold without running out of memory. The initialization code can refer to the declarations of the canister and tube types obtained by DECLARE_EVL_CANISTER() and DECLARE_EVL_TUBE():  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;evl/tube.h\u0026gt; /* The tube can convey up to sixteen j1587 items at once. */ static DECLARE_EVL_CANISTER(j1587_canister, struct j1587_data) canisters[16]; static DECLARE_EVL_TUBE(j1587_tube, j1587_canister) tube; evl_init_tube(\u0026amp;tube, canisters, 16); printf(\u0026quot;j1587 tube can convey %ld messages concurrently\\n\u0026quot;, tube.max_items); /* should display as '16' */ Once the tube is initialized, threads can communicate over it by calling evl_send_tube() and evl_receive_tube().\nYou can use any valid C type to hold the payload into a canister. However, keep in mind that conveying a payload entails two copies of the corresponding data: first to install it into the canister when evl_send_tube() is called, next when evl_receive_tube() extracts it from the canister at the other end to private memory, so you definitely want to keep the payload size reasonable. If you need to convey large bulks of data as single messages flowing through the tube, then you should consider declaring a canister type which only stores a pointer to the final data, so that only that pointer needs to be copied, e.g.:\n #include \u0026lt;evl/tube.h\u0026gt; DECLARE_EVL_CANISTER(massive_bitmap_canister, void *); The macro call above defines the C struct type describing a canister which conveys a reference to a large bulk of data using an opaque pointer. It would be expanded as follows in the code:\n struct massive_bitmap_canister { void *payload; struct massive_bitmap_canister *next; }; Using tubes for inter-process messaging A special form of tube can be used for transferring data between processes, using the *_rel() interface variant, which stands for relative addressing. As this implies, all internal references within the tube data structure use base-offset addressing instead of absolute memory pointers, so that such data structure can be mapped to a piece of memory shared between processes via mmap(2). DECLARE_EVL_TUBE_REL() should be used to define the C type of the new inter-process tube, along with DECLARE_EVL_CANISTER_REL() for declaring the canister type for such a tube. Eventually, evl_init_tube_rel() should be used for initializing the the new tube, evl_send_tube_rel() for pushing data through it, and evl_receive_tube_rel() for pulling available messages from it.\nOf course, you still need to refrain from conveying absolute pointers referring to a particular process address space into the message payload. Here is an example of mapping an EVL tube which can handle up to 1024 messages flowing concurrently, to a new shared memory segment which we will be creating for the purpose:\n #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;evl/tube.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; struct j1587_data { uint16_t pid; uint16_t ecu; unsigned char data[17]; }; DECLARE_EVL_CANISTER_REL(j1587_rel_canister, struct j1587_data); DECLARE_EVL_TUBE_REL(j1587_rel_tube, j1587_rel_canister); j1587_rel_tube *tube; int shmfd, ret; size_t len; void *ptr; /* Create a shared memory object. */ shmfd = shm_open(some_shm_pathname, O_RDWR, 0660); ... /* * Use ftruncate() to fit the shared memory to the tube size. We want * up to 1024 data items to flow concurrently through the tube. */ len = evl_get_tube_size_rel(j1787_rel_tube, 1024); ret = ftruncate(shmfd, len); ... /* Now map this new segment into our address space. */ ptr = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_SHARED, shmfd, 0); ... /* * Finally, set up the tube over the shared memory and retrieve its * descriptor for further commands. From this point, you can send and * receive j1787_data messages through it. */ tube = evl_init_tube_rel(j1587_rel_tube, j1587_rel_canister, ptr, len); Using tubes for out-of-band ⇔ in-band messaging Since sending and receiving to/from a tube is performed locklessly and does not involve any system call, this data structure can be used for implementing a basic message queue between threads which may belong to different stages, which is an alternative to using a cross-buffer.\nBlocking input and/or output congestion control A tube is inherently non-blocking, it neither imposes any policy nor provides support for sleeping in absence of input or for dealing with output congestion. It only provides a very simple lockless mechanism for transferring arbitrary data between peers. This means that evl_send_tube() may return a failure status (i.e. boolean false) if no free canister is available for conveying data at the time of the call. Conversely, evl_receive_tube() may also return a failure status in case no data is immediately available at the receiving end of the tube. If you need the sender(s) to handle output congestion by sleeping until canisters are free for sending to the other side, or the receiver(s) to sleep until some data is available for input, the trick is to combine a tube with the proper synchronization mechanisms. For instance:\n  if both the sender(s) and receiver(s) run on the out-of-band stage, then a pair of EVL semaphores would suffice: the sender would wait on a semaphore counting the number of free canisters in the tube before attempting to push a new data item, and the receiver would wait on the other semaphore counting the number of canisters available for reading from the tube. Conversely, the receiver would signal the semaphore counting the free canisters after it has successfully pulled a data item, and the sender would signal the input semaphore after it has successfully pushed a new data item.\n  if the sender or the receiver run on the out-of-band stage but its peer may only run in-band, you could use one EVL semaphore, and a proxy associated with a regular eventfd(2). How to synchronize two threads which belong to distinct execution stages using such combo is explained in this document.\n  The following figure summarizes these options for synchronizing tube operations:\nTube services  void evl_init_tube(struct {tube_tag} *tube, struct {canister_tag} freevec[], int count)  Initialize a tube data structure for process-local use, which can only happen once the canister and tube types have been defined.\ntubeThe address of the tube structure to initialize. The C type of the new tube should have been declared earlier by the DECLARE_EVL_TUBE() macro.\n\nfreevecThe start address of an array of canisters which should be used to convey the payload through the tube. The C type of the basic element should have been declared earlier by the DECLARE_EVL_CANISTER() macro.\n\ncountThe number of elements in freevec.\n\n  void evl_init_tube_rel({tube_tag}, {canister_tag}, void *mem, size_t memsize)  Initialize a relative-addressing tube data structure for inter-process use, which can only happen once the canister and tube types have been defined. Unlike with the process-local variant, the relative-addressing variant requires both the tube meta-data and the canisters to be part of the same (shareable) memory segment so that all peers have easily access to both of them.\ntube_tagThe C tag of the tube type to initialize. The C type of the new tube should have been declared earlier by the DECLARE_EVL_TUBE_REL() macro.\n\nmemThe start address of the shared memory area where both the tube meta-data and canisters will live.\n\nmemsizeThe length in bytes of mem. Because a portion of mem is reserved for storing meta-data, the number of canisters available for conveying actual payload through the tube is lesser than memsize / sizeof(struct \u0026lt;canister_tag\u0026gt;). You can use evl_get_tube_size_rel() to calculate the exact amount of memory you would need for storing a process-shared tube given the canister type and a maximum number of messages flowing concurrently through the tube. The returned value should be used to allocate the shared memory segment, then passed to evl_init_tube_rel.\n\nThis macro returns the tube address which should be used in other relative-addressing calls. You can retrieve the number of canisters available with this tube by fetching the max_items members from the tube C type.\n  bool evl_send_tube[_rel](struct {tube_tag} *tube, {payload})  Send a data item through a tube in a FIFO manner. The _rel variant should be used for sending to a relative-addressing tube. This payload is first copied to a free canister, which is then queued for consumption by the receiving end.\ntubeThe address of the tube to push the data to.\n\npayloadThe data to push to the tube. This argument is passed by reference to the evl_send_tube() macro.\n\nThis macro returns a boolean true value on success, or false in case no free canister was available for sending more data at the time of the call. If you need a mechanism to have the sender block on output contention, you may want to have a look at this section.\n  bool evl_receive_tube[_rel](struct {tube_tag} *tube, {payload})  Receive the next available data item from a tube. The _rel variant should be used for receiving from a relative-addressing tube. The incoming payload is eventually copied to the payload argument before the conveying canister is made available anew for further sending.\ntubeThe address of the tube to pull data from.\n\npayloadA variable to be assigned the payload data received from the tube. This argument is passed by reference to the evl_receive_tube() macro.\n\nThis macro returns a boolean true value on success, or false in case no free canister was available for sending more data at the time of the call. If you need a mechanism to have the receiver block on lack of input, you may want to have a look at this section.\n  bool evl_get_tube_size[_rel]({tube_tag}, int count)  Calculate the exact amount of memory required for storing a tube data structure given the canister type and a maximum number of messages flowing concurrently through that tube. This size represents the total amount of memory which is needed for storing a complete tube data structure.\ntube_tagThe C tag of the tube type to get the size of.\n\ncountThe number of messages which can flow concurrently through the tube.\n\nReturn the size in bytes representing the amount of memory required for storing both the meta-data and the canisters for the given tube type.\n Last modified: Mon, 30 Dec 2019 16:12:56 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/stage_escalation/",
	"title": "Stage escalation",
	"tags": [],
	"description": "",
	"content": "Sometimes you may need to escalate the current execution stage from in-band to out-of-band, only for running a particular routine. This can be done using run_oob_call(). For instance, the EVL core is using this service to escalate calls to its rescheduling procedure to the out-of-band stage, as described in the discussion about switching task contexts with Dovetail\u0026rsquo;s support for alternate scheduling.\n  int run_oob_call(int (*fn)(void *arg), void *arg)  fnThe address of the routine to execute on the out-of-band stage.\n\nargThe routine argument.\n\nrun_oob_call() first switches the current execution stage to out-of-band - if need be - then calls the routine with hard interrupts disabled (i.e. disabled in the CPU). Upon return, the integer value returned by fn() is passed back to the caller.\nBecause the routine may switch the execution stage back to in-band for the calling context, run_oob_call() restores the original stage only if it did not change in the meantime. In addition, the interrupt log of the current CPU is synchronized before returning to the caller. The following matrix describes the logic for determining which epilogue should be performed before leaving run_oob_call(), depending on the active stage on entry to the latter and on return from fn():\n   On entry to run_oob_call() At exit from fn() Epilogue     out-of-band out-of-band sync current stage if not stalled   in-band out-of-band switch to in-band + sync both stages   out-of-band in-band sync both stages   in-band in-band sync both stages    run_oob_call() is a lightweight operation that switches the CPU to the out-of-band interrupt stage for the duration of the call, whatever the underlying context may be. This is different from switching a task context to the out-of-band stage by offloading it to the autonomous core for scheduling. The latter operation would involve a more complex procedure.\n  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/pipeline/locking/",
	"title": "Locking",
	"tags": [],
	"description": "",
	"content": "Additional spinlock types The pipeline core introduces two spinlock types:\n hard spinlocks manipulate the CPU interrupt mask, and don\u0026rsquo;t affect the kernel preemption state in locking/unlocking operations.  This type of spinlock is useful for implementing a critical section to serialize concurrent accesses from both in-band and out-of-band contexts, i.e. from in-band and oob stages. Obviously, sleeping into a critical section protected by a hard spinlock would be a very bad idea. In other words, hard spinlocks are not subject to virtual interrupt masking, therefore can be used to serialize with out-of-band activities, including from the in-band kernel code. At any rate, those sections ought to be quite short, for keeping latency low.\n hybrid spinlocks are used internally by the pipeline core to protect access to IRQ descriptors (struct irq_desc::lock), so that we can keep the original locking scheme of the generic IRQ core unmodified for handling out-of-band interrupts.  Mutable spinlocks behave like hard spinlocks when traversed by the low-level IRQ handling code on entry to the pipeline, or common raw spinlocks otherwise, preserving the kernel (virtualized) interrupt and preemption states as perceived by the in-band context. This type of lock is not meant to be used in any other situation.\nLockdep support The lock validator automatically reconciles the real and virtual interrupt states, so it can deliver proper diagnosis for locking constructs defined in both in-band and out-of-band contexts. This means that hard and hybrid spinlocks are included in the validation set when LOCKDEP is enabled.\nThese two additional types are subject to LOCKDEP analysis. However, be aware that latency figures are likely to be really bad when LOCKDEP is enabled, due to the large amount of work the lock validator may have to do with interrupts disabled for the CPU (i.e. hard locking) for enforcing critical sections.\n  Last modified: Sat, 21 Nov 2020 17:55:57 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/misc/",
	"title": "Miscellanea",
	"tags": [],
	"description": "",
	"content": "A set of ancillary services which are not directly related to EVL elements.\n  struct evl_version evl_get_version(void)  This function returns the available version information about the current libevl API and EVL core.\nA structure of type struct evl_version containing such information is returned (this call cannot fail). The definition of this type is as follows:\nstruct evl_version { int api_level;\t/* libevl.so: __EVL__ */ int abi_level;\t/* core: EVL_ABI_PREREQ, -1 for ESHI */ const char *version_string; };   api_level matches the value carried by the __EVL__ macro-definition when the libevl code was compiled. A list of released API versions is available in this document.\n  the abi_level is dynamically returned from the EVL core running on the current machine. Details about ABI management in EVL can be found in this document.\n  a version string which collates all the revision information available for pretty-printing.\n    void evl_sigdebug_handler(int sig, siginfo_t *si, void *ctxt)  This routine is a basic signal handler for the SIGDEBUG signal which simply prints out the HM diagnostics received to stdout then returns.\nlibevl does not install this handler by default, this is up to your application to do so if need be.\nsig, si and ctxt correspond to the parameters received by the sa_sigaction handler which your application should install using sigaction() (SA_SIGINFO must be set in the action flags).\n Last modified: Fri, 01 May 2020 18:32:54 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/",
	"title": "Porting Dovetail",
	"tags": [],
	"description": "",
	"content": "Porting the Dovetail interface to a different kernel release, or a different CPU architecture involves enabling the interrupt pipeline first, then the alternate scheduling feature which builds on the former.\nInterrupt pipelining is enabled by turning on CONFIG_IRQ_PIPELINE: getting this feature to work flawlessly is a prerequisite before the rest of the Dovetail port can proceed. At the very least, you should check that these kernel features get along with interrupt pipelining:\n CONFIG_PREEMPT CONFIG_TRACE_IRQFLAGS (selected by CONFIG_IRQSOFF_TRACER, CONFIG_PROVE_LOCKING) CONFIG_LOCKDEP (selected by CONFIG_PROVE_LOCKING, CONFIG_LOCK_STAT, CONFIG_DEBUG_WW_MUTEX_SLOWPATH, CONFIG_DEBUG_LOCK_ALLOC) CONFIG_CPU_IDLE  Once and only when this layer is rock-solid should you start working on enabling the alternate scheduling support, which will allow your autonomous core to schedule tasks created by the kernel. At the end of this process, turning on CONFIG_DOVETAIL should enable full support for coupling your autonomous core to the kernel.\nTo clarify what such porting effort involves, let\u0026rsquo;s have a look at the main kernel sub-systems/features impacted by the Dovetail-related changes:\nIRQFLAGS This is the most basic change to introduce into the kernel, in order to turn the architecture-specific API manipulating the CPU\u0026rsquo;s interrupt flag into the equivalent virtualized calls the interrupt pipeline provides. By virtualizing these operations, Dovetail keeps the hardware interrupt events flowing in while still preserving the in-band kernel code from undue interrupt delivery as explained in this document. You need to follow this procedure for implementing such virtualization.\nAtomic OPS Once the IRQFLAGS have been adapted to interrupt pipelining, the original atomic operations which rely on explicitly disabling the hardware interrupts to guarantee atomicity cannot longer work, unless the call sites are restricted to in-band context, which is not an option as we will certainly need them for carrying out atomic operations from out-of-band context too. So we need to iron them in a way that adds back serialization between callers during updates, regardless of the caller\u0026rsquo;s context. Both a few generic and architecture-specific operations need fix ups to achieve this as documented here.\nIPI Handling With SMP-capable hardware, the kernel uses Inter-Processor Interrupts (aka IPI) to notify remote cores about particular events which may have happened on the send side. For Dovetail to enable the autonomous core to trigger and receive some of those special interrupts like any common (e.g. device) interrupt, some architecture-specific code is required. For instance, a SMP-capable autonomous core will need the rescheduling IPI Dovetail defines in order to force a remote core to reschedule its tasks.\nKernel entry A kernel entry starts with low level assembly code receiving interrupts, traps/exceptions and system calls. For this reason, we need the following set of changes:\n  first, we want to channel IRQ events to the interrupt pipeline, instead of delivering IRQs directly to the original low-level in-band handler (e.g. handle_arch_irq() for ARM). With this change in, the pipeline can dispatch events immediately to out-of-band handlers if any, then conditionally dispatch them to the in-band code too if it accepts interrupts, or defer them until it does. This change is a prerequisite for enabling the interrupt pipeline.\n  because faults/exceptions can happen when running on the out-of-band stage (e.g. some task running out-of-band blunders, causing a memory access violation), returning from a fault handling must skip the epilogue code which checks for in-band-specific conditions, such as opportunities for userr task rescheduling or/and kernel preemption.\n  system calls are a particular type of synchronous traps, voluntarily triggerred by application code in order to have the kernel perform some action for them. Dovetail routes system calls issued by tasks for which alternate scheduling is enabled to the companion core. Most architectures Dovetail supports handle system calls from C code these days (such as x86 and arm64). A few may still do this from a low level assembly section though (e.g. ARM), in which case Dovetail\u0026rsquo;s routing logic takes place from there. There is a clear trend in the mainline kernel to move most syscall handling code from assembly to a (generic) C implementation, which Dovetail benefits from already.\n   Last modified: Wed, 30 Sep 2020 16:01:08 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Generic requirements The interrupt pipeline requires the following features to be available from the target Linux kernel:\n  Generic IRQ handling (CONFIG_GENERIC_IRQ) and IRQ domains (CONFIG_IRQ_DOMAIN), which most architectures should support these days.\n  Generic clock event abstraction (CONFIG_GENERIC_CLOCKEVENTS).\n  Generic clock source abstraction (!CONFIG_ARCH_USES_GETTIMEOFFSET).\n  Other assumptions ARM   a target ARM machine port must be allowed to specify its own IRQ handler at run time (CONFIG_MULTI_IRQ_HANDLER).\n  only armv6 CPUs and later are supported, excluding older generations of ARM CPUs. Support for ASID (CONFIG_CPU_HAS_ASID) is required.\n  machine does not have VIVT cache.\n  armv5 is not supported due to the use of VIVT caches on these CPUs, which don\u0026rsquo;t cope well - at all - with low latency requirements. A work aimed at leveraging the legacy FCSE PID register for reducing the cost of cache invalidation in context switches has been maintained until 2013 by Gilles Chanteperdrix, as part of the legacy I-pipe project, Dovetail\u0026rsquo;s ancestor. This work can still be cloned from this GIT repository.\n  Last modified: Tue, 26 Jun 2018 19:27:55 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/irqflow/",
	"title": "Interrupt flow",
	"tags": [],
	"description": "",
	"content": "Adapting the generic interrupt management (genirq) Interrupt pipelining involves a basic change in controlling the interrupt flow: handle_domain_irq() from the IRQ domain API redirects all parent IRQs to the pipeline entry by calling generic_pipeline_irq(), instead of generic_handle_irq().\nGeneric flow handlers acknowledge the incoming IRQ event in the hardware as usual, by calling the appropriate irqchip routine (e.g. irq_ack(), irq_eoi()) according to the interrupt type. However, the flow handlers do not immediately invoke the in-band interrupt handlers. Instead, they hand the event over to the pipeline core by calling handle_oob_irq().\nIf an out-of-band handler exists for the interrupt received, handle_oob_irq() invokes it immediately, after switching the execution context to the oob stage if not current yet. Otherwise, the event is marked as pending in the in-band stage\u0026rsquo;s log for the current CPU.\nThe execution flow throughout the kernel code in Dovetail\u0026rsquo;s pipelined interrupt model is illustrated by the following figure. Note the two-step process: first we try delivering the incoming IRQ to any out-of-band handler if present, then we may play any IRQ pending in the current per-CPU log, among which non-OOB events may reside.\nAs illustrated above, interrupt flow handlers may run twice for a single IRQ in Dovetail\u0026rsquo;s pipelined interrupt model:\n  first to submit the event immediately to any out-of-band handler which may be interested in it. This is achieved by calling handle_oob_irq(), whose role is to invoke such handler(s) if present, or schedule an in-band handling of the IRQ event if not.\n  finally to run the in-band handler(s) accepting the IRQ event if it was not delivered to any out-of-band handler. To deliver the event to any in-band handler(s), the interrupt flow handler is called again by the pipeline core. When this happens, the flow handler processes the interrupt as usual, skipping the call to handle_oob_irq() though.\n  Any incoming IRQ event is either dispatched to one or more out-of-band handlers, or one or more in-band handlers, but never to a mix of them. Also, because every interrupt which is not handled by an out-of-band handler will end up into the in-band stage\u0026rsquo;s event log unconditionally, all external interrupts must have a handler in the in-band code - which should be the case for a sane kernel anyway.\n Once generic_pipeline_irq() has returned, if the preempted execution context was running over the in-band stage unstalled, the pipeline core synchronizes the interrupt state immediately, meaning that all IRQs found pending in the in-band stage\u0026rsquo;s log are immediately delivered to their respective in-band handlers. In all other situations, the IRQ frame is left immediately without running those handlers. The IRQs may remain pending until the in-band code resumes from preemption, then clears the virtual interrupt disable flag, which would cause the interrupt state to be synchronized, running the in-band handlers eventually.\nIn-band IRQ delivery glue code For delivering pending interrupts to the in-band stage, the generic Dovetail core synchronizing the IRQ stage calls a routine named arch_do_IRQ_pipelined(), which you must provide as part of the pipeline\u0026rsquo;s arch-specific support code. This function is passed both device IRQs and IPIs, it should dispatch the event accordingly according to the following logic:\ngraph LR; S(IRQ stage sync) -- E[\"arch_do_IRQ_pipelined()\"] style E fill:#99ccff; E -- A{Is IPI?} style A fill:#99ccff; A --|Yes| B[call IPI handler] style B fill:#99ccff; A --|No| C[\"do_domain_irq()\"] style C fill:#99ccff;  For ARM and ARM64, the corresponding code looks like this:\nvoid arch_do_IRQ_pipelined(struct irq_desc *desc) { struct pt_regs *regs = raw_cpu_ptr(\u0026amp;irq_pipeline.tick_regs); unsigned int irq = irq_desc_get_irq(desc); #ifdef CONFIG_SMP /* * Check for IPIs, handing them over to the specific dispatch * code. */ if (irq \u0026gt;= OOB_IPI_BASE \u0026amp;\u0026amp; irq \u0026lt; OOB_IPI_BASE + NR_IPI + OOB_NR_IPI) { __handle_IPI(irq - OOB_IPI_BASE, regs); return; } #endif do_domain_irq(irq, regs); } A couple of notes reading this code:\n  do_domain_irq() is a routine the generic Dovetail core implements, which fires the in-band handler for a device IRQ.\n  How IPIs differentiate from other IRQs, which handler should be called for them is an arch-specific implementation you should provide in porting Dovetail. In the code example above, the IPI handling routine is named __handle_IPI().\n  Since the interrupt delivery is deferred for the in-band stage until the latter is synchronized eventually, we don\u0026rsquo;t have access to the preempted register frame for a delayed interrupt event. Said differently, the interrupt context has already returned, only logging the interrupt event but not dispatching it yet, and the stack-based register frame of the preempted context is long gone. Fortunately, the kernel is normally only interested in analyzing frames attached to timer events (e.g. for profiling), so Dovetail only needs to save the register frame corresponding to the last tick event received to the per-CPU irq_pipeline.tick_regs variable. A pointer to such frame for the current CPU can be passed by your implementation of arch_do_IRQ_pipelined() to the interrupt handler.\n  Deferring level-triggered IRQs In absence of any out-of-band handler for the event, the device may keep asserting the interrupt signal until the cause has been lifted in its own registers. At the same time, we might not be allowed to run the in-band handler immediately over the current interrupt context if the in-band stage is currently stalled, we would have to wait for the in-band code to accept interrupts again. However, the interrupt disable bit in the CPU would certainly be cleared in the meantime. For this reason, depending on the interrupt type, the flow handlers as modified by the pipeline code may have to mask the interrupt line until the in-band handler has run from the in-band stage, lifting the interrupt cause. This typically happens with level-triggered interrupts, preventing the device from storming the CPU with a continuous interrupt request.\n The pathological case\n /* no OOB handler, in-band stage stalled on entry leading to deferred dispatch to handler */ asm_irq_entry ... -\u0026gt; generic_pipeline_irq() ... \u0026lt;IRQ logged, delivery deferred\u0026gt; asm_irq_exit /* * CPU allowed to accept interrupts again with IRQ cause not * acknowledged in device yet =\u0026gt; **IRQ storm**. */ asm_irq_entry ... asm_irq_exit asm_irq_entry ... asm_irq_exit Since all of the IRQ handlers sharing an interrupt line are either in-band or out-of-band in a mutually exclusive way, such masking cannot delay out-of-band events.\nThe logic behind masking interrupt lines until events are processed at some point later - out of the original interrupt context - applies exactly the same way to the threaded interrupt model (i.e. IRQF_THREAD). In this case, interrupt lines may be masked until the IRQ thread is scheduled in, after the interrupt handler clears the event cause eventually.\n Adapting the interrupt flow handlers to pipelining The logic for adapting flow handlers dealing with interrupt pipelining is composed of the following steps:\n  (optionally) enter the critical section protected by the IRQ descriptor lock, if the interrupt is shared among processors (e.g. device interrupts). If so, check if the interrupt handler may run on the current CPU (irq_may_run()). By definition, no locking would be required for per-CPU interrupts.\n  check whether we are entering the pipeline in order to deliver the interrupt to any out-of-band handler registered for it. on_pipeline_entry() returns a boolean value denoting this situation.\n  if on pipeline entry, we should pass the event on to the pipeline core by calling handle_oob_irq(). Upon return, this routine tells the caller whether any out-of-band handler was fired for the event.\n  if so, we may assume that the interrupt cause is now cleared in the device, and we may leave the flow handler, after having restored the interrupt line into a normal state. In case of a level-triggered interrupt which has been masked on entry to the flow handler, we need to unmask the line before leaving.\n  if no out-of-band handler was called, we should have performed any acknowledge and/or EOI to release the interrupt line in the controller, while leaving it masked if required before exiting the flow handler. In case of a level-triggered interrupt, we do want to leave it masked for solving the pathological case with interrupt deferral explained earlier.\n    if not on pipeline entry (i.e. second entry of the flow handler), then we must be running over the in-band stage, accepting interrupts, therefore we should fire the in-band handler(s) for the incoming event.\n   Example: adapting the handler dealing with level-triggered IRQs\n --- a/kernel/irq/chip.c +++ b/kernel/irq/chip.c void handle_level_irq(struct irq_desc *desc) { raw_spin_lock(\u0026amp;desc-\u0026gt;lock); mask_ack_irq(desc); if (!irq_may_run(desc)) goto out_unlock; +\tif (on_pipeline_entry()) { +\tif (handle_oob_irq(desc)) +\tgoto out_unmask; +\tgoto out_unlock; +\t} + desc-\u0026gt;istate \u0026amp;= ~(IRQS_REPLAY | IRQS_WAITING); /* @@ -642,7 +686,7 @@ void handle_level_irq(struct irq_desc *desc) kstat_incr_irqs_this_cpu(desc); handle_irq_event(desc); - +out_unmask: cond_unmask_irq(desc); out_unlock: raw_spin_unlock(\u0026amp;desc-\u0026gt;lock); } This change reads as follows:\n  on entering the pipeline, which means immediately over the interrupt frame context set up by the CPU for receiving the event, tell the pipeline core about the incoming IRQ.\n  if this IRQ was handled by an out-of-band handler (handle_oob_irq() returns true), consider the event to have been fully processed, unmasking the interrupt line before leaving. We can\u0026rsquo;t do more than this, simply because the in-band kernel code might expect not to receive any interrupt at this point (i.e. the virtual interrupt disable flag might be set for the in-band stage).\n  otherwise, keep the interrupt line masked until handle_level_irq() is called again from a safe context for handling in-band interrupts, at which point the event should be delivered to the in-band interrupt handler of the main kernel. We have to keep the line masked to prevent the IRQ storm which would certainly happen otherwise, since no handler has cleared the cause of the interrupt event in the device yet.\n  Fixing up the IRQ chip drivers We must make sure the following handlers exported by irqchip drivers can operate over the out-of-band context safely:\n irq_mask() irq_ack() irq_mask_ack() irq_eoi() irq_unmask()  For so-called device interrupts, no change is required, because the genirq layer ensures a single CPU at most handles a given IRQ event by holding the per-descriptor irq_desc::lock spinlock across calls to those irqchip handlers, and such lock is automatically turned into an hybrid spinlock when pipelining interrupts. In other words, those handlers are properly serialized, running with interrupts disabled in the CPU as their non-pipelined implementation expects it.\nUnlike for device interrupts, per-CPU interrupt handling does not need to be serialized this way, since by definition, there cannot be multiple CPUs racing for access with such type of events.\nHowever, there might other reasons to fix up some of those handlers:\n  they must not invoke any in-band kernel service, which might cause an invalid context re-entry.\n  there may be inner spinlocks locally defined by some irqchip drivers for serializing access to a common interrupt controller hardware for distinct IRQs which are handled by multiple CPUs concurrently. Adapting such spinlocked sections found in irqchip drivers to support interrupt pipelining may involve converting the related spinlocks to hard spinlocks.\n  Other section of code which were originally serialized by common interrupt disabling may need to be made fully atomic for running consistenly in pipelined interrupt mode. This can be done by introducing hard masking, converting local_irq_save() calls to hard_local_irq_save(), conversely local_irq_restore() to hard_local_irq_restore().\nFinally, IRQCHIP_PIPELINE_SAFE must be added to the struct irqchip::flags member of a pipeline-aware irqchip driver, in order to notify the kernel that such controller can operate in pipelined interrupt mode. Even if you did not introduce any other change to support pipelining, this one is required: it tells the kernel that you did review the code for that purpose.\n Adapting the ARM GIC driver to interrupt pipelining\n --- a/drivers/irqchip/irq-gic.c +++ b/drivers/irqchip/irq-gic.c @@ -93,7 +93,7 @@ struct gic_chip_data { #ifdef CONFIG_BL_SWITCHER -static DEFINE_RAW_SPINLOCK(cpu_map_lock); +static DEFINE_HARD_SPINLOCK(cpu_map_lock); #define gic_lock_irqsave(f)\t\\ raw_spin_lock_irqsave(\u0026amp;cpu_map_lock, (f)) @@ -424,7 +424,8 @@ static const struct irq_chip gic_chip = { .irq_set_irqchip_state\t= gic_irq_set_irqchip_state, .flags\t= IRQCHIP_SET_TYPE_MASKED | IRQCHIP_SKIP_SET_WAKE | -\tIRQCHIP_MASK_ON_SUSPEND, +\tIRQCHIP_MASK_ON_SUSPEND | +\tIRQCHIP_PIPELINE_SAFE, }; void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)  Adapting the BCM2835 pin control driver to interrupt pipelining\n --- a/drivers/pinctrl/bcm/pinctrl-bcm2835.c +++ b/drivers/pinctrl/bcm/pinctrl-bcm2835.c @@ -79,7 +79,7 @@ struct bcm2835_pinctrl { struct gpio_chip gpio_chip; struct pinctrl_gpio_range gpio_range; -\traw_spinlock_t irq_lock[BCM2835_NUM_BANKS]; +\thard_spinlock_t irq_lock[BCM2835_NUM_BANKS]; }; /* pins are just named GPIO0..GPIO53 */ @@ -608,6 +608,7 @@ static struct irq_chip bcm2835_gpio_irq_chip = { .irq_ack = bcm2835_gpio_irq_ack, .irq_mask = bcm2835_gpio_irq_disable, .irq_unmask = bcm2835_gpio_irq_enable, +\t.flags = IRQCHIP_PIPELINE_SAFE, }; In some (rare) cases, we might have a bit more work for adapting an interrupt chip driver. For instance, we might have to convert a sleeping spinlock to a raw spinlock first, so that we can convert the latter to a hard spinlock eventually. Hard spinlocks like raw ones should be manipulated via the raw_spin_lock() API, unlike sleeping spinlocks.\nirq_set_chip() will complain loudly with a kernel warning whenever the irqchip descriptor passed does not bear the IRQCHIP_PIPELINE_SAFE flag and CONFIG_IRQ_PIPELINE is enabled. Take this warning as a sure sign that your port of the IRQ pipeline to the target system is incomplete.\n Kernel preemption control (CONFIG_PREEMPT) When pipelining is enabled, Dovetail ensures preempt_schedule_irq() reconciles the virtual interrupt state - which has not been touched by the assembly level code upon kernel entry - with basic assumptions made by the scheduler core, such as entering with interrupts virtually disabled (i.e. the in-band stage should be stalled).\n Last modified: Sat, 21 Nov 2020 17:55:57 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/atomic/",
	"title": "Atomic operations",
	"tags": [],
	"description": "",
	"content": "The effect of virtualizing interrupt protection must be reversed for atomic helpers everywhere interrupt disabling is needed to serialize callers, regardless of the stage they live on. Typically, the following files are concerned:\n include/asm-generic/atomic.h include/asm-generic/cmpxchg-local.h include/asm-generic/cmpxchg.h  Likewise in the architecture-specific code:\narch/arm/include/asm/atomic.h arch/arm/include/asm/bitops.h arch/arm/include/asm/cmpxchg.h\nThis is required to keep those helpers usable on data which might be accessed from both stages. A common way to revert such virtualization involves substituting calls to the - virtualized - local_irq_save(), local_irq_restore() API with their hard, non-virtualized counterparts.\n Restoring strict serialization for operations on generic atomic counters\n --- a/include/asm-generic/atomic.h +++ b/include/asm-generic/atomic.h @@ -80,9 +80,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ } #define ATOMIC_OP_RETURN(op, c_op)\t\\ @@ -91,9 +91,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = (v-\u0026gt;counter = v-\u0026gt;counter c_op i);\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ } @@ -104,10 +104,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = v-\u0026gt;counter;\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ } Likewise, such operations may exist in architecture-specific code, overriding their generic definitions. For instance, the ARM port defines its own version of atomic operations for which real interrupt protection has to be reinstated:\n Restoring strict serialization for operations on atomic counters for ARM\n --- a/arch/arm/include/asm/atomic.h +++ b/arch/arm/include/asm/atomic.h @@ -168,9 +168,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ @@ -179,10 +179,10 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ val = v-\u0026gt;counter;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -193,10 +193,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ val = v-\u0026gt;counter;\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -206,11 +206,11 @@ static inline int atomic_cmpxchg(atomic_t *v, int old, int new) int ret; unsigned long flags; -\traw_local_irq_save(flags); +\tflags = hard_local_irq_save(); ret = v-\u0026gt;counter; if (likely(ret == old)) v-\u0026gt;counter = new; -\traw_local_irq_restore(flags); +\thard_local_irq_restore(flags); return ret; }  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/arch/",
	"title": "Architecture-specific bits",
	"tags": [],
	"description": "",
	"content": "Interrupt mask virtualization The architecture-specific code which manipulates the interrupt flag in the CPU\u0026rsquo;s state register in arch//include/asm/irqflags.h should be split between real and virtual interrupt control. The real interrupt control operations are inherited from the in-band kernel implementation. The virtual ones should be built upon services provided by the interrupt pipeline core.\n firstly, the original *arch_local_** helpers should be renamed as *native_** helpers, affecting the hardware interrupt state in the CPU. This naming convention is imposed on the architecture code by the generic helpers in _include/asm-generic/irq_pipeline.h_.   Example: introducing the native interrupt state accessors for the ARM architecture\n --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h #if __LINUX_ARM_ARCH__ \u0026gt;= 6 #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_irq_save\\n\u0026quot; \u0026quot;\tcpsid\ti\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { asm volatile( -\t\u0026quot;\tcpsie i\t@ arch_local_irq_enable\u0026quot; +\t\u0026quot;\tcpsie i\t@ native_irq_enable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { asm volatile( -\t\u0026quot;\tcpsid i\t@ arch_local_irq_disable\u0026quot; +\t\u0026quot;\tcpsid i\t@ native_irq_disable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); @@ -69,12 +76,12 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state \u0026amp; disable IRQs */ #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags, temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_save\\n\u0026quot; \u0026quot;\torr\t%1, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %1\u0026quot; : \u0026quot;=r\u0026quot; (flags), \u0026quot;=r\u0026quot; (temp) @@ -87,11 +94,11 @@ static inline unsigned long arch_local_irq_save(void) * Enable IRQs */ #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_enable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_enable\\n\u0026quot; \u0026quot;\tbic\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -103,11 +110,11 @@ static inline void arch_local_irq_enable(void) * Disable IRQs */ #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_disable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_disable\\n\u0026quot; \u0026quot;\torr\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -153,11 +160,11 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state. */ #define arch_local_save_flags arch_local_save_flags -static inline unsigned long arch_local_save_flags(void) +static inline unsigned long native_save_flags(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ local_save_flags\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_save_flags\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } @@ -166,21 +173,28 @@ static inline unsigned long arch_local_save_flags(void) * restore saved IRQ \u0026amp; FIQ state */ #define arch_local_irq_restore arch_local_irq_restore -static inline void arch_local_irq_restore(unsigned long flags) +static inline void native_irq_restore(unsigned long flags) { asm volatile( -\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ local_irq_restore\u0026quot; +\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ native_irq_restore\u0026quot; : : \u0026quot;r\u0026quot; (flags) : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_irqs_disabled_flags arch_irqs_disabled_flags -static inline int arch_irqs_disabled_flags(unsigned long flags) +static inline int native_irqs_disabled_flags(unsigned long flags) { return flags \u0026amp; IRQMASK_I_BIT; } +static inline bool native_irqs_disabled(void) +{ +\tunsigned long flags = native_save_flags(); +\treturn native_irqs_disabled_flags(flags); +} + +#include \u0026lt;asm/irq_pipeline.h\u0026gt; #include \u0026lt;asm-generic/irqflags.h\u0026gt; #endif /* ifdef __KERNEL__ */  finally, a new set of arch_local_* helpers should be provided, affecting the virtual interrupt disable flag implemented by the pipeline core for controlling the in-band stage protection against interrupts. It is good practice to implement this set in a separate file available for inclusion from \u0026lt;asm/irq_pipeline.h\u0026gt;.   Example: providing the virtual interrupt state accessors for the ARM architecture\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h @@ -0,0 +1,138 @@ +#ifndef _ASM_ARM_IRQ_PIPELINE_H +#define _ASM_ARM_IRQ_PIPELINE_H + +#include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; + +#ifdef CONFIG_IRQ_PIPELINE + +static inline notrace unsigned long arch_local_irq_save(void) +{ +\tint stalled = inband_irq_save(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline notrace void arch_local_irq_enable(void) +{ +\tbarrier(); +\tinband_irq_enable(); +} + +static inline notrace void arch_local_irq_disable(void) +{ +\tinband_irq_disable(); +\tbarrier(); +} + +static inline notrace unsigned long arch_local_save_flags(void) +{ +\tint stalled = inband_irqs_disabled(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +static inline notrace void arch_local_irq_restore(unsigned long flags) +{ +\tif (!arch_irqs_disabled_flags(flags)) +\t__inband_irq_enable(); +\tbarrier(); +} + +#else /* !CONFIG_IRQ_PIPELINE */ + +static inline unsigned long arch_local_irq_save(void) +{ +\treturn native_irq_save(); +} + +static inline void arch_local_irq_enable(void) +{ +\tnative_irq_enable(); +} + +static inline void arch_local_irq_disable(void) +{ +\tnative_irq_disable(); +} + +static inline unsigned long arch_local_save_flags(void) +{ +\treturn native_save_flags(); +} + +static inline void arch_local_irq_restore(unsigned long flags) +{ +\tnative_irq_restore(flags); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +#endif /* !CONFIG_IRQ_PIPELINE */ + +#endif /* _ASM_ARM_IRQ_PIPELINE_H */  This new file should include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; early to get access to the pipeline declarations it needs. This inclusion should be unconditional, even if the kernel is built with CONFIG_IRQ_PIPELINE disabled.\n Providing support for merged interrupt states The generic interrupt pipeline implementation requires the arch-level support code to provide for a pair of helpers aimed at translating the virtual interrupt disable flag to the interrupt bit in the CPU\u0026rsquo;s status register (e.g. PSR_I_BIT for ARM) and conversely. These helpers are used to create combined state words merging the virtual and real interrupt states.\n  arch_irqs_virtual_to_native_flags(int stalled) must return a long word remapping the boolean value of @stalled to the CPU\u0026rsquo;s interrupt bit position in the status register. All other bits must be cleared.\n On ARM, this can be expressed as (stalled ? PSR_I_BIT : 0). on x86, that would rather be (stalled ? 0 : X86_EFLAGS_IF).    arch_irqs_native_to_virtual_flags(unsigned long flags) must return a long word remapping the CPU\u0026rsquo;s interrupt bit in @flags to an arbitrary bit position, choosen not to conflict with the former. In other words, the CPU\u0026rsquo;s interrupt state bit received in @flags should be shifted to a free position picked arbitrarily in the return value. All other bits must be cleared.\n  On ARM, using bit position 31 to reflect the virtual state, this is expressed as (hard_irqs_disabled_flags(flags) ? (1 \u0026lt;\u0026lt; 31) : 0).\n  On any other architecture, the implementation would be similar, using whatever bit position is available which would not conflict with the CPU\u0026rsquo;s interrupt bit position.\n    --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h /* * CPU interrupt mask handling. */ #ifdef CONFIG_CPU_V7M #define IRQMASK_REG_NAME_R \u0026quot;primask\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;primask\u0026quot; #define IRQMASK_I_BIT\t1 +#define IRQMASK_I_POS\t0 #else #define IRQMASK_REG_NAME_R \u0026quot;cpsr\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;cpsr_c\u0026quot; #define IRQMASK_I_BIT\tPSR_I_BIT +#define IRQMASK_I_POS\t7 #endif +#define IRQMASK_i_POS\t31  IRQMASK_i_POS (note the minus \u0026lsquo;i\u0026rsquo;) is the free bit position in the combo word where the ARM port stores the original CPU\u0026rsquo;s interrupt state in the combo word. This position can\u0026rsquo;t conflict with IRQMASK_I_POS, which is an alias to PSR_I_BIT (bit position 0 or 7).\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h + +static inline notrace +unsigned long arch_irqs_virtual_to_native_flags(int stalled) +{ +\treturn (!!stalled) \u0026lt;\u0026lt; IRQMASK_I_POS; +} +static inline notrace +unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags) +{ +\treturn (!!hard_irqs_disabled_flags(flags)) \u0026lt;\u0026lt; IRQMASK_i_POS; +}  Once all of these changes are in, the generic helpers from \u0026lt;linux/irqflags.h\u0026gt; such as local_irq_disable() and local_irq_enable() actually refer to the virtual protection scheme when interrupts are pipelined, which eventually allows to implement interrupt deferral for the protected in-band code running over the in-band stage.\n Adapting the assembly code to IRQ pipelining Interrupt entry As generic IRQ handling is a requirement for supporting Dovetail, the low-level interrupt handler living in the assembly portion of the architecture code can still deliver all interrupt events to the original C handler provided by the irqchip driver. That handler should in turn invoke:\n  handle_domain_irq() for parent device IRQs\n  generic_handle_irq() for cascaded device IRQs (decoded from the parent handler)\n  For those routines, the initial task of inserting an interrupt at the head of the pipeline is directly handled from the genirq layer they belong to. This means that there is usually not much to do other than making a quick check in the implementation of the parent IRQ handler in the relevant irqchip driver, applying the rules of thumb carefully.\nOn some ARM platform equipped with a fairly common GIC controller, that would mean inspecting the function gic_handle_irq() for instance.\n  the arch-specific handle_IPI() or equivalent for special inter-processor interrupts  IPIs must be dealt with by specific changes introduced by the port we will cover later.\nInterrupt exit When interrupt pipelining is disabled, the kernel normally runs an epilogue after each interrupt or exception event was handled. If the event happened while the CPU was running some kernel code, the epilogue would check for a potential rescheduling opportunity in case CONFIG_PREEMPT is enabled. If a user-space task was preempted by the event, additional conditions would be checked for such as a signal pending delivery for that task.\nBecause interrupts are only virtually masked for the in-band code when pipelining is enabled, IRQs can still be taken by the CPU and passed on to the low-level assembly handlers, so that they can enter the interrupt pipeline.\n Running the regular epilogue afer an IRQ is valid only if the kernel was actually accepting interrupts when the event happened (i.e. the virtual interrupt disable flag was clear), and running in-band code.\n In all other cases, except for the interrupt pipeline core, the rest of the kernel does not expect those IRQs to ever happen in the first place. Therefore, running the epilogue in such circumstances would be at odds with the kernel\u0026rsquo;s logic. In addition, low-level handlers must have been made aware that they might receive an event under such conditions.\nFor instance, the original ARM code for handling an IRQ which has preempted a kernel context would look like this:\n__irq_svc: svc_entry irq_handler #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count ldr\tr0, [tsk, #TI_FLAGS]\t@ get flags teq\tr8, #0\t@ if preempt count != 0 movne\tr0, #0\t@ force flags to 0 tst\tr0, #_TIF_NEED_RESCHED blne\tsvc_preempt #endif In order to properly handle interrupts in a pipelined delivery model, we have to detect whether the in-band kernel was ready to receive such event, acting upon it accordingly. To this end, the ARM port passes the event to a trampoline routine instead (handle_arch_irq_pipelined()), expecting on return a decision whether or not the epilogue code should run next. In the illustration below, this decision is returned as a boolean status to the caller, non-zero meaning that we may run the epilogue, zero otherwise.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S .macro\tirq_handler #ifdef CONFIG_MULTI_IRQ_HANDLER -\tldr\tr1, =handle_arch_irq mov\tr0, sp badr\tlr, 9997f +#ifdef CONFIG_IRQ_PIPELINE +\tldr\tr1, =handle_arch_irq_pipelined +\tmov\tpc, r1 +#else\t+\tldr\tr1, =handle_arch_irq ldr\tpc, [r1] -#else +#endif +#elif CONFIG_IRQ_PIPELINE +#error \u0026quot;Legacy IRQ handling not pipelined\u0026quot; +#else\tarch_irq_handler_default #endif 9997: .endm The trampoline routine added to the original code, first delivers the interrupt to the machine-defined handler, then tells the caller whether the regular epilogue may run for such event.\n--- a/arch/arm/kernel/irq.c +++ b/arch/arm/kernel/irq.c @@ -112,6 +112,15 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *)) } #endif +#ifdef CONFIG_IRQ_PIPELINE +asmlinkage int __exception_irq_entry +handle_arch_irq_pipelined(struct pt_regs *regs) +{ +\thandle_arch_irq(regs); +\treturn running_inband() \u0026amp;\u0026amp; !irqs_disabled(); +} +#endif + Eventually, the low-level assembly handler receiving the interrupt event is adapted, in order to carry out the earlier decision by handle_arch_irq_pipelined(), skipping the epilogue code if required to.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S __irq_svc: svc_entry irq_handler +#ifdef CONFIG_IRQ_PIPELINE +\ttst\tr0, r0 +\tbeq\t1f +#endif #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count blne\tsvc_preempt #endif +1: svc_exit r5, irq = 1\t@ return from exception  Taking the fast exit path when applicable is critical to the stability of the target system to prevent invalid re-entry of the in-band kernel code.\n Fault exit Similarly to the interrupt exit case, the low-level fault handling code must skip the epilogue code when the fault was taken over an out-of-band context. Upon fault, the current interrupt state is not considered for determining whether we should run the epilogue, since a fault may occur independently of such state.\n Running the regular epilogue after a fault is valid only if that fault was triggered by some in-band code, excluding any fault raised by out-of-band code.\n For instance, the original ARM code for returning from an exception event would be modified as follows:\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S @@ -754,7 +772,7 @@ ENTRY(ret_from_exception) UNWIND(.cantunwind\t) get_thread_info tsk mov\twhy, #0 -\tb\tret_to_user +\tret_to_user_pipelined r1 UNWIND(.fnend\t) With the implementation of ret_to_user_pipelined checking for the current stage, skipping the epilogue if the faulting code was running over an out-of-band context:\n--- a/arch/arm/kernel/entry-header.S +++ b/arch/arm/kernel/entry-header.S +/* + * Branch to the exception epilogue, skipping the in-band work + * if running over the oob interrupt stage. + */ +\t.macro ret_to_user_pipelined, tmp +#ifdef CONFIG_IRQ_PIPELINE +\tldr\t\\tmp, [tsk, #TI_LOCAL_FLAGS] +\ttst\t\\tmp, #_TLF_OOB +\tbne\tfast_ret_to_user +#endif +\tb\tret_to_user +\t.endm + _TLF_OOB is a local thread_info flag denoting a current task running out-of-band code over the out-of-band stage. If set, the epilogue must be skipped.\nReconciling the virtual interrupt state to the epilogue logic A tricky issue to address when pipelining interrupts is about making sure that the logic from the epilogue routine (e.g. do_work_pending(), do_notify_resume()) actually runs in the expected (virtual) interrupt state for the in-band stage.\nReconciling the virtual interrupt state to the in-band logic dealing with interrupts is required because in a pipelined interrupt model, the virtual interrupt state of the in-band stage does not necessarily reflect the CPU\u0026rsquo;s interrupt state on entry to the early assembly code handling the IRQ events. Typically, a CPU would always automatically disable interrupts hardware-wise when taking an IRQ, which may contradict the software-managed virtual state until both are eventually reconciled.\nThose rules of thumb should be kept in mind when adapting the epilogue routine to interrupt pipelining:\n  most often, such routine is supposed to be entered with (hard) interrupts off when called from the assembly code which handles kernel entry/exit transitions (e.g. arch/arm/kernel/entry-common.S). Therefore, this routine may have to reconcile the virtual interrupt state with such expectation, since according to the interrupt exit rules we discussed earlier, such state has to be originally enabled (i.e. the in-band stall bit is clear) for the epilogue code to run in the first place.\n  conversely, we must keep the hard interrupt state consistent upon return from the epilogue code with the one received on entry. Typically, hard interrupts must be disabled before leaving this code if we entered it that way.\n  calling schedule() should be done with IRQs enabled in the CPU, in order to minimize latency for the out-of-band stage (i.e. hard_irqs_disabled() should return false before the call).\n  generally speaking, while we may need the in-band stage to be stalled when the in-band kernel code expects this, we still want most of the epilogue code to run with hard interrupts enabled to shorten the interrupt latency for the out-of-band stage, where the autonomous core lives.\n   Reconciling the interrupt state in ARM64 epilogue\n  #include \u0026lt;linux/errno.h\u0026gt; #include \u0026lt;linux/kernel.h\u0026gt; #include \u0026lt;linux/signal.h\u0026gt; +#include \u0026lt;linux/irq_pipeline.h\u0026gt; #include \u0026lt;linux/personality.h\u0026gt; #include \u0026lt;linux/freezer.h\u0026gt; #include \u0026lt;linux/stddef.h\u0026gt; @@ -915,24 +916,34 @@ static void do_signal(struct pt_regs *regs) asmlinkage void do_notify_resume(struct pt_regs *regs, unsigned long thread_flags) { +\tWARN_ON_ONCE(irq_pipeline_debug() \u0026amp;\u0026amp; +\t(irqs_disabled() || running_oob())); + /* * The assembly code enters us with IRQs off, but it hasn't * informed the tracing code of that for efficiency reasons. * Update the trace code with the current status. */ -\ttrace_hardirqs_off(); +\tif (!irqs_pipelined()) +\ttrace_hardirqs_off(); do { +\tif (irqs_pipelined()) +\tlocal_irq_disable(); + /* Check valid user FS if needed */ addr_limit_user_check(); if (thread_flags \u0026amp; _TIF_NEED_RESCHED) { /* Unmask Debug and SError for the next task */ -\tlocal_daif_restore(DAIF_PROCCTX_NOIRQ); +\tlocal_daif_restore(irqs_pipelined() ? +\tDAIF_PROCCTX : DAIF_PROCCTX_NOIRQ); schedule(); } else { local_daif_restore(DAIF_PROCCTX); +\tif (irqs_pipelined()) +\tlocal_irq_enable(); if (thread_flags \u0026amp; _TIF_UPROBE) uprobe_notify_resume(regs); @@ -950,9 +961,17 @@ asmlinkage void do_notify_resume(struct pt_regs *regs, fpsimd_restore_current_state(); } +\t/* +\t* CAUTION: we may have restored the fpsimd state for +\t* current with no other opportunity to check for +\t* _TIF_FOREIGN_FPSTATE until we are back running on +\t* el0, so we must not take any interrupt until then, +\t* otherwise we may end up resuming with some OOB +\t* thread's fpsimd state. +\t*/ local_daif_mask(); thread_flags = READ_ONCE(current_thread_info()-\u0026gt;flags); -\t} while (thread_flags \u0026amp; _TIF_WORK_MASK); +\t} while (inband_irq_pending() || (thread_flags \u0026amp; _TIF_WORK_MASK)); } Mapping descriptor-less per-CPU IRQs to pipelined IRQ numbers Some architecture ports may not assign interrupt descriptors (i.e. struct irq_desc) to per-CPU interrupts. Instead, those per-CPU events are immediately handled by arch-specific code, instead of being channeled through the common generic IRQ layer for delivery to their respective handler. With Dovetail, we do generally need all interrupts to have a valid interrupt descriptor, including per-CPU events, so that we can request them to be handled on the out-of-band stage using the generic IRQ API, like we would request any regular device interrupt.\nThe easiest way to achieve this mapping is to create a new synthetic interrupt domain for IPIs, which are in essence per-CPU events. Therefore, the flow handler for interrupts from this domain should be handle_synthetic_irq().\nPrior to kernel v5.10-rc6, all Dovetail ports (x86, ARM and arm64) required a synthetic interrupt domain for the purpose of mapping descriptor-less per-CPU interrupts. Since v5.10-rc6 and the introduction of a native mapping of each IPI to a common interrupt descriptor for ARM and arm64, only Dovetail/x86 still needs to implement a specific interrupt domain in order to map the APIC system interrupts.\nDealing with IPIs Although the pipeline does not directly use inter-processor interrupts internally, it provides a simple API to autonomous cores for implementing IPI-based messaging between CPUs. This feature requires the Dovetail port to implement a few bits of architecture-specific code. The arch-specific Dovetail implementation must provide support for two operations:\n  sending the additional IPI signals defined by the Dovetail API using the mechanism available from your hardware for inter-processor messaging, upon request from irq_send_oob_ipi().\n  dispatching deferred IPIs to their respective in-band handler upon request from the Dovetail core.\n  Prior to kernel v5.10-rc6 Until v5.10-rc6, the ARM and arm64 implementations would register IPIs from a synthetic IPI interrupt domain, from IRQ2048 (OOB_IPI_BASE) and on.\nThe arch-specific implementation of handle_IPI() in a Dovetail-enabled kernel should generate IRQs from this IPI domain each time an IPI event is received from the hardware, by calling generic_pipeline_irq().\n IPI domain for ARM\n static struct irq_domain *sipic_domain; static void sipic_irq_noop(struct irq_data *data) { } static unsigned int sipic_irq_noop_ret(struct irq_data *data) { return 0; } static struct irq_chip sipic_chip = { .name\t= \u0026quot;SIPIC\u0026quot;, .irq_startup\t= sipic_irq_noop_ret, .irq_shutdown\t= sipic_irq_noop, .irq_enable\t= sipic_irq_noop, .irq_disable\t= sipic_irq_noop, .irq_ack\t= sipic_irq_noop, .irq_mask\t= sipic_irq_noop, .irq_unmask\t= sipic_irq_noop, .flags\t= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE, }; static int sipic_irq_map(struct irq_domain *d, unsigned int irq, irq_hw_number_t hwirq) { irq_set_percpu_devid(irq); irq_set_chip_and_handler(irq, \u0026amp;sipic_chip, handle_synthetic_irq); return 0; } static struct irq_domain_ops sipic_domain_ops = { .map\t= sipic_irq_map, }; static void create_ipi_domain(void) { /* * Create an IRQ domain for mapping all IPIs (in-band and * out-of-band), with fixed sirq numbers starting from * OOB_IPI_BASE. The sirqs obtained can be injected into the * pipeline upon IPI receipt like other interrupts. */ sipic_domain = irq_domain_add_simple(NULL, NR_IPI + OOB_NR_IPI, OOB_IPI_BASE, \u0026amp;sipic_domain_ops, NULL); } void __init arch_irq_pipeline_init(void) { #ifdef CONFIG_SMP create_ipi_domain(); #endif }  Initializing the IPI domain should be done from the arch_irq_pipeline_init() handler, which Dovetail calls while setting up the interrupt pipelining machinery early at kernel boot.\n Since kernel v5.10-rc6 Since v5.10-rc6, the ARM and arm64 implementations assign a common interrupt descriptor to each IPI, therefore we need no synthetic interrupt domain for that purpose anymore.\nShort of IPI vectors? Multiplex! Your hardware may be limited with respect to the number of distinct IPI signals available from the interrupt controller. Typically, the ARM generic interrupt controller (aka GIC) available with the Cortex CPU series provides 16 distinct IPIs, half of which should be reserved to the firmware, which leaves only 8 IPIs available to the kernel. Since all of them are already in use for in-band work in the mainline implementation, we are short of available IPI vectors for adding the two additional interrupts we need. For this reason, the Dovetail ports for ARM and ARM64 have reshuffled the way IPI signaling is implemented.\nIn the upstream kernel implementation, a 1:1 mapping exists between the logical IPI numbers used by the kernel to refer to inter-processor messages, and the physical, so-called SGI numbers which stands for Software Generated Interrupts:\n   IPI Message Logical IPI number Physical SGI number     IPI_WAKEUP 0 0   IPI_TIMER 1 1   IPI_RESCHEDULE 2 2   IPI_CALL_FUNC 3 3   IPI_CPU_STOP 4 4   IPI_IRQ_WORK 5 5   IPI_COMPLETION 6 6   IPI_CPU_BACKTRACE 7 7    After the introduction of IPI multiplexing by Dovetail, all pre-existing in-band IPIs are now multiplexed over SGI0, which leaves seven SGIs available for adding out-of-band IPI messages, from which we only need two in the current implementation.\nPrior to kernel v5.10-rc6 The resulting mapping is as follows:\n   IPI Message Logical IPI number Physical SGI number Pipelined IRQ number     IPI_WAKEUP 0 0 2048   IPI_TIMER 1 0 2049   IPI_RESCHEDULE 2 0 2050   IPI_CALL_FUNC 3 0 2051   IPI_CPU_STOP 4 0 2052   IPI_IRQ_WORK 5 0 2053   IPI_COMPLETION 6 0 2054   IPI_CPU_BACKTRACE 7 0 2055   TIMER_OOB_IPI 8 1 2056   RESCHEDULE_OOB_IPI 9 2 2057    The implementation of the IPI multiplexing for ARM takes place in arch/arm/kernel/smp.c. The logic - as illustrated below - is fairly straightforward:\n  on the issuer side, if the IPI we need to send belongs to the in-band set (ipinr \u0026lt; NR_IPI), log the pending signal into a per-CPU global bitmask (ipi_messages) then issue SGI0. Otherwise, issue either SGI1 or SGI2 for signaling the corresponding out-of-band IPIs directly.\n  upon receipt, if we received SGI0, iterate over the pending in-band IPIs by reading the per-CPU bitmask (ipi_messages) demultiplexing the logical IPI numbers as we go before pushing the corresponding IRQ event to the pipeline entry, see generic_pipeline_irq(). If SGI1 or SGI2 are received instead, the incoming event is remapped to either TIMER_OOB_IPI or RESCHEDULE_OOB_IPI before it is fed into the pipeline (i.e. IRQ2056 or IRQ2057).\n  static DEFINE_PER_CPU(unsigned long, ipi_messages); static inline void send_IPI_message(const struct cpumask *target, unsigned int ipinr) { unsigned int cpu, sgi; if (ipinr \u0026lt; NR_IPI) { /* regular in-band IPI (multiplexed over SGI0). */ trace_ipi_raise_rcuidle(target, ipi_types[ipinr]); for_each_cpu(cpu, target) set_bit(ipinr, \u0026amp;per_cpu(ipi_messages, cpu)); smp_mb(); sgi = 0; } else\t/* out-of-band IPI (SGI1-2). */ sgi = ipinr - NR_IPI + 1; __smp_cross_call(target, sgi); } static inline void handle_IPI_pipelined(int sgi, struct pt_regs *regs) { unsigned int ipinr, irq; unsigned long *pmsg; if (sgi) {\t/* SGI1-2 */ irq = sgi + NR_IPI - 1 + OOB_IPI_BASE; generic_pipeline_irq(irq, regs); return; } /* In-band IPI (0..NR_IPI - 1) multiplexed over SGI0. */ pmsg = raw_cpu_ptr(\u0026amp;ipi_messages); while (*pmsg) { ipinr = ffs(*pmsg) - 1; clear_bit(ipinr, pmsg); irq = OOB_IPI_BASE + ipinr; generic_pipeline_irq(irq, regs); } } As illustrated in the example of in-band delivery glue code, the ARM ports distinguishes between device IRQs and IPIs based on the pipelined IRQ number, with anything in the range [OOB_IPI_BASE..OOB_IPI_BASE + 10] being dispatched as an IPI to the __handle_IPI() routine.\nSince out-of-band IPI messages are supposed to be exclusively handled by out-of-band handlers, __handle_IPI() is not required to handle them specifically.\nSince kernel v5.10-rc6 The resulting mapping is as follows:\n   IPI Message Logical IPI number Physical SGI number Pipelined IRQ number     IPI_WAKEUP 0 0 ipi_irq_base   IPI_TIMER 1 0 ipi_irq_base   IPI_RESCHEDULE 2 0 ipi_irq_base   IPI_CALL_FUNC 3 0 ipi_irq_base   IPI_CPU_STOP 4 0 ipi_irq_base   IPI_IRQ_WORK 5 0 ipi_irq_base   IPI_COMPLETION 6 0 ipi_irq_base   IPI_CPU_BACKTRACE 7 0 ipi_irq_base   TIMER_OOB_IPI x 1 ipi_irq_base + 1   RESCHEDULE_OOB_IPI x 2 ipi_irq_base + 2    The implementation of the IPI multiplexing for ARM takes place in arch/arm/kernel/smp.c:\n  ipi_irq_base stands for the first logical interrupt number assigned to the series of SGIs. All in-band IPIs are multiplexed over SGI0, therefore all of them are signaled by IRQ #ipi_irq_base.\n  on the issuer side, smp_cross_call() deals with in-band IPIs exclusively, logging them into a per-CPU global bitmask (ipi_messages) before issuing SGI0. irq_send_oob_ipi() sends out-of-band IPIs by triggering SGI1 (TIMER_OOB_IPI) or SGI2 (RESCHEDULE_OOB_IPI).\n  upon receipt of SGI0, the in-band IPI handler (ipi_handler) iterates over the pending in-band IPIs by reading the per-CPU bitmask (ipi_messages) pushing each demultiplexed IRQ event to the pipeline entry, see generic_pipeline_irq(). If SGI1 or SGI2 are received instead, the incoming event is remapped to either TIMER_OOB_IPI or RESCHEDULE_OOB_IPI before it is fed into the pipeline.\n  There is no generic out-of-band IPI handler: since each IPI has a dedicated interrupt descriptor, the out-of-band code may request it directly, installing its own handler by a call to __request_percpu_irq().\n static DEFINE_PER_CPU(unsigned long, ipi_messages); static void smp_cross_call(const struct cpumask *target, unsigned int ipinr) { unsigned int cpu; /* regular in-band IPI (multiplexed over SGI0). */ for_each_cpu(cpu, target) set_bit(ipinr, \u0026amp;per_cpu(ipi_messages, cpu)); wmb(); __smp_cross_call(target, 0); } static irqreturn_t ipi_handler(int irq, void *data) { unsigned long *pmsg; unsigned int ipinr; /* * Decode in-band IPIs (0..MAX_IPI - 1) multiplexed over * SGI0. Out-of-band IPIs (SGI1, SGI2) have their own * individual handler. */ pmsg = raw_cpu_ptr(\u0026amp;ipi_messages); while (*pmsg) { ipinr = ffs(*pmsg) - 1; clear_bit(ipinr, pmsg); __this_cpu_inc(ipi_counts[ipinr]); do_handle_IPI(ipinr); } return IRQ_HANDLED; }  Last modified: Sun, 10 Jan 2021 12:43:10 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/timer/",
	"title": "Tick devices",
	"tags": [],
	"description": "",
	"content": "Proxy tick device The proxy tick device is a synthetic clock event device for handing over the control of the hardware tick device to a high-precision, out-of-band timing logic, which cannot be delayed by the in-band kernel code. With this proxy in place, any out-of-band code can gain control over the timer hardware for carrying out its own timing duties. In the same move, it is required to honor the timing requests received from the in-band timer layer (i.e. hrtimers) since the latter won\u0026rsquo;t be able to program timer events directly into the hardware while the proxy is active.\nIn other words, the proxy tick device shares the functionality of the actual device between the in-band and out-of-band contexts, with only the latter actually programming the hardware.\nAdapting clock chip devices for proxying The proxy tick device borrows a real clock chip device from the in-band kernel, controlling it under the hood while substituting for the current tick device. Clock chips which may be controlled by the proxy tick device need their drivers to be specifically adapted for such use, as follows:\n  clockevents_handle_event() must be substituted to any open-coded invocation of the event handler in the interrupt handler.\n  struct clock_event_device::irq must be properly set to the actual IRQ number signaling an event from this device.\n  struct clock_event_device::features must include CLOCK_EVT_FEAT_PIPELINE.\n  __IRQF_TIMER must be set for the action handler of the timer device interrupt.\n   Adapting the ARM architected timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_arch_timer.c +++ b/drivers/clocksource/arm_arch_timer.c @@ -585,7 +585,7 @@ static __always_inline irqreturn_t timer_handler(const int access, if (ctrl \u0026amp; ARCH_TIMER_CTRL_IT_STAT) { ctrl |= ARCH_TIMER_CTRL_IT_MASK; arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -704,7 +704,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt, static void __arch_timer_setup(unsigned type, struct clock_event_device *clk) { -\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT; +\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE; if (type == ARCH_TIMER_TYPE_CP15) { if (arch_timer_c3stop)  Only oneshot-capable clock event devices can be shared via the proxy tick device.\n  Adapting the ARM Global timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_global_timer.c +++ b/drivers/clocksource/arm_global_timer.c @@ -156,11 +156,11 @@ static irqreturn_t gt_clockevent_interrupt(int irq, void *dev_id) *\tthe Global Timer flag _after_ having incremented *\tthe Comparator register\tvalue to a higher value. */ -\tif (clockevent_state_oneshot(evt)) +\tif (clockevent_is_oob(evt) || clockevent_state_oneshot(evt)) gt_compare_set(ULONG_MAX, 0); writel_relaxed(GT_INT_STATUS_EVENT_FLAG, gt_base + GT_INT_STATUS); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -171,7 +171,7 @@ static int gt_starting_cpu(unsigned int cpu) clk-\u0026gt;name = \u0026quot;arm_global_timer\u0026quot;; clk-\u0026gt;features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT | -\tCLOCK_EVT_FEAT_PERCPU; +\tCLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PIPELINE; clk-\u0026gt;set_state_shutdown = gt_clockevent_shutdown; clk-\u0026gt;set_state_periodic = gt_clockevent_set_periodic; clk-\u0026gt;set_state_oneshot = gt_clockevent_shutdown; @@ -195,11 +195,6 @@ static int gt_dying_cpu(unsigned int cpu) return 0; } @@ -302,8 +307,8 @@ static int __init global_timer_of_register(struct device_node *np) goto out_clk; } -\terr = request_percpu_irq(gt_ppi, gt_clockevent_interrupt, -\t\u0026quot;gt\u0026quot;, gt_evt); +\terr = __request_percpu_irq(gt_ppi, gt_clockevent_interrupt, +\tIRQF_TIMER, \u0026quot;gt\u0026quot;, gt_evt); if (err) { pr_warn(\u0026quot;global-timer: can't register interrupt %d (%d)\\n\u0026quot;, gt_ppi, err); This is another example of adapting an existing clock chip driver for serving out-of-band timing requests, with a subtle change in the way we should test for the current state of the clock device in the interrupt handler:\n  A real/original device (such as the ARM global timer in this example) is switched to reserved mode when the proxy tick driver hands it over to the autonomous core, which is similar to the detached mode in effect. Therefore, the ARM global timer state is always reserved from the standpoint of the kernel when proxied, never oneshot. For this reason, clockevent_state_oneshot() would always lead to false in this case.\n  However, since a real device controlled by the proxy for receiving out-of-band events has to be driven in one-shot mode under the hood, testing for clockevent_state_oob() in addition to clockevent_state_oneshot() guarantees that we do take the branch, setting the comparator register to ULONG_MAX when proxied too.\n  Failing to fix up the way we test for the clock device state would certainly lead to an interrupt storm with any ARM global timer suffering erratum 740657, quickly locking up the board.\n Theory of operations Calling tick_install_proxy() registers an instance of the proxy tick device on each CPU mentioned in the cpumask it receives. This routine is also passed the address of a routine which should setup the given struct clock_proxy_device descriptor for the current CPU. This routine is called indirectly by tick_install_proxy(), for each CPU marked in cpumask.\n Initializing the proxy descriptor\n tick_install_proxy() prepares the new proxy device for the current CPU, pre-initializing it with settings compatible with the real device\u0026rsquo;s it interposes on, then calls the setup_proxy() routine. The descriptor is defined as follows:\nstruct clock_proxy_device { struct clock_event_device proxy_device; struct clock_event_device *real_device; void (*handle_oob_event)(struct clock_event_device *dev); /* Internal data - don't depend on this. */ void (*__setup_handler)(struct clock_proxy_device *dev); void (*__original_handler)(struct clock_event_device *dev); }; The user setup call must at least set the .handle_oob_event() handler: this is the address of the routine which should be called each time an out-of-band tick is received from the underlying timer hardware the proxy controls. This is the only information required from the setup handler, the rest may be inherited from the pre-set data if the user does not need any particular setting.\nIf the user code proxying the tick device prefers dealing with nanoseconds instead of clock ticks directly, CLOCK_EVT_FEAT_KTIME should be added to proxy_device.features, along with a valid proxy_device.set_next_ktime() handler and proper min/max delta values.\n A setup_proxy() routine preparing a proxy device\n static DEFINE_PER_CPU(struct clock_proxy_device, tick_device); static void oob_event_handler(struct clock_event_device *dev) { /* * We are running on the out-of-band stage, in NMI-like mode. * Schedule a tick on the proxy device to satisfy the * corresponding timing request asap. */ tick_notify_proxy(); } static void setup_proxy(struct clock_proxy_device *dev) { * Create a proxy which acts as a transparent device, simply * relaying the timing requests to the in-band code, without * any additional out-of-band processing. */ dev-\u0026gt;handle_oob_event = oob_event_handler; }  The proxy_device.set_next_event() or proxy_device.set_next_ktime() members can be set with the address of a handler which receives timer requests from the in-band kernel. This handler is normally implemented by the autonomous core which takes control over the timer hardware via the proxy device. Whenever that core determines that a tick is due for an outstanding request received from such handler, it should call tick_notify_proxy() to signal the event to the main kernel.\n Once the user-supplied setup_proxy() routine returns, the following events happen in sequence:\n  the proxy device is registered on the clock event framework.\n  the real device is detached from the clockevent framework, switched to the CLOCK_EVT_STATE_RESERVED state, which makes it non-eligible for any regular operation from the framework. However, its hardware is left in a functional state. In the same move, the proxy device is picked as the new tick device by the framework. From that point, requests to the proxy device may be indirectly channeled to the real device via the proxy when operations on the hardware should be carried out.\n  the proxy device now controls the real device under the hood to carry out timing requests from the in-band kernel. When the hrtimer layer from the in-band kernel wants to program the next shot of the current tick device, it invokes the set_next_event() handler of the proxy device, which was defined by the user (which defaults to the real device\u0026rsquo;s set_next_event() handler). If the autonomous core implements its own timer management, this handler should be scheduling in-band ticks at the requested time based on such scheme.\n  the timer interrupt triggered by the real device is switched to out-of-band handling. As a result, handle_oob_event() receives tick events sent by the real device hardware directly from the out-of-band stage of the interrupt pipeline. This ensures high-precision timing, which the in-band stage cannot delay via interrupt masking. From that point, the out-of-band code can carry out its own timing duties, in addition to honoring the in-band kernel requests for timing.\n  Step 3. involves emulating ticks scheduled by the in-band kernel by a software logic controlled by some out-of-band timer management, paced by the real ticks received as described in step 4. When this logic decides than the next in-band tick is due, it should call tick_notify_proxy() to trigger the corresponding event for the in-band kernel, which would honor the pending (hr)timer request.\n Under the hood\n \t[in-band timing request] proxy_dev-\u0026gt;set_next_event(proxy_dev) oob_program_event(proxy_dev) real_dev-\u0026gt;set_next_event(real_dev) ... \u0026lt;tick event\u0026gt; inband_event_handler() [out-of-band stage] clockevents_handle_event(real_dev) handle_oob_event(proxy_dev) ...(in-band tick emulation)... tick_notify_proxy() ... proxy_irq_handler(proxy_dev) [in-band stage] clockevents_handle_event(proxy_dev) inband_event_handler(proxy_dev)  Last modified: Tue, 26 Jun 2018 19:27:55 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/clocksource/",
	"title": "Reading clock sources",
	"tags": [],
	"description": "",
	"content": "Your autonomous core most likely needs a fast access to the current clock source from the out-of-band context, for reading precise timestamps which are in sync with the kernel\u0026rsquo;s idea of time. The best way to achieve this is by enabling the fast clock_gettime(3) helper in the vDSO support for the target CPU architecture. At least, you may want user-space tasks controlled by the core to have access to the POSIX-defined CLOCK_MONOTONIC and CLOCK_REALTIME clocks from the out-of-band context, using a vDSO call, with no execution and response time penalty involved in invoking an [in-band syscall] (/dovetail/altsched/#inband-switch).\nReading timestamps via the vDSO in a nutshell Basically, A vDSO-based clock_gettime(3) implementation wants to read from non-privileged CPU mode the same monotonic hardware counter which is currently used by the kernel for timekeeping, before converting the count to nanoseconds. In other words, this implementation should mirror the kernel clocksource behavior. However, it should do so relying exclusively on resources which may be accessed from user mode, since the vDSO code segment containing this helper is merely a kernel-defined extension of the application code running in non-privileged mode. In order to perform such conversion, the vDSO code also needs additional timekeeping information maintained by the kernel, which it usually gets from a small data segment the kernel maps into every application as part of the vDSO support (see the various update_vsyscall() implementations for more on this).\nThere are two common ways of reading the hardware counter used for timekeeping by the kernel from a non-privileged environement:\n  either by reading some non-privileged on-chip register (e.g. powerpc\u0026rsquo;s timebase register, or x86\u0026rsquo;s TSC register).\n  or by reading a memory-mapped counter, from a memory mapping the calling application has access to.\n  The ARM situation For several CPU architectures Linux supports, reading the CLOCK_MONOTONIC and CLOCK_REALTIME clocks is already possible via vDSO calls, including from tasks running out-of-band without incurring any execution stage switch.\nFor ARM, the situation is clumsy: the mainline kernel implementation supports reading timestamps directly from the vDSO only for the so-called architected timer the armv8 specification requires from compliant CPUs. With CPUs following an earlier specification, a truckload of different hardware chips may be used for that purpose instead, which the vDSO implementation does not provide any support for. Sometimes, the architected timer is present but not usable for timekeeping duties because of firmware issues. In these cases, a plain in-band system call would be issued whenever the vDSO-based clock_gettime(3) is called from an application, which would be a showstopper for keeping the response time short and bounded.\nThe generic vDSO and USER_MMIO clock sources If your target kernel supports the generic vDSO implementation (CONFIG_HAVE_GENERIC_VDSO), Dovetail already provides the core support for accessing MMIO-based clock sources from the vDSO, which is essentially part of the application context:\n  firstly, it extends the clocksource_mmio semantics with the MMIO-accessed clock sources mapped to user space aka struct clocksource_user_mmio, implemented in drivers/clocksource/mmio.c, which we call USER_MMIO for short in this document. In essence, a USER_MMIO clock source is a MMIO-based clock source which any application may map into its own address space, so that it can read the hardware counter directly. Specifically, the MMIO space covering the hardware counter register(s) is mapped into the caller\u0026rsquo;s address space.\n  Secondly, Dovetail extends the generic vDSO implementation in lib/vdso.c and kernel/time/vsyscall.c in order to make USER_MMIO clock sources visible to applications, in addition to the architected timer if present.\n  Finally, Dovetail converts some MMIO clock sources to USER_MMIO clock sources, such as the OMAP2 general-purpose timer, the ARM global timer counter and the DesignWare APB timer. More conversions will come over time, as Dovetail is ported to a broader range of hardware.\n  The mapping operation happens once in a process lifetime, during the very first call to clock_gettime(3) issued by the application. Since this involves running in-band code for updating the caller\u0026rsquo;s address space, this particular call gives absolutely no response time guarantee. So it\u0026rsquo;s good practice to force an initial dummy call to clock_gettime(3) from the library code which initializes the interface between applications and your autonomous core (i.e. some user-space library which implements the out-of-band system calls wrappers to send requests to this core). For EVL, this is done in libevl.\n Making a MMIO clock source accessible from the vDSO If you need to convert an existing MMIO clock source to a user-mappable one visible from the generic vDSO, you can follow this three-step process:\n  in the Kconfig stanza enabling the clock source, select [CONFIG_]GENERIC_CLOCKSOURCE_VDSO in the dependency list to compile the required support in the generic vDSO code, which in turn selects the USER_MMIO support it depends on.\n  in the clock source implementation, convert the struct clocksource descriptor to a struct clocksource_user_mmio descriptor. The original struct clocksource object is now available as a member of the struct clocksource_user_mmio descriptor, so you may have to move the original initializers accordingly. You also need to fix up the clock source\u0026rsquo;s .read() handler, changing it to one of the helpers clocksource_user_mmio knows about. Do not use any other helper outside of the following set, or you would receive -EINVAL from clocksource_user_mmio_init():\n  clocksource_mmio_readl_up(), for reading a 32bit count-up register (i.e. reg_higher is NULL in clocksource_mmio_regs as described below).\n  clocksource_mmio_readl_down(), for reading a 32bit count-down register.\n  clocksource_mmio_readw_up(), for reading a 16bit count-up register.\n  clocksource_mmio_readw_down(), for reading a 16bit count-down register.\n  clocksource_dual_mmio_readl_up(), for reading a count-up counter composed of two 32bit registers (i.e. both reg_lower and reg_higher must be valid in clocksource_mmio_regs as described below).\n  clocksource_dual_mmio_readl_down(), for reading a count-down counter composed of two 32bit registers.\n    Only continuous clock sources can be converted to clocksource_user_mmio, otherwise the registration fails with -EINVAL in clocksource_user_mmio_init(). Therefore, only clock sources originally bearing the CLOCK_SOURCE_IS_CONTINUOUS flag can be converted.\n   eventually, substitute the call to clocksource_register_hz() by a call to clocksource_user_mmio_init() instead. This function takes the following arguments:\n  the USER_MMIO descriptor address\n  the address of a clocksource_mmio_regs structure which defines the method and parameters for reading the hardware counter. Such counter can be represented by up to two 32bit MMIO registers, making a 64bit value. A lower precision is acceptable too, the vDSO code deals with wrapping as needed. However, the higher the precision, the better the accuracy for applications. The clocksource_mmio_regs structure should be filled with the following information:\n - _reg\\_lower_ is the **virtual** address of the counter's low 32bit register. This address was most likely obtained from `ioremap()` in the original clock source driver code; it cannot be NULL.    bits_lower is a bitmask defining the significant bits to read from the low register, starting from the low order bit. For instance, if the first 31 bits only are significant, 0x7fffffff should be passed.\n  reg_higher is the virtual address of the counter\u0026rsquo;s high register. This address can be NULL if the hardware counter is only 32bit wide or less, in which case bits_higher is ignored too.\n  bits_higher is a bitmask defining the significant bits to read from the high register.\n  revmap is a reverse mapping helper, for resolving the physical address of the low and high registers mentioned above, based on the virtual address passed in reg_lower and reg_higher. If revmap is NULL, clocksource_user_mmio_init() tries to figure this out by resolving the address of the containing memory frame via a call to find_vma(), which is usually fine. If this resolution should be done in a different way, you should specify your own handler in revmap, which receives the virtual address to resolve, and should return the corresponding physical address, or zero upon failure.\n    the hardware clock rate that was originally passed to clocksource_register_hz().\n    Example: converting the OMAP2 GP-timer The Beaglebone Black is an AM335X processor equipped with a Cortex A8 CPU, therefore no ARM architected timer is available. Instead, Linux runs one of the available general purpose timers on this platform for timekeeping purpose. The clock source driver for such devices is implemented in arch/arm/mach-omap2/timer.c. The patch below illustrates the changes Dovetail introduces to convert this clock source to a user-mappable one, which the ARM vDSO implementation can use.\n--- a/arch/arm/mach-omap2/Kconfig +++ b/arch/arm/mach-omap2/Kconfig @@ -96,6 +96,7 @@ config ARCH_OMAP2PLUS select ARCH_HAS_HOLES_MEMORYMODEL select ARCH_OMAP select CLKSRC_MMIO +\tselect GENERIC_CLOCKSOURCE_VDSO select GENERIC_IRQ_CHIP select GPIOLIB select MACH_OMAP_GENERIC diff --git a/arch/arm/mach-omap2/timer.c b/arch/arm/mach-omap2/timer.c index 69c3a6d94933..bc7d177759c3 100644 --- a/arch/arm/mach-omap2/timer.c +++ b/arch/arm/mach-omap2/timer.c @@ -413,17 +413,14 @@ static bool use_gptimer_clksrc __initdata; /* * clocksource */ -static u64 clocksource_read_cycles(struct clocksource *cs) -{ -\treturn (u64)__omap_dm_timer_read_counter(\u0026amp;clksrc, -\tOMAP_TIMER_NONPOSTED); -} -static struct clocksource clocksource_gpt = { -\t.rating\t= 300, -\t.read\t= clocksource_read_cycles, -\t.mask\t= CLOCKSOURCE_MASK(32), -\t.flags\t= CLOCK_SOURCE_IS_CONTINUOUS, +static struct clocksource_user_mmio clocksource_gpt = { +\t.mmio.clksrc = { +\t.rating\t= 300, +\t.read\t= clocksource_mmio_readl_up, +\t.mask\t= CLOCKSOURCE_MASK(32), +\t.flags\t= CLOCK_SOURCE_IS_CONTINUOUS, +\t}, }; static u64 notrace dmtimer_read_sched_clock(void) @@ -505,21 +502,22 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id, const char *fck_source, const char *property) { +\tstruct clocksource_mmio_regs mmr; int res; clksrc.id = gptimer_id; clksrc.errata = omap_dm_timer_get_errata(); res = omap_dm_timer_init_one(\u0026amp;clksrc, fck_source, property, -\t\u0026amp;clocksource_gpt.name, +\t\u0026amp;clocksource_gpt.mmio.clksrc.name, OMAP_TIMER_NONPOSTED); if (soc_is_am43xx()) { -\tclocksource_gpt.suspend = omap2_gptimer_clksrc_suspend; -\tclocksource_gpt.resume = omap2_gptimer_clksrc_resume; +\tclocksource_gpt.mmio.clksrc.suspend = omap2_gptimer_clksrc_suspend; +\tclocksource_gpt.mmio.clksrc.resume = omap2_gptimer_clksrc_resume; clocksource_gpt_hwmod = -\tomap_hwmod_lookup(clocksource_gpt.name); +\tomap_hwmod_lookup(clocksource_gpt.mmio.clksrc.name); } BUG_ON(res); @@ -529,12 +527,18 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id, OMAP_TIMER_NONPOSTED); sched_clock_register(dmtimer_read_sched_clock, 32, clksrc.rate); -\tif (clocksource_register_hz(\u0026amp;clocksource_gpt, clksrc.rate)) +\tmmr.reg_lower = clksrc.func_base + (OMAP_TIMER_COUNTER_REG \u0026amp; 0xff); +\tmmr.bits_lower = 32; +\tmmr.reg_upper = 0; +\tmmr.bits_upper = 0; +\tmmr.revmap = NULL; + +\tif (clocksource_user_mmio_init(\u0026amp;clocksource_gpt, \u0026amp;mmr, clksrc.rate)) pr_err(\u0026quot;Could not register clocksource %s\\n\u0026quot;, -\tclocksource_gpt.name); +\tclocksource_gpt.mmio.clksrc.name); else pr_info(\u0026quot;OMAP clocksource: %s at %lu Hz\\n\u0026quot;, -\tclocksource_gpt.name, clksrc.rate); +\tclocksource_gpt.mmio.clksrc.name, clksrc.rate); }  In some rare cases, converting the available clocksource(s) so that we can read them directly from the vDSO might not be an option, typically because reading them would require supervisor privileges in the CPU, which the vDSO context excludes by definition. For these almost desperate situations, there is still the option for your companion core to intercept system calls to clock_gettime(3) from the out-of-band handler, handling them directly from that spot. This would be slower compared to a direct readout from the vDSO, but the core would manage to get timestamps for CLOCK_MONOTONIC and CLOCK_REALTIME clocks at least without involving the in-band stage. EVL solves a limitation with clock sources on legacy x86 hardware this way.\n  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/syscall/",
	"title": "Syscall path",
	"tags": [],
	"description": "",
	"content": " Last modified: Sun, 08 Mar 2020 13:06:41 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/rawprintk/",
	"title": "Raw printk support",
	"tags": [],
	"description": "",
	"content": "Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, the output is deferred until the in-band stage gets back in control, which means that:\n  you can\u0026rsquo;t reliably trace out-of-band code on the spot, deferred output issued from an out-of-band context, or from a section of code running with interrupts disabled in the CPU may appear after subsequent in-band messages under some circumstances, due to a buffering effect.\n  if the debug traces are sent at high pace (e.g. from an out-of-band IRQ handler every few hundreds of microseconds), the machine is likely to come to a stall due to the massive output the heavy printk() machinery would have to handle, leading to an apparent lockup.\n  The only sane option for printk-like debugging in demanding out-of-band context is using the raw_printk() routine for issuing raw debug messages to a serial console, so that you may get some sensible feedback for understanding what is going on with the execution flow. This feature should be enabled by turning on CONFIG_RAW_PRINTK, otherwise all output sent to raw_printk() is discarded.\nBecause a stock serial console driver won\u0026rsquo;t be usable from out-of-band context, enabling raw printk support requires adapting the serial console driver your platform is using, by adding a raw write handler to the console description. Just like the write() handler, the write_raw() output handler receives a console pointer, the character string to output and its length as parameters. This handler should send the characters to the UART as quickly as possible, with little to no preparation.\nAll output formatted by the generic raw_printk() routine is passed to the raw write handler of the current serial console driver if present. Calls to the raw output handler are serialized in raw_printk() by holding a hard spinlock, which means that interrupts are disabled in the CPU when running the handler.\nA raw write handler is normally derived from the regular write handler for the same serial console device, skipping any in-band locking construct, only waiting for the bare minimum time for the output to drain in the UART since we want to keep interrupt latency low.\nYou cannot expect mixed output sent via printk() then raw_printk() to appear in the same sequence as their respective calls: normal printk() output may be deferred for an undefined amount of time until some console driver sends it to the terminal device, which may involve a task rescheduling. On the other hand, raw_printk() immediately writes the output to the hardware device, bypassing any buffering from printk(). So the output from a sequence of printk() followed by raw_printk() may appear in the opposite order on the terminal device. The converse never happen though.\n  Adding RAW_PRINTK support to the AMBA PL011 serial driver\n --- a/drivers/tty/serial/amba-pl011.c +++ b/drivers/tty/serial/amba-pl011.c @@ -2206,6 +2206,40 @@ static void pl011_console_putchar(struct uart_port *port, int ch) pl011_write(ch, uap, REG_DR); } +#ifdef CONFIG_RAW_PRINTK + +/* + * The uart clk stays on all along in the current implementation, + * despite what pl011_console_write() suggests, so for the time being, + * just emit the characters assuming the chip is clocked. If the clock + * ends up being turned off after writing, we may need to clk_enable() + * it at console setup, relying on the non-zero enable_count for + * keeping pl011_console_write() from disabling it. + */ +static void +pl011_console_write_raw(struct console *co, const char *s, unsigned int count) +{ +\tstruct uart_amba_port *uap = amba_ports[co-\u0026gt;index]; +\tunsigned int old_cr, new_cr, status; + +\told_cr = readw(uap-\u0026gt;port.membase + UART011_CR); +\tnew_cr = old_cr \u0026amp; ~UART011_CR_CTSEN; +\tnew_cr |= UART01x_CR_UARTEN | UART011_CR_TXE; +\twritew(new_cr, uap-\u0026gt;port.membase + UART011_CR); + +\twhile (count-- \u0026gt; 0) { +\tif (*s == '\\n') +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, '\\r'); +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, *s++); +\t} +\tdo +\tstatus = readw(uap-\u0026gt;port.membase + UART01x_FR); +\twhile (status \u0026amp; UART01x_FR_BUSY); +\twritew(old_cr, uap-\u0026gt;port.membase + UART011_CR); +} + +#endif /* !CONFIG_RAW_PRINTK */ + static void pl011_console_write(struct console *co, const char *s, unsigned int count) { @@ -2406,6 +2440,9 @@ static struct console amba_console = { .device\t= uart_console_device, .setup\t= pl011_console_setup, .match\t= pl011_console_match, +#ifdef CONFIG_RAW_PRINTK +\t.write_raw\t= pl011_console_write_raw, +#endif .flags\t= CON_PRINTBUFFER | CON_ANYTIME, .index\t= -1, .data\t= \u0026amp;amba_reg, ARM-specific raw console driver The vanilla ARM kernel port already provides an UART-based raw output routine called printascii() when CONFIG_DEBUG_LL is enabled, provided the right debug UART channel is defined too (CONFIG_DEBUG_UART_xx).\nWhen CONFIG_RAW_PRINTK and CONFIG_DEBUG_LL are both defined in the kernel configuration, the ARM implementation of Dovetail automatically registers a special console device for emitting debug output (see arch/arm/kernel/raw_printk.c), which redirects calls to its raw write handler by raw_printk() to printascii(). In other words, if CONFIG_DEBUG_LL already provides you with a functional debug output channel, you don\u0026rsquo;t need the active serial console driver to implement a raw write handler for enabling raw_printk(), the raw console device should handle raw_printk() requests just fine.\nEnabling CONFIG_DEBUG_LL with a wrong UART debug channel is a common cause of lockup at boot. You do want to make sure the proper CONFIG_DEBUG_UART_xx symbol matching your hardware is selected along with CONFIG_DEBUG_LL.\n  Last modified: Mon, 01 Jun 2020 19:30:22 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/misc/",
	"title": "Misc",
	"tags": [],
	"description": "",
	"content": "printk() support printk() may be called by out-of-band code safely, without encurring extra latency. The output is conveyed like NMI-originated output, which involves some delay until the in-band code resumes, and the console driver(s) can handle it.\nTracing Tracepoints can be traversed by out-of-band code safely. Dynamic tracing is available to a kernel running the pipelined interrupt model too.\n Last modified: Tue, 26 Jun 2018 19:27:55 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/core/user-api/api-revs/",
	"title": "API revisions",
	"tags": [],
	"description": "",
	"content": "You can obtain the current API revision of libevl either at compilation time using the value of the __EVL__ macro defined in the \u0026lt;evl/evl.h\u0026gt; main header file, or dynamically by calling evl_get_version().\nrev. 18 (libevl r26) Introduces the socket interface:\n  oob_recvmsg() to receive a message in out-of-band mode.\n  oob_sendmsg() to send a message in out-of-band mode.\n  The regular socket(2) call as extended by ABI 26 is capable of creating oob-capable sockets when receiving the SOCK_OOB type flag, so there is no EVL-specific call for this operation.\nrev. 17 (libevl r17) Enables HM support for threads. Since ABI 23, the core is able to channel T_WOSS, T_WOLI and T_WOSX error notifications (SIGDEBUG_xxx) through the thread observable component if present. Introduce the T_HMSIG and T_HMOBS mode bits for configuring the HM notification source(s) of a thread with evl_set_thread_mode().\nSIGDEBUG_xxx codes are renamed to EVL_HMDIAG_xxx diag codes, so that we have a single nomenclature for these errors regardless of whether threads are notified via SIGDEBUG or their observable component.\nrev. 16 (libevl r17) Introduces the API changes for supporting the new Observable element:\n  adds evl_subscribe() and evl_unsubscribe() to the thread API.\n  adds the evl_create_observable(), evl_update_observable() and evl_read_observable() services for the new Observable API.\n  allows to pass an opaque data to evl_add_pollfd() and evl_mode_pollfd(), which is returned into the struct evl_poll_event descriptor.\n  rev. 15 (libevl r16) Adds evl_set_thread_mode() and evl_clear_thread_mode().\nrev. 14 Adds evl_unblock_thread() and evl_demote_thread().\nrev. 13 Adds evl_yield().\nrev. 12 (libevl r15) Element visibility is introduced, as a result:\n  Most element classes provides a new long-form evl_create_*() call, in order to receive creation flags. Currently, the visibility attribute of elements is the only flag supported (see EVL_CLONE_PRIVATE, EVL_CLONE_PUBLIC). The additional creation calls are evl_create_event(), evl_create_flags(), evl_create_mutex(), evl_create_proxy(), evl_create_sem() and evl_create_xbuf(). Likewise, the new evl_attach_thread() and evl_detach_thread() calls receive attachment and detachment flags for threads. evl_attach_self() is now equivalent to attaching a private thread by default, unless the thread name says otherwise. evl_detach_self() is unchanged.\n  All evl_new_*() calls become shorthands to their respective evl_create_*() counterparts, picking reasonable default creation parameters for the new element, including private visibility (unless overriden by the leading slash rule explained in this document).\n  All long-form evl_new_*_any() calls have been removed from the API. Applications should use the corresponding evl_create_*() call instead.\n  evl_new_proxy() creates a proxy element with no write granularity by default, which caused this this parameter to be dropped from the call signature. Applications should use evl_create_proxy() to specify a non-default granularity.\n  evl_new_xbuf() creates a cross-buffer element with identically sized input and output buffers by default, which caused one of the two size parameters to be dropped from the call signature. Applications should use evl_create_xbuf() to specify distinct sizes for input and output.\n  All former long-form static initializers EVL_*_ANY_INITIALIZER() have been renamed EVL_*_INITIALIZER(), dropping the former short-form (if any). For instance, EVL_MUTEX_ANY_INITIALIZER() has been renamed EVL_MUTEX_INITIALIZER(), with an additional parameter for mentioning both the lock type and visibility attribute.\n  Selecting the lock type of a mutex is now done using the evl_create_mutex() call, ORing either EVL_MUTEX_NORMAL or EVL_MUTEX_RECURSIVE into the creation flags. This method replaces the type argument to the former evl_new_mutex_any() call.\n   rev. 11 (libevl r14) For naming consistency, evl_sched_control() was renamed evl_control_sched().\n Last modified: Sat, 21 Aug 2021 18:26:39 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/porting/devnotes/",
	"title": "Developer&#39;s Notes",
	"tags": [],
	"description": "",
	"content": "Generic Fundamentally preemption-safe contexts Over a few contexts, we may traverse code using unprotected, preemption-sensitive accessors such as percpu() without disabling preemption specifically, because either one condition is true;\n  if preempt_count() bears either of the PIPELINE_MASK or STAGE_MASK bits, which turns preemption off, therefore CPU migration cannot happen (debug_smp_processor_id() and preempt checks in percpu accessors would detect such context properly too).\n  if we are running over the context of the in-band stage\u0026rsquo;s event log syncer (sync_current_stage()) playing a deferred interrupt, in which case the virtual interrupt disable bit is set, so no CPU migration may occur either.\n  For instance, the following contexts qualify:\n  clockevents_handle_event(), which should either be called from the oob stage - therefore STAGE_MASK is set - when the [proxy tick device is active] (/dovetail/porting/timer/#proxy-tick-logic) on the CPU, and/or from the in-band stage playing a timer interrupt event from the corresponding device.\n  any IRQ flow handler from kernel/irq/chip.c. When called from generic_pipeline_irq() for pushing an external event to the pipeline, on_pipeline_entry() is true, which indicates that PIPELINE_MASK is set. When called for playing a deferred interrupt on the in-band stage, the virtual interrupt disable bit is set.\n  Checking for out-of-band interrupt property The IRQF_OOB action flag should not be used for testing whether an interrupt is out-of-band, because out-of-band handling may be turned on/off dynamically on an IRQ descriptor using irq_switch_oob(), which would not translate to IRQF_OOB being set/cleared for the attached action handlers.\nirq_is_oob() is the right way to check for out-of-band handling.\nstop_machine() hard disables interrupts The stop_machine() service guarantees that all online CPUs are spinning non-preemptible in a known code location before a subset of them may safely run a stop-context function. This service is typically useful for live patching the kernel code, or changing global memory mappings, so that no activity could run in parallel until the system has returned to a stable state after all stop-context operations have completed.\nWhen interrupt pipelining is enabled, Dovetail provides the same guarantee by restoring hard interrupt disabling where virtualizing the interrupt disable flag would defeat it.\nAs those lines are written, all stop_machine() use cases must also exclude any oob stage activity (e.g. ftrace live patching the kernel code for installing tracepoints), or happen before any such activity can ever take place (e.g. KPTI boot mappings). Dovetail makes a basic assumption that stop_machine() could not get in the way of latency-sensitive processes, simply because the latter could not keep running safely until a call to the former has completed anyway.\nHowever, one should keep an eye on stop_machine() usage upstream, identifying new callers which might cause unwanted latency spots under specific circumstances (maybe even abusing the interface).\nVirtual interrupt disable state breakage When some WARN_ON() triggers due to a wrong interrupt disable state (e.g. entering the softirqs/bh code with IRQs unexpectedly [virtually] disabled), this may be due to the CPU and virtual interrupt states being out-of-sync when traversing the epilogue code after a syscall, IRQ or trap has been handled during the latest kernel entry.\nTypically, do_work_pending() or do_notify_resume() should make sure to reconcile both states in the work loop, and also to restore the virtual state they received on entry before returning to their caller.\nThe routines just mentioned always enter from their assembly call site with interrupts hard disabled in the CPU. However, they may be entered with the virtual interrupt state enabled or disabled, depending on the kind of event which led to them eventually. Typically, a system call epilogue would always enter with the virtual state enabled, but a fault might also occur when the virtual state is disabled though. The epilogue routine called for finalizing some IRQ handling must enter with the virtual state enabled, since the latter is a pre-requisite for running such code.\n Losing the timer tick The symptom of a common issue in a Dovetail port is losing the timer interrupt when the autonomous core takes control over the tick device, causing the in-band kernel to stall. After some time spent hanging, the in-band kernel may eventually complain about a RCU stall situation with a message like INFO: rcu_preempt detected stalls on CPUs/tasks followed by stack dump(s). In other cases, the machine may simply lock up due to an interrupt storm.\nThis is typical of timer interrupt events not flowing down normally to the in-band kernel anymore because something went wrong as soon as the proxy tick device replaced the regular device for serving in-band timing requests. When this happens, you should check the following code spots for bugs:\n  the timer acknowledge code is wrong once called from the oob stage, which is going to be the case as soon as an autonomous core installs the proxy tick device for interposing on the timer. Being wrong here means performing actions which are not legit from such a context.\n  the irqchip driver managing the interrupt event for the timer tick is wrong somehow, causing such interrupt to stay masked or stuck for some reason whenever it is switched to out-of-band mode. You need to double-check the implementation of the chip handlers, considering the effects and requirements of interrupt pipelining.\n  power management (CONFIG_CPUIDLE) gets in the way, often due to the infamous C3STOP misfeature turning off the original timer hardware controlled by the proxy device. A detailed explanation is given in Documentation/irq_pipeline.rst when discussing the few changes to the scheduler core for supporting the Dovetail interface. If this is acceptable from a power saving perspective, having the autonomous core prevent the in-band kernel from entering a deeper C-state is enough to fix the issue, by overriding the irq_cpuidle_control() routine as follows:\n  bool irq_cpuidle_control(struct cpuidle_device *dev, struct cpuidle_state *state) { /* * Deny entering sleep state if this entails stopping the * timer (i.e. C3STOP misfeature). */ if (state \u0026amp;\u0026amp; (state-\u0026gt;flags \u0026amp; CPUIDLE_FLAG_TIMER_STOP)) return false; return true; }  Printk-debugging such timer issue requires enabling raw printk() support, you won\u0026rsquo;t get away with tracing the kernel behavior using the plain printk() routine for this, because most of the output would remain stuck into a buffer, never reaching the console driver before the board hangs eventually.\n Hard interrupt masking in clock chip handlers The only valid way of sharing a clock tick device between the in-band and out-of-band stages is to access it through the tick proxy. For this reason, we don\u0026rsquo;t need to enforce hard interrupt masking in clock chip handlers to make them pipeline-safe, because once proxying is active for a tick device, hardware interrupts are off across calls to its handlers when applicable. As a result, either all accesses to the clock chip handlers are proxied and proper masking is already in place, or there is no proxy, which means the in-band kernel is still controlling the device, in which case there is no way we might conflict with out-of-band accesses.\nObviously, this fact does not preclude why we would still want to serialize CPUs when accessing shared data there, in which case hard locking should be in place to ensure this.\nMake no assumption in virtualizing arch_local_irq_restore() Do not make any assumption with respect to the current interrupt state when arch_local_irq_restore() is called, specifically don\u0026rsquo;t expect the inband stage to be stalled on entry. Some archs use constructs like follows, which breaks such assumption:\n\tlocal_save_flags(flags); local_irq_enable(); ... local_irq_restore(flags); In that case, we do want the stall bit to be restored unconditionally from flags. A correct implementation would be:\nstatic inline notrace void arch_local_irq_restore(unsigned long flags) { inband_irq_restore(arch_irqs_disabled_flags(flags)); barrier(); } RCU and out-of-band context The out-of-band context is semantically equivalent to the NMI context, therefore the current CPU cannot be in an extended quiescent state RCU-wise if running oob. CAUTION: the converse assertion is NOT true (i.e. a CPU running code on the in-band stage may be idle RCU-wise).\nCommon services which are safe in out-of-band context Dovetail guarantees that the following services are safe to call from the out-of-band stage:\n  irq_work() may be called from the out-of-band stage to schedule a (synthetic) interrupt in the in-band stage.\n  __raise_softirq_irqoff() may be called from the out-of-band stage to schedule a softirq. For instance, the EVL network stack uses this to kick the NET_TX_SOFTIRQ event in the in-band stage.\n  printk() may be called from any context, including out-of-band interrupt handlers. Messages are queued, then passed to the output device(s) only when the in-band stage resumes though.\n  ARM Context assumption with outer L2 cache There is no reason for the outer cache to be invalidated/flushed/cleaned from an out-of-band context, all cache maintenance operations must happen from in-band code. Therefore, we neither need nor want to convert the spinlock serializing access to the cache maintenance operations for L2 to a hard lock.\nThis above assumption is unfortunately only partially right, because at some point in the future we may want to run DMA transfers from the out-of-band context, which could entail cache maintenance operations.\n Conversion to hard lock may cause latency to skyrocket on some i.MX6 hardware, equipped with PL22x cache units, or PL31x with errata 588369 or 727915 for particular hardware revisions, as each background operation would be awaited for completion with hard irqs disabled, in order to work around some silicon bug.\n Last modified: Sat, 17 Apr 2021 11:55:02 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/altsched/",
	"title": "Alternate scheduling",
	"tags": [],
	"description": "",
	"content": "For specific use cases requiring reliable, ultra-low response times, we want to enable hosted autonomous software cores to control common Linux tasks based on their own scheduler infrastructure, fully decoupled from the host\u0026rsquo;s scheduler, with absolute priority over all other kernel activities.\nThis being said, Dovetail also promotes the idea that a dual kernel system should keep the functional overlap between the main kernel and the autonomous core minimal. To this end, a task from such core should be merely seen as a regular Linux task with additional scheduling capabilities guaranteeing very low and bounded response times. To support such idea, Dovetail enables kthreads and regular user tasks to run alternatively in the out-of-band execution context introduced by the interrupt pipeline (aka out-of-band stage), or the common in-band kernel context for GPOS operations (aka in-band stage). These new capabilities are built on the interrupt pipeline machinery.\nAs a result, autonomous core applications in user-space benefit from the common Linux programming model - including virtual memory protection -, and still have access to the main kernel services when carrying out non time-critical work.\nDovetail provides mechanisms to autonomous cores for supporting this as follows:\n  services for moving Linux tasks back and forth between the in-band and out-of-band stages in an orderly and safe way, properly synchronizing the operation between the two schedulers involved.\n  notifications about in-band events the autonomous core may want to know about for maintaining its own version of the current task\u0026rsquo;s state.\n  notifications about events such as faults, syscalls and other exceptions issued from the out-of-band execution stage.\n  integrated support for performing context switches between out-of-band tasks, including memory context and FPU management.\n  Theory of operations   a Linux task running in user-space, or a kernel thread, need to initialize the alternate scheduling feature for themselves with a call to dovetail_init_altsched(). For instance, the EVL core does so as part of the attachment process of a thread to the autonomous core when evl_attach_self() is called by the application.\n  once the alternate scheduling feature is initialized, the task should enable it by a call to dovetail_start_altsched(). From this point, that task:\n  can switch between the in-band and out-of-band execution stages freely.\n  can emit out-of-band system calls the autonomous core can handle.\n  is tracked by Dovetail\u0026rsquo;s notification system, so that in-band and out-of-band events which may involve such task are dispatched to the core.\n  can be involved in Dovetail-based context switches triggered by the autonomous core, independently from the scheduling operations carried out by the main kernel.\nConversely, dovetail_stop_altsched() disables the notifications for a task, which is likely to be detached from the autonomous core later on.\n    at any point in time, any Linux task is either controlled by the main kernel or by the autonomous core, scheduling-wise. There cannot be any overlap (for obvious reasons):\n  a task which is currently scheduled by the autonomous core runs or sleeps on the out-of-band stage. At the same time, such task is deemed to be sleeping in TASK_INTERRUPTIBLE state for the main kernel.\n  conversely, a task which is controlled by the main kernel runs or sleeps on the in-band stage. It must be considered as blocked on the other side. For instance, the EVL core defines the T_INBAND blocking condition for representing such state.\n    since the out-of-band stage receives interrupts first, regardless of any interrupt masking which may be in effect for the ongoing in-band work, the autonomous core can run high-priority interrupt handlers then reschedule with no delay.\n  tasks may switch back and forth between the in-band and out-of-band stages at will. However, this comes at a cost:\n  the process of switching stages is heavyweight, it includes a double scheduling operation at least (i.e. one to suspend on the exited stage, one to resume from the opposite stage).\n  only the out-of-band stage guarantees bounded response times to external and internal events. Therefore, a task which leaves the out-of-band stage for resuming in-band looses such guarantee, until it has fully switched back to out-of-band context at some point later.\n    at any point in time, the autonomous core keeps the CPU busy until no more task it knows about is runnable on that CPU, on the out-of-band stage. When the out-of-band activity quiesces, the core is expected to relinquish the CPU to the in-band stage by scheduling in the context which was originally preempted. This is commonly done by having a task placeholder with the lowest possible priority represent the main kernel and its in-band context linked to each per-CPU run queue maintained by the autonomous core. For instance, the EVL core assigns such a placeholder task to its SCHED_IDLE policy, which get picked when other policies have no runnable task on the CPU.\n  once the in-band context resumes, interrupt events which have no out-of-band handlers are delivered to the regular in-band IRQ handlers installed by the main kernel, if the virtual masking state allows it.\n    void dovetail_init_altsched(struct dovetail_altsched_context *p)  This call initializes the alternate scheduling context for the current task; this should be done once, before the task calls dovetail_start_altsched().\npThe alternate scheduling context is kept in a per-task structure of type dovetail_altsched_context which should be maintained by the autonomous core. This can be done as part of the per-task context management feature Dovetail introduces.\n\n  void dovetail_start_altsched(void)  This call tells the kernel that the current task may request alternate scheduling operations any time from now on, such as switching out-of-band or back in-band. It also activates the event notifier for the task, which allows it to emit out-of-band system calls to the core.\n  void dovetail_stop_altsched(void)  This call disables the event notifier for the current task, which must be done before dismantling the alternate scheduling support for that task in the autonomous core.\n What you really need to know at this point There is a not-so-subtle but somewhat confusing distinction between running a Linux task out-of-band, and running whatever code from the out-of-band execution stage.\n  In the first case, the task is not controlled by the main kernel scheduler anymore, but runs under the supervision of the autonomous core. This is obtained by performing an out-of-band switch for a task.\n  In the other case, the current underlying context (task or whatever else) may or may not be controlled by the main kernel, but the interrupt pipeline machinery has switched the CPU to the out-of-band execution mode, which means that only interrupts bearing the IRQF_OOB flag are delivered. Typically, run_oob_call() is a service provided by the interrupt pipeline which executes a function call over this context, without requiring the calling task to be scheduled by the autonomous core.\n  You will also find references to pseudo-routines called core_schedule(), core_suspend_task() or core_resume_task() in various places. Don\u0026rsquo;t look for them into the code, they don\u0026rsquo;t actually exist: those routines are mere placeholders for the corresponding services you would provide in your autonomous core. For instance, the EVL core implements them as evl_schedule(), evl_suspend_thread() and evl_release_thread().\nYou may want to keep this in mind when going through the rest of this document.\nSwitching between execution stages Out-of-band switch  Switching out-of-band is the operation by which a Linux task moves under the control of the alternate scheduler the autonomous core adds to the main kernel. From that point, the scheduling decisions are made by this core regarding that task. There are two reasons a Linux task may switch from in-band to out-of-band execution:\n  either such transition is explicitly requested via a system call of the autonomous core, such as EVL\u0026rsquo;s evl_switch_oob().\n  or the autonomous core has forced such transition, in response to a user request, such as issuing a system call which can only be handled from the out-of-band stage. The EVL core ensures this.\n  Using Dovetail, a task which is executing on the in-band stage can switch out-of-band following this sequence of actions:\n  this task calls dovetail_leave_inband() from the in-band stage it runs on (blue), which prepares for the transition, puts the caller to sleep (TASK_INTERRUPTIBLE) then reschedules immediately. At this point, the migrating task is in flight to the out-of-band stage (light red). Meanwhile, schedule() resumes the next in-band task which should run on the current CPU.\n  as the next in-band task context resumes, the scheduling tail code checks for any task pending transition to out-of-band stage, before the CPU is fully relinquished to the resuming in-band task. This check is performed by the inband_switch_tail() call present in the main scheduler. Such call has two purposes:\n  detect when a task is in flight to the out-of-band stage, so that we can notify the autonomous core for finalizing the migration process.\n  detect when a task is resuming on the out-of-band stage, which happens when the autonomous core switches context back to the current task, once the migration process is complete.\n    When switching out-of-band, case #1 is met, which triggers a call to the resume_oob_task() handler the companion core should implement for completing the transition. This would typically mean: unblock the migrating task from the standpoint of its own scheduler, then reschedule. In the following flowchart, core_resume_task() and core_schedule() stand for these two operations, with each dotted link representing a context switch:\ngraph LR; S(\"start transition\") -- A style S fill:#99ccff; A[\"dovetail_leave_inband()\"] -- B[\"schedule()\"] style A fill:#99ccff; style B fill:#99ccff; B -.- C[\"inband_switch_tail()\"] C -- D{task in flight?} D --|Yes| E[\"resume_oob_task()\"] style E fill:#99ccff; D --|No| F{out-of-band?} E -- G[\"core_resume_task()\"] style G fill:#99ccff; G -- H[\"core_schedule()\"] style H fill:#ff950e; F --|Yes| I(transition complete) style I fill:#ff950e; F --|No| J(regular switch tail) style J fill:#99ccff; H -.- C  At the end of this process, we should have observed a double context switch, with the migrating task offloaded to the out-of-band scheduler:\nevl_switch_oob() implements the switch to out-of-band context in the EVL core, with support from evl_release_thread() and evl_schedule() for resuming and rescheduling threads respectively.\n In-band switch Switching in-band is the operation by which a Linux task moves under the control of the main scheduler, coming from the out-of-band execution stage. From that point, the scheduling decisions are made by the main kernel regarding that task, and it may be subject to preemption by in-band interrupts as well. In other words, a task switching in-band looses all guarantees regarding bounded response time; however, it regains access to the entire set of GPOS services in the same move.\nThere are several reasons a Linux task which was running out-of-band so far may have to switch in-band:\n  it has requested it explicitly using a system call provided by the autonomous core such as EVL\u0026rsquo;s evl_switch_inband().\n  the autonomous core has forced such transition for a task running in user-space:\n  in response to some request this task did, such as issuing a regular system call which can only be handled from the in-band stage. Typically, this behavior is enforced by the EVL core.\n  because the task received a synchronous fault or exception, such as a memory access violation, FPU exception and so on. A demotion is required, because handling such events directly from the out-of-band stage would require a fair amount of code duplication, and most likely raise all sorts of funky conflicts between the out-of-band handlers and several in-band sub-systems. Besides, there is not much point in expecting real-time guarantees from a code that basically fixes up a situation caused by a dysfunctioning application in the first place.\n  a signal is pending for the task. Because the main kernel logic may require signals to be acknowledged by the recipient, we have to transition through the in-band stage to make sure the pending signal(s) will be delivered asap.\n    Using Dovetail, a task which executes on the out-of-band stage moves in-band following this sequence of actions:\n  the execution of an in-band handler is scheduled from the context of the migrating task on the out-of-band stage. Once it runs, this handler should call wake_up_process() to unblock that task from the standpoint of the main kernel scheduler, since it is sleeping in TASK_INTERRUPTIBLE state there. Typically, the irq_work mechanism can be used for this, because:\n  as extended by the interrupt pipeline support, this interface can be used from the out-of-band stage.\n  the handler is guaranteed to run on the CPU from which the request was issued. Because the in-band work will wait until the out-of-band activity quiesces on that CPU, this in turn ensures all other operations we have to carry out from the out-of-band stage for preparing the migration are done before the task is woken up eventually.\n    the autonomous core blocks/suspends the migrating task, rescheduling immediately afterwards. For instance, the EVL core adds the T_INBAND block bit to the task\u0026rsquo;s state for this purpose.\n  at some point later, the out-of-band context is exited by the current CPU when no more out-of-band work is left, causing the in-band kernel code to resume execution at the latest preemption point. The handler scheduled at step #1 eventually runs, waking up the migrating task from the standpoint of the main kernel. The TASK_RUNNING state is set for the task.\n  the migrating task resumes from the tail scheduling code of the alternate scheduler, where it suspended in step #2. Noticing the migration, the core calls dovetail_resume_inband() eventually, for finalizing the transition of the incoming task to the in-band stage.\n  In the following flowchart, core_suspend_task() and core_schedule() stand for the operations described at step #2, with each dotted link representing a context switch. The out-of-band idle state represents the CPU transitioning from out-of-band (light red) to in-band (blue) execution stage, as the core has no more out-of-band task to schedule:\ngraph LR; S(\"start transition\") -- A style S fill:#ff950e; A[\"irq_work(wakeup_req)\"] -- B[\"core_suspend_task()\"] style A fill:#ff950e; style B fill:#ff950e; B -- C[\"core_schedule()\"] style C fill:#ff950e; C -.- Y((OOB idle)) Y -.- D[\"wake_up_process()\"] style D fill:#99ccff; D -- E[\"schedule()\"] style E fill:#99ccff; E -.- X[\"out-of-band switch tail\"] style X fill:#99ccff; X -- G[\"dovetail_resume_inband()\"] style G fill:#99ccff; G -- I(\"transition complete\") style I fill:#99ccff;  At the end of this process, the task has transitioned from a running state to a blocked state in the autonomous core, and conversely from TASK_INTERRUPTIBLE to TASK_RUNNING in the main scheduler.\nevl_switch_inband() switches the caller to in-band context in the EVL core, with support from evl_suspend_thread() and evl_schedule() for suspending and rescheduling threads respectively.\n Switching tasks out-of-band Dovetail allows an autonomous core embedded into the main kernel to schedule a set of Linux tasks out-of-band compared to the regular in-band kernel work, so that:\n  the worse-case response time of such tasks only depends on the performance of a software core which has a limited complexity.\n  the main kernel\u0026rsquo;s logic does not have to cope with stringent bounded response time requirements, which otherwise tends to lower the throughput while making the design, implementation and maintenance significantly more complex.\n  The flowchart below represents the typical context switch sequence an autonomous core should implement, from the context of the outgoing PREV task to the incoming NEXT task:\ngraph LR; S(PREV) -- A[\"core_schedule()\"] A -- B{in-band IRQ stage?} B --|Yes| D[\"jump out-of-band\"] D -- A B --|No| C[\"pick NEXT\"] style C fill:#ff950e; C -- P{PREV == NEXT?} style P fill:#ff950e; P --|Yes| Q(no change) style Q fill:#ff950e; P --|No| H[\"dovetail_context_switch()\"] style H fill:#ff950e; H -.- I(NEXT)  Those steps are:\n  core_schedule() is called over the PREV context for picking the NEXT task to schedule, by priority order. If PREV still has the highest priority among all runnable tasks on the current CPU, the sequence stops there. CAVEAT: the core must make sure to perform context switches from the out-of-band execution stage, otherwise weird things may happen down the road. run_oob_call() is a routine the interrupt pipeline provides which may help there. See the implementation of evl_schedule() in the EVL core for a typical usage.\n  dovetail_context_switch() is called, switching the memory context as/if required, and the CPU register file to NEXT\u0026rsquo;s, saving PREV\u0026rsquo;s in the same move. If NEXT is not the low priority placeholder task but PREV is, we will be preempting the in-band kernel: in this case, we must tell the kernel about such preemption by passing leave_inband=true to dovetail_context_switch().\n  NEXT resumes from its latest switching point, which may be:\n  the switch tail code in core_schedule(), if NEXT was running out-of-band prior to sleeping, in which case dovetail_context_switch() returns false.\n  the switch tail code of schedule() if NEXT is completing an in-band switch, in which case dovetail_context_switch() returns true.\n      bool dovetail_context_switch(struct dovetail_altsched_context *prev, struct dovetail_altsched_context *next, bool leave_inband)  prevThe alternate scheduling context block of the outgoing task.\n\nnextThe alternate scheduling context block of the incoming task.\n\nleave_inbandA boolean indicating whether we are leaving the in-band tasking mode, which happens when prev is the autonomous core\u0026rsquo;s low priority placeholder task standing for the in-band kernel context as a whole.\n\nThis routine performs an out-of-band context switch. It must be called with hard IRQs off. The arch-specific arch_dovetail_context_resume() handler is called by the resuming task before leaving dovetail_context_switch(). This weak handler should be overriden by a Dovetail port which requires arch-specific tweaks for completing the reactivation of next. For instance, the arm64 port performs the fpsimd management from this handler.\ndovetail_context_switch() returns a boolean value telling the caller whether the current task just returned from a transition from out-of-band to in-band context.\n  int dovetail_leave_inband(void)  dovetail_leave_inband() should be called by your companion core in order to perform the out-of-band switch for the current task.\nOn success, zero is returned, and the calling context is running on the out-of-band stage. Otherwise, -ERESTARTSYS is returned if a signal was pending at the time of the call, in which case the transition could not take place.\n The usage is illustrated by the implementation of evl_switch_oob() in the EVL core.\n   void dovetail_resume_inband(void)  dovetail_resume_inband() should be called by your companion core as part of the in-band switch process for the current task, after the current task has resumed on the in-band execution stage from the out-of-band suspension call in step 2 of the in-band switch process. This mandatory call finalizes the transition to this stage, by reconciling the current task state with the internal state of the in-band scheduler.\n The usage is illustrated by the implementation of evl_switch_inband() in the EVL core.\n   __weak void resume_oob_task(void)  This handler should be implemented by your companion core, in order to complete step 2 of the out-of-band switch process. Basically, this handler should lift the blocking condition added to the task at step 2 of the in-band switch process which denotes in-band execution, such as T_INBAND for the EVL core.\nresume_oob_task is called with interrupts disabled in the CPU, out-of-band stage is stalled.\n An implementation of resume_oob_task() is present in the EVL core.\n The event notifier Once dovetail_start_altsched() has been called for a regular in-band task, it may receive events of interest with respect to running under the supervision of an autonomous core. Those events are delivered by invoking handlers which should by implemented by this core.\nOut-of-band exception handling If a processor exception is raised while the CPU is busy running a task on the out-of-band stage (e.g. due to some invalid memory access, bad instruction, FPU or alignment error etc.), the task has to leave such context before it may run the in-band fault handler. Dovetail notifies the core about incoming exceptions early from the low-level fault trampolines, but only when some out-of-band code was running when the exception was taken. The core may then fix up the current context, such as switching to the in-band execution stage.\nEnabling debuggers to trace tasks running on the out-of-band stage involves dealing with debug traps ptrace() may poke into the debuggee\u0026rsquo;s code for breakpointing.\n The notification of entering a trap is delivered to the handle_oob_trap_entry() handler the core should override for receiving those events (__weak binding). handle_oob_trap_entry() is passed the exception code as defined in arch/*/include/asm/dovetail.h, and a pointer to the register frame of the faulting context (struct pt_regs).\nBefore the in-band trap handler eventually exits, it invokes handle_oob_trap_exit() which the core should override if it needs to perform any fixup before the trap context is left (__weak binding). handle_oob_trap_exit() is passed the same arguments than handle_oob_trap_entry().\n  __weak void handle_oob_trap_entry(unsigned int trapnr, struct pt_regs *regs)  This handler is called when a CPU trap is received by a Dovetail-enabled task while running on the out-of-band execution stage. In such an event, the caller must be switched to the in-band stage, in order to safely perform the normal trap handling operations on return. In other words, Dovetail invokes this handler to ask your companion core to switch back to a safe in-band context, before the in-band kernel can actually handle such trap.\nObviously, the caller would lose the benefit of running on the out-of-band stage, inducing latency in the process, but since it has taken a CPU trap, it looks like things did not go as expected already anyway. So the best option in this case is to switch the caller to in-band mode, leaving the actual trap handling and any related fixup to the in-band code once the transition is done.\ntrapnrThe trap code number. Such code depends on the CPU architecture:\n the documented Intel trap numbers are used for x86 (#GP, #DE, #OF etc.) other architectures may use a Dovetail-specific enumeration defined in arch/*/include/asm/dovetail.h.  \nregsThe register file at the time of the trap.\n\nInterrupts are disabled in the CPU when this handler is called.\n An implementation of handle_oob_trap_entry() is present in the EVL core.\n  __weak void handle_oob_trap_exit(unsigned int trapnr, struct pt_regs *regs)  This handler is called when the in-band trap handler is about to unwind a CPU trap context for a Dovetail-enabled task.\nhandle_oob_trap_exit is paired with handle_oob_trap_exit, and receives the same arguments.\ntrapnrThe trap code number. Such code depends on the CPU architecture:\n the documented Intel trap numbers are used for x86 (#GP, #DE, #OF etc.) other architectures may use a Dovetail-specific enumeration defined in arch/*/include/asm/dovetail.h.  \nregsThe register file at the time of the trap.\n\nInterrupts are disabled in the CPU when this handler is called.\n An implementation of handle_oob_trap_exit() is present in the EVL core.\n System calls The autonomous core is likely to introduce its own set of system calls application tasks may invoke. From the standpoint of the in-band kernel, this is a foreign set of calls, which can be distinguished unambiguously from regular ones. If a task attached to the core issues any system call, regardless of which of the kernel or the core should handle it, the latter must be given the opportunity to:\n  handle the request directly, possibly switching the caller to out-of-band context first if required.\n  pass the request downward to the in-band system call path on the in-band stage, possibly switching the caller to in-band context if needed.\n  Dovetail intercepts system calls early in the kernel entry code, delivering them to one of these handlers the core should override:\n  the call is delivered to the handle_oob_syscall() handler if the system call number is not in the valid range for the in-band kernel - i.e. it has to belong to the core instead -, and the caller is currently running on the out-of-band stage. This is the fast path, when a task running out-of-band is requesting a service the core provides.\n  otherwise the slow path is taken, in which handle_pipelined_syscall() is probed for handling the request from the appropriate execution stage. In this case, Dovetail performs the following actions:\n  graph LR; S(\"switch to out-of-band IRQ stage\") -- A style S fill:#ff950e; A[\"handle_pipelined_syscall()\"] -- B{returned zero?} B --|Yes| C{on in-band IRQ stage?} B --|No| R{on in-band IRQ stage?} R --|Yes| U(branch to in-band user mode exit) R --|No| V(done) C --|Yes| Q(branch to in-band syscall handler) style Q fill:#99ccff; C --|No| P[switch to in-band IRQ stage] style P fill:#ff950e; P -- A  In the flowchart above, handle_pipelined_syscall() should return zero if it wants Dovetail to propagate an unhandled system call down the pipeline at each of the two possible steps, non-zero if the request was handled. Branching to the in-band user mode exit code ensures any pending (in-band) signal is delivered to the current task and rescheduling opportunities are taken when (in-band) kernel preemption is enabled.\nWhat makes a system call number out-of-range for the in-band kernel is architecture-dependent. All architectures currently use the most-significant bit in the syscall number as a differentiator (i.e. regular if MSB cleared, foreign if LSB set). See how __EVL_SYSCALL_BIT is used for this purpose in the libevl implementation for arm64, x86 and ARM respectively.\nInterrupts are always enabled in the CPU when any of these handlers is called.\nThe core may need to switch the calling task to the converse execution stage (i.e. in-band \u0026lt;-\u0026gt; out-of-band) either from the handle_oob_syscall() or handle_pipelined_syscall() handlers, this is fine. Dovetail would notice and continue accordingly to the current stage on return of these handlers.\n   __weak void handle_oob_syscall(struct pt_regs *regs)  This handler should implement the fast path for handling out-of-band system calls. It is called by Dovetail when a system call is detected on entry of the common syscall path implemented by the in-band kernel, which should be handled directly by the companion core instead. Both of the following conditions must be met for this route to be taken:\n  the caller is a user task running on the out-of-band execution stage.\n  the system call number is not in the range of the regular in-band system call numbers.\n  A system call meeting those conditions denotes a request issued by an application to a companion core which it can handle directly from its native execution stage (i.e. out-of-band). This handler should be defined in the code of your companion core implementation for receiving such system calls. Dovetail defines a dummy weak implementation of it, which the implementation of the core would supersede if defined (__weak binding).\nregsThe register file at the time of the system call, which contains the call arguments passed by the application.\n\nThe handler should write the error code of the request to the in-memory storage of the proper CPU register in regs, as defined by the ABI convention for the CPU architecture.\nhandle_oob_syscall is called with interrupts enabled in the CPU, out-of-band stage is unstalled. Whether the in-band stage accepts interrupts is undefined.\n The EVL core exhibits a typical implementation of such a handler.\n   __weak void handle_pipelined_syscall(struct irq_stage *stage, struct pt_regs *regs)  This handler is called by Dovetail when a system call is detected on entry of the common syscall path implemented by the in-band kernel, if the requirements for delivering it to the companion core via the fast route implemented by handle_oob_syscall are not met, and any of the following condition is true:\n  the caller is a user task for which alternate scheduling was enabled.\n  the system call number is not in the range of the regular in-band system call numbers, which means that a companion core might be able to handle it (if not eventually, such a request would cause the caller to receive -ENOSYS).\n  stageThe descriptor address of the calling stage, either oob_stage or inband_stage.\n\nregsThe register file at the time of the system call.\n\nThe handler should write the error code of the request to the in-memory storage of the proper CPU register in regs, as defined by the ABI convention for the CPU architecture. In addition, handle_pipelined_syscall should return an integer status, which specifies the action Dovetail should take on return:\n  zero tells Dovetail to pass the system call to the regular in-band handler next. This status makes sense if the companion core did not handle the request, but did switch the calling context to in-band mode. This typically happens when the companion core wants to automatically demote the execution stage of the caller when it detects a regular in-band system call issued over the wrong (i.e. out-of-band) context, in which case it may switch it in-band automatically for preserving the system integrity, before asking Dovetail to forward the request to the right (in-band) handler.\n  a strictly positive status tells Dovetail to branch to the system call exit path immediately. This status makes sense if the companion core did handle the request, leaving the caller on the out-of-band execution stage.\n  a negative status tells Dovetail to branch to the regular system call epilogue, without passing the system call to the regular in-band handler though. This status makes sense if the companion core already handled the request, switching to in-band mode in the process. In such an event, the in-band kernel still wants to check for pending signals and rescheduling opportunity, which is the purpose of the epilogue code.\n  handle_pipelined_syscall is called with interrupts enabled in the CPU, out-of-band stage is unstalled. If current, the in-band stage accepts interrupts, otherwise whether the in-band stage is stalled or not is undefined.\n An implementation of handle_pipelined_syscall() is present in the EVL core.\n In-band events The last set of notifications involves pure in-band events which the autonomous core may need to know about, as they may affect its own task management. Except for INBAND_TASK_EXIT and INBAND_PROCESS_CLEANUP which are called for any exiting user-space task, other notifications are only issued for tasks for which dovetailing is enabled (see dovetail_start_altsched()).\nThe notification is delivered to the handle_inband_event() handler. The execution context of this handler is always in-band. The out-of-band and in-band stages are both unstalled at the time of the call. The notification handler receives the event type code, and a single pointer argument which depends on the event type. The following events are defined (see include/linux/dovetail.h):\n  INBAND_TASK_SIGNAL(struct task_struct *target)\nsent when @target is about to receive a signal. The core may decide to schedule a transition of the recipient to the in-band stage in order to have it handle that signal asap, which is required for keeping the kernel sane. This notification is always sent from the context of the issuer.\n  INBAND_TASK_MIGRATION(struct dovetail_migration_data *p)\nsent when p-\u0026gt;task is about to move to CPU p-\u0026gt;dest_cpu.\n  INBAND_TASK_EXIT(struct task_struct *current)\nsent from do_exit() before the current task has dropped the files and mappings it owns.\n  INBAND_PROCESS_CLEANUP(struct mm_struct *mm)\nsent before mm is entirely dropped, before the mappings are exited. Per-process resources which might still be maintained by the core could be released there, as all tasks sharing this memory context have exited. Unlike other events, this one is triggered for every user-space task which is being dismantled by the kernel, regardless of whether some of its threads were known from your autonomous core.\n  INBAND_TASK_RETUSER(void)\nsent as a result of arming the return-to-user request via a call to dovetail_request_ucall(). In other words, Dovetail fires this event because you asked for it by calling the latter service. The INBAND_TASK_RETUSER event handler in your companion core is allowed to switch the caller to the out-of-band stage before returning, ensuring the application code resumes in that context.\n  INBAND_TASK_PTSTOP(void)\nsent when the current in-band task is about to sleep in ptrace_stop(), which is the place a task is supposed to wait until a debugger allows it to continue. A user-space task which receives SIGTRAP as a result of hitting a breakpoint, or SIGSTOP from the ptrace(2) machinery parks itself by calling ptrace_stop(), until resumed by a ptrace(2) request.\n  INBAND_TASK_PTCONT(void)\nsent when the current in-band task is waking up from ptrace_stop() after the debugger allowed it to continue. The task may return to user-space afterwards, or go handling some pending signals.\n  INBAND_TASK_PTSTEP(struct task_struct *target)\nsent by the ptrace(2) implementation when it is about to (single-)step the @target task. Like PTSTOP and PTCONT, PTSTEP can be used to synchronize your companion core with the ptrace(2) logic. For instance, EVL uses these events to provide support for synchronous breakpoints when debugging applications over gdb.\n    __weak void handle_inband_event(enum inband_event_type event, void *data)  The handler which should be defined in the code of your companion core implementation for receiving in-band event notifications. Dovetail defines a dummy weak implementation of it, which your implementation would supersede if defined.\neventThe event code (INBAND_TASK_*) as defined above.\n\ndataAn opaque pointer to some data further qualifying the event, which actual type depends on the event code.\n\nThe in-band stage is always current and accepts interrupts on entry to this call.\n An implementation of handle_inband_event() is present in the EVL core.\n Alternate task context Your autonomous core will need to keep its own set of per-task data, starting with the alternate scheduling context block. To help in maintaining such information on a per-task, per-architecture basis, Dovetail adds the oob_state member of type struct oob_thread_state to the (stack-based) thread information block (aka struct thread_info) each kernel architecture port defines.\nYou may want to fill that structure reserved to out-of-band support with any information your core may need for maintaining a task context. By having this information defined in a file accessible from the architecture-specific code by including \u0026lt;dovetail/thread_info.h\u0026gt;, the core-specific structure is automatically added to struct thread_info as required. For instance:\n arch/arm/include/dovetail/thread_info.h\n struct oob_thread_state { /* Define your core-specific per-task data here. */ }; The core may then retrieve the address of the structure by calling dovetail_current_state(). For instance, this is the definition the EVL core has for the oob_thread_state structure, storing a backpointer to its own task control block, along with a core-specific preemption count for fast stack-based access:\nstruct evl_thread; struct oob_thread_state { struct evl_thread *thread; int preempt_count; }; Which gives the following chain:\ngraph LR; T(\"struct thread_info\") -- |includes| t t(\"struct oob_thread_state\") -.- |refers to| c c(\"struct evl_thread\")  Note that we should not add all of the out-of-band state data directly into the oob_thread_state structure, because the latter is present in every task_struct descriptor, although only a few tasks may actually need it. Hence the backpointer to the evl_thread structure which only consumes a few bytes, so leaving it unused (NULL) for all other - in-band only - tasks is no big deal.\n  struct oob_thread_state *dovetail_current_state(void)  This call retrieves the address of the out-of-band data structure for the current task, which is always a valid pointer. The content of this structure is zeroed when the task is created, and stays so until your autonomous core initializes it.\nExtended memory context Your autonomous core may also need to keep its own set of per-process data. To help in maintaining such information on a per-mm basis, Dovetail adds the oob_state member of type struct oob_mm_state to the generic struct mm_struct descriptor . Because kernel threads can only borrow memory contexts temporarily but do not actually own any, this Dovetail extension is only available to EVL threads running in user-space, not to threads created by evl_run_kthread().\nYou may want to fill that structure reserved to out-of-band support with any information your core may need for maintaining a per-mm context. By having this information defined in a file accessible from the architecture-specific code by including \\\u0026lt;dovetail/mm_info.h\\\u0026gt;, the core-specific structure is automatically added to struct mm_struct as required. For instance:\n arch/arm/include/dovetail/mm_info.h\n struct oob_mm_state { /* Define your core-specific per-mm data here. */ }; The core may then retrieve the address of the structure for the current task by calling dovetail_mm_state(). The EVL core does not define any extended memory context yet, but it could be used to maintain a per-process file descriptor table for instance.\nCatching the INBAND_PROCESS_CLEANUP event would allow you to drop any resources attached to the extended memory context information, since it is called when this data is no longer in use by any thread of the exiting process.\n   struct oob_mm_state *dovetail_mm_state(void)  This call retrieves the address of the out-of-band data structure within the mm descriptor of the current user-space task. The content of this structure is zeroed by the in-band kernel when it creates the memory context, and stays so until your autonomous core initializes it. If called from a kernel thread, NULL is returned instead.\n Definition of dovetail_mm_state()\n static inline struct oob_mm_state *dovetail_mm_state(void) { if (current-\u0026gt;flags \u0026amp; PF_KTHREAD) return NULL; return \u0026amp;current-\u0026gt;mm-\u0026gt;oob_state; } Intercepting return to in-band user mode In some specific cases, the implementation of your companion core may need some thread running in-band to call back before it resumes execution of the application code in user mode. For instance, you might want to force that thread to switch back to the out-of-band stage before it leaves the kernel and resumes user mode execution. For this to happen, you would need that thread to jump back to the core at that point, which would do the right thing from there. dovetail_request_ucall allows that.\n  void dovetail_request_ucall(struct task_struct *target)  targetThe task which should call back at the first opportunity. target should have enabled the alternate scheduling support by a previous call to dovetail_start_altsched(). If not, dovetail_request_ucall has no effect.\n\nPend a request for target to fire the INBAND_TASK_RETUSER event at the first opportunity, which happens when the task is about to resume execution in user mode from the in-band stage.\nIf the INBAND_TASK_RETUSER event handler in the companion core sends any signal to the current task, it will be delivered before the application code resumes. The event handler may pend a request to be called back again using dovetail_request_ucall().\n Last modified: Sat, 05 Jun 2021 19:29:37 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/files/",
	"title": "File tracking",
	"tags": [],
	"description": "",
	"content": "A companion core usually wants its device drivers to export a file interface to applications. It may even generalize this to all of the resources it provides, which can then be implemented by device drivers and referred to by common file descriptors from applications. For this, we need a way for the core to maintain its own per-process index of files which may support out-of-band I/O, since we will not be allowed to reuse the in-band VFS services from the out-of-band context for this purpose (e.g. current-\u0026gt;files may not be safely accessed from the out-of-band stage by the companion core). Dovetail provides a simple mechanism for tracking file descriptor creation and deletion happening system-wide from the companion core, based on calling hooks directly from the VFS, which the companion core can connect to by overriding their empty weak definitions:\n  install_inband_fd() is invoked whenever a new file descriptor is installed into the file descriptor table of the current process.\n  replace_inband_fd() is invoked whenever the file referred to by an existing file descriptor has changed.\n  uninstall_inband_fd() is called whenever a file descriptor is closed.\n  In addition, the file description structure maintained by the VFS (struct file) contains an opaque pointer named oob_data, which drivers may use freely for keeping any information related to out-of-band handling. The oob_data member comes in handy as a way for the companion core to figure out whether a given file (struct) supports out-of-band requests. If non-NULL, it may assume the out-of-band capable driver set it for such a file when opening some device. For instance, install_inband_fd() can check this information to determine whether the file which is being indexed by the in-band kernel does relate to a file which supports out-of-band operations too, eventually storing the [ file descriptor, file description structure ] tuple into its own table if so.\n As an example, the EVL core uses this tracking capabilities for implementing its own interface to out-of-band file operations such as oob_ioctl(), oob_read() and oob_write().\n   void __weak install_inband_fd(unsigned int fd, struct file *file, struct files_struct *files)  fdThe file descriptor which is being installed in the file descriptor table of the current task so as to refer to file.\n\nfileThe file which is indexed on fd.\n\nfilesThe file table of the current task.\n\ninstall_inband_fd() is invoked whenever a new file descriptor is installed into the file descriptor table of the current process, typically when indexing a newly opened file. This implies that install_inband_fd() happens once the file structure it refers to is fully built, representing a connection to some character-based device for some driver. In its simplest form, duplicating a descriptor would also cause install_inband_fd() to be invoked for the new descriptor.\nNOTE: files-\u0026gt;file_lock is locked on entry to this call, the in-band stage accepts interrupts.\n  void __weak replace_inband_fd(unsigned int fd, struct file *file, struct files_struct *files)  fdThe file descriptor which is being reassigned to a new file in the file descriptor table of the current task.\n\nfileThe file which is indexed on fd.\n\nfilesThe file table of the current task.\n\nreplace_inband_fd() is invoked whenever the file referred to by an existing file descriptor has changed, typically as a result of calling dup2(2), asking to replace an active descriptor.\nNOTE: files-\u0026gt;file_lock is locked on entry to this call, the in-band stage accepts interrupts.\n  void __weak uninstall_inband_fd(unsigned int fd, struct file *file, struct files_struct *files)  fdThe file descriptor which is being removed from the file descriptor table of the current task.\n\nfileThe file which was indexed on fd.\n\nfilesThe file table of the current task.\n\nuninstall_inband_fd() is called whenever a file descriptor is closed, either explicitly by the current process or implicitly by the in-band kernel, as a result of calling dup(2) or any of its derivatives, or releasing the resources of an exiting process.\nUnlike with other notifications, the process cleanup operations may be called from a process context which is different from the process owning the resources being released. Therefore, the implementation of uninstall_inband_fd() should never assume that current-\u0026gt;mm or current-\u0026gt;files are relevant in the context of that particular call.\n NOTE: files-\u0026gt;file_lock is locked on entry to this call, the in-band stage accepts interrupts.\n Last modified: Sun, 08 Mar 2020 13:06:41 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/dovetail/sockets/",
	"title": "Socket handling",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://the-going.github.io/website/dovetail/rulesofthumb/",
	"title": "Rules Of Thumb",
	"tags": [],
	"description": "",
	"content": "Turn on debug options in the kernel configuration! During the development phase, do yourself a favour: turn on CONFIG_DEBUG_IRQ_PIPELINE and CONFIG_DEBUG_DOVETAIL.\nThe first one will catch many nasty issues, such as calling unsafe in-band code from out-of-band context. The second one checks the integrity of the alternate scheduling support, detecting issues in the architecture port.\nThe runtime overhead induced by enabling these options is marginal. Just don\u0026rsquo;t port Dovetail or implement out-of-band client code without them enabled in your target kernel, seriously.\nSerialize stages when accessing shared data If some writable data is shared between in-band and out-of-band code, you have to guard against out-of-band code preempting or racing with the in-band code which accesses the same data. This is required to prevent dirty reads and dirty writes:\n  one the same CPU, by disabling interrupts in the CPU.\n  from different CPUs, by using hard or hybrid spinlocks.\n  Check that the pipeline torture tests pass Before any consideration is made to implement out-of-band code on a platform, check that interrupt pipelining is sane there, by enabling CONFIG_IRQ_PIPELINE_TORTURE_TEST in the configuration. As its name suggests, this option enables test code which exercizes the interrupt pipeline core, and related features such as the proxy tick device.\nSince the torture tests need to enable the out-of-band stage for their own purpose, you may have to disable any Dovetail-based autonomous core in the kernel configuration for running those tests, like switching off CONFIG_EVL if the EVL core is enabled.\n When those tests pass, the following output should appear in the kernel log:\nStarting IRQ pipeline tests... IRQ pipeline: high-priority torture stage added. irq_pipeline-torture: CPU0 initiates stop_machine() irq_pipeline-torture: CPU3 responds to stop_machine() irq_pipeline-torture: CPU1 responds to stop_machine() irq_pipeline-torture: CPU2 responds to stop_machine() irq_pipeline-torture: CPU1: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU2: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU3: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: in-band-\u0026gt;in-band irq_work trigger works irq_pipeline-torture: CPU0: stage escalation request works irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: oob-\u0026gt;in-band irq_work trigger works IRQ pipeline: torture stage removed. IRQ pipeline tests OK. Otherwise, if you observe any issue when running any of those tests, then the IRQ pipeline definitely needs fixing.\nKnow how to differentiate safe from unsafe in-band code Not all in-band kernel code is safe to be called from out-of-band context, actually most of it is unsafe for doing so.\nA code is deemed safe in this respect when you are 101% sure that it never does, directly or indirectly, any of the following:\n  attempts to reschedule in-band wise, meaning that schedule() would end up being called. The rule of thumb is that any section of code traversing the might_sleep() check cannot be called from out-of-band context.\n  takes a spinlock from any regular type like raw_spinlock_t or spinlock_t. The former would affect the virtual interrupt disable flag which is invalid outside of the in-band context, the latter might reschedule if CONFIG_PREEMPT is enabled.\n  In the early days of dual kernel support in Linux, some people would mistakenly invoke the do_gettimeofday() routine from an out-of-band context in order to get a wallclock timestamp for their real-time code. Doing so would create a deadlock situation if some in-band code running do_gettimeofday() is preempted by the out-of-band code re-entering the same routine on the same CPU. The out-of-band code would then wait spinning indefinitely for the in-band context to leave the routine - which won\u0026rsquo;t happen by design - leading to a lockup. Nowadays, enabling CONFIG_DEBUG_IRQ_PIPELINE would be enough to detect such mistake early enough to preserve your mental health.\n Careful with disabling interrupts in the CPU When pipelining is enabled, use hard interrupt protection with caution, especially from in-band code. Not only this might send latency figures over the top, but this might even cause random lockups would a rescheduling happen while interrupts are hard disabled.\nDealing with spinlocks Converting regular kernel spinlocks (e.g. spinlock_t, raw_spin_lock_t) to hard spinlocks should involve a careful review of every section covered by such lock. Any such section would then inherit the following requirements:\n  no unsafe in-band kernel service should be called within the section.\n  the section covered by the lock should be short enough to keep interrupt latency low.\n  Enable RAW_PRINTK support for printk-like debugging Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, you should rather use the raw_printk() interface for this.\n Last modified: Sat, 21 Nov 2020 17:55:57 \u0026#43;0100\n"
},
{
	"uri": "https://the-going.github.io/website/",
	"title": "Xenomai 4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://the-going.github.io/website/devprocess/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Xenomai 4 development process The Xenomai 4 project works on three software components:\n  The Dovetail interface, which introduces a high-priority execution stage into the linux kernel logic, on which a functionally-independent companion software core may receive interrupts and run threads.\n  A compact and SMP-scalable real-time core - aka the EVL core - leveraging Dovetail\u0026rsquo;s capabilities, which is intended to be the reference implementation for other Dovetail-based companion cores. As such, the Dovetail code base progresses as much as the EVL core runs on the most recent kernel releases and exercises this interface, uncovering issues. The EVL core lives in kernel space, as an optional component of the linux kernel.\n  A library named libevl, which implements the system call interface between applications running in user-space and the EVL core, along with a few other basic services, utilities and sanity tests.\n  The following aspects are addressed in the way the code is currently maintained:\n  We want to track the latest mainline kernel code as closely as it makes sense, while dealing with the issue of developing out-of-tree code from the standpoint of the mainline kernel.\n  The Dovetail code must be accessible to other projects for their own use, based on releases of the reference (mainline) kernel.\n  Maintaining the EVL core (and Dovetail) over multiple kernel releases should remain a tractable problem.\n  EVL core development and releases The EVL core is maintained in the following GIT repository:\n git@git.xenomai.org:Xenomai/xenomai4/linux-evl.git https://git.xenomai.org/xenomai4/linux-evl.git  This repository tracks the Dovetail kernel tree, which in turn tracks the mainline kernel. Details of the current policy are as follows:\n  since the EVL core is always based on a Dovetail port to a mainline kernel release, every EVL branch is initially based on a Dovetail branch. For instance v5.10.y-evl-rebase in the EVL tree originates from v5.10.y-dovetail-rebase in the Dovetail tree.\n  the content of *-rebase branches may be rebased without notice, so that Dovetail- and EVL-related commits always appears on top of the vanilla mainline code. All EVL branches are currently rebased, we have no merge branch yet.\n  EVL release tags are of the form:\n v\u0026lt;kversion\u0026gt;-evl\u0026lt;serial\u0026gt;-rebase if added to a rebase branch. v\u0026lt;kversion\u0026gt;-evl\u0026lt;serial\u0026gt; if added to a merge branch.    libevl development and releases The strictly linear development workflow of libevl is simpler in comparision to the EVL core. There is a single master branch in the libevl GIT tree where the development takes place. Over time, libevl releases are tagged from arbitrary commits into this branch. These tags look like r\u0026lt;serial\u0026gt;, with the serial number progressing indefinitely as subsequent releases are issued.\nA new libevl release may be tagged whenever any of the following happens:\n  the ABI exported by the EVL core has changed in a way which is not backward-compatible with the latest libevl release. In other words, EVL_ABI_PREREQ in some release of libevl is not in the range defined by [EVL_ABI_BASE..EVL_ABI_LEVEL] anymore.\n  the API libevl implements has changed, usually due to the addition of new services or changes to the signature of existing routine(s). This should happen rarely, since libevl is only meant to provide a small set of basic services exported by the EVL core. The API version implemented by libevl is an integer, which is assigned to the __EVL__ C macro-definition. This information can also be retrieved at runtime by calling the evl_get_version() routine.\n  In any case, there is no strict relationship between a given EVL core release tag and a libevl release tag. A particular libevl release might be usable with multiple subsequent EVL core releases and conversely, provided the ABI requirements are met.\n Last modified: Mon, 01 Jan 0001 00:00:00 UTC\n"
},
{
	"uri": "https://the-going.github.io/website/core/abi-revs/",
	"title": "ABI revisions",
	"tags": [],
	"description": "",
	"content": "  #abimap { width: 35%; margin-left: auto; margin-right: auto; } #abimap th { text-align: center; } #abimap td { text-align: center; } #abimap tr:nth-child(even) { background-color: #f2f2f2; }   Revision Purpose libevl release   26 Add socket interface. r21   25 Add latmus request for measuring in-band response time to synthetic interrupt latency. r21   24 Add proxy read side. r19   23 Add the Observable element, and thread observability. r17   22 Add EVL_THRIOC_UNBLOCK, EVL_THRIOC_DEMOTE and EVL_THRIOC_YIELD, update EVL_THRIOC_*_MODE operations. r16   21 Introduce element visibility attribute r15   20 Add support for compat mode (32-bit exec over 64-bit kernel) r12   19 Make y2038 safe r11   18 Plan for supporting a range of ABI revisions -   17 Replace SIGEVL_ACTION_HOME with RETUSER event -   16 Add synchronous breakpoint support -   15 Notify stax-related oob exclusion via SIGDEBUG_STAGE_LOCKED -   14 Add stax test helpers to 'hectic' driver -   13 Add stage exclusion lock mechanism -   12 Add support for recursive gate monitor lock -   11 Read count of timer expiries as a 64bit value -   10 Track count of remote thread wakeups -   9 Complete information returned by EVL_THRIOC_GET_STATE -   8 Add query for CPU state -   7 Drop time remainder return from EVL_CLKIOC_SLEEP -   6 Enable fixed-size writes to proxy -   5 Ensure latmus sends the ultimate bulk of results -   4 Split per-thread debug mode flags -   3 Add count of referrers to poll object shared state -   2 Drop obsolete T_MOVED from thread status bits -   1 Add protocol specifier to monitor element -   0 Initial revision -     Last modified: Sat, 21 Aug 2021 18:26:39 \u0026#43;0200\n"
},
{
	"uri": "https://the-going.github.io/website/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://the-going.github.io/website/license/",
	"title": "Licensing terms",
	"tags": [],
	"description": "",
	"content": "SPDX license identifiers are used throughout the code to state the licensing terms of each file clearly. This boils down to:\n  GPL-2.0 for the EVL core in kernel space.\n  [GPL-2.0] (https://spdx.org/licenses/GPL-2.0.html) WITH Linux-syscall-note for the UAPI bits exported to user-space, so that libevl knows at build time about the ABI details of the system call interface implemented by the EVL core.\n  MIT for all code from libevl, which implements the EVL system call wrappers, a few utilities and test programs.\n  "
},
{
	"uri": "https://the-going.github.io/website/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]